<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="/atom.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2017-11-28T13:38:25+08:00</updated><id>/</id><title type="html">地狱爬行者’s Blog</title><subtitle>原创网络安全博客</subtitle><author><name>demon</name></author><entry><title type="html">黑盒漏洞挖掘（WEB篇）</title><link href="/pentest/2017/11/13/pentest-for-blackbox/" rel="alternate" type="text/html" title="黑盒漏洞挖掘（WEB篇）" /><published>2017-11-13T16:41:18+08:00</published><updated>2017-11-13T16:41:18+08:00</updated><id>/pentest/2017/11/13/pentest-for-blackbox</id><content type="html" xml:base="/pentest/2017/11/13/pentest-for-blackbox/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;在安全测试中，我们常常会遇到一些重复性工作。而鄙人记性不佳，有时候并不能高效的进行工作。因此，自己萌生了撰写工作总结的想法，将以往的工作列清单，做一个总结，以作查阅之用。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;当然，这个系列也希望会持续更新。人，总是要有进步不是。&lt;/p&gt;

&lt;h3 id=&quot;克敌机先&quot;&gt;克敌机先&lt;/h3&gt;

&lt;p&gt;在测试某个网站时，我们常常需要先探测该网站的基本信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;是否存在WAF，像创宇盾这种检测到攻击会马上暂时性封IP，阿里云盾偷偷摸摸检测到攻击行为后，会直接无前兆地封禁你IP，至于安全狗、D盾、云锁、百度云安全这种就不一一例举了，暂时不细说。&lt;/li&gt;
  &lt;li&gt;路径扫描，如果存在WAF或者目标较为脆弱，可以直接调整为非常低的线程数，比如一个线程，然后对目标进行文件/目录低速探测。如果没有太多限制，那就使劲儿造作。&lt;/li&gt;
  &lt;li&gt;如果需要测试的网站比较大且比较耐操，且没有WAF/IDS，建议配置好去重和线程，使用大型扫描器对网站进行探测。这样做的好处是不会漏掉一些常用的漏洞，而且可以获取网站的结构。但缺点是需要等待比较长的时间，也容易被管理发现。&lt;/li&gt;
  &lt;li&gt;部分网站的后台，可能存在于域名的其他端口上。如果服务器不存在CDN，这时候可以借助类nmap的扫描器来探测端口开放情况，比如使用命令：
```
nmap -sS -sV -p- –script=”http-title,banner” 192.168.2.10&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;```&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;插件检测server类型，是否为tomcat/jboss/glassfish之类，探测是否存在可访问webserver console。&lt;/li&gt;
  &lt;li&gt;检测网站是否为公开的CMS，探测CMS类型，漏洞库是否存在相关漏洞。&lt;/li&gt;
  &lt;li&gt;查看有无列目录的权限。&lt;/li&gt;
  &lt;li&gt;查看是否提供附件下载，如果是参数传递的URL，这类链接通常比较敏感。如果可以明显给出了文件的相对/绝对路径/可预测的类文件名id，我们可以优先进行任意文件下载测试，其次再进行常规漏洞测试。如果仅仅是给出了普通的id值，那可以直接进行常规漏洞测试。&lt;/li&gt;
  &lt;li&gt;查看网站是否有明显的配置漏洞，比如提供了allow *的crossdomain.xml；没有进行过滤的robots.txt；容易猜测名字的源码压缩包；泄露敏感信息的配置文件、JS文件甚至普通html文件；.git/.svn文件供下载等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;权限权限&quot;&gt;权限，权限！&lt;/h3&gt;

&lt;p&gt;在遇到某个网站时，窃以为应该先寻找能够获取更大权限的地方，而登录后的权限无疑会更大。&lt;/p&gt;

&lt;p&gt;这时，我们可以尝试进行注册，或者夺取原有的账户权限。&lt;/p&gt;

&lt;p&gt;此时有一点需要注意，在登录的入口处和搜索处，可能存在一定的脆弱性，建议同其他前台产生的敏感数据包一同进行fuzz检测。&lt;/p&gt;

&lt;h5 id=&quot;自给自足&quot;&gt;自给自足&lt;/h5&gt;

&lt;p&gt;在本身无权限时，我们可以去寻找注册的地方，间接获取更高的权限。
同时在此过程中，我们有几个点需要注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;研究找回密码功能，查看是否有无逻辑漏洞可以跳过，可否预测和爆破验证码。&lt;/li&gt;
  &lt;li&gt;可能存在上传，查看能否直接获取webshell&lt;/li&gt;
  &lt;li&gt;可能存在XSS，如果在注册能直接通过时，需要等待注册完毕，去用户中心再继续测试。如若需要等待审核，可以尝试植入XSS平台代码，等待机器上线。&lt;/li&gt;
  &lt;li&gt;如若存在手机和邮箱验证，可尝试有无邮箱炸弹漏洞。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;鸠占鹊巢&quot;&gt;鸠占鹊巢&lt;/h5&gt;

&lt;p&gt;有时候管理或者开发不够细心，我们可以通过各种手段去获取原有普通用户或者管理用户的权限。在我们能够比较容易的获取后台登陆口，或者在无法注册用户时，我们可以如此操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过手工猜测密码/识别简单验证码/验证码绕过/直接爆破，猜测密码，来获取原有用户的权限。&lt;/li&gt;
  &lt;li&gt;查看用户名是否存在，尝试能否预测/枚举用户，可能有机会猜测管理账号。&lt;/li&gt;
  &lt;li&gt;搜寻文档，后台页面可能存在一些敏感说明文档提供下载，里面有可能会记录测试账号等敏感内容，对测试会比较有帮助。&lt;/li&gt;
  &lt;li&gt;万能密码绕过，这种情况一般asp和php类型的网站会存在的稍微多些，不过新一点的网站这样的问题越来越少了。&lt;/li&gt;
  &lt;li&gt;禁用JS，直接卡入后台管理页面。这种情况一般需要预先爆破或者在JS源码里找寻后台路径，然后再使用该方法。&lt;/li&gt;
  &lt;li&gt;工具/插件修改cookie，直接进入管理后台。这种方法比较古老了，现在成功的几率是比较小的。&lt;/li&gt;
  &lt;li&gt;善用类Google/类Github/类shodan引擎，搜寻泄露的后台账号密码/带认证的链接等等。&lt;/li&gt;
  &lt;li&gt;设法找寻网站管理和开发者的相关信息，查找社工库组合相关密码。如果是针对企业和组织进行渗透，可设法查询社工库里相关的邮箱密码，再借助可登入的邮箱翻看敏感内容，从而达到进入后台的目的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;用户中心&quot;&gt;用户中心&lt;/h3&gt;

&lt;p&gt;登入网站用户中心后，我们一般可以做如下测试。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;抓包测试，查看在数据包中修改参数，能否越权枚举和修改用户的基本信息/订单/地址等内容。&lt;/li&gt;
  &lt;li&gt;在用户互发私信功能处，可能存在存储型XSS。&lt;/li&gt;
  &lt;li&gt;如若在注册时，并未测试存储XSS、邮箱/手机/私信炸弹、越权修改密码，可在此测试。&lt;/li&gt;
  &lt;li&gt;查看是否存在附件上传/头像上传，文件命名规律也可稍作检查，最后查看能否直接获取webshell。&lt;/li&gt;
  &lt;li&gt;查看搜索功能是否有别于前台，能否间接枚举用户或者手机号。&lt;/li&gt;
  &lt;li&gt;在用户中心进行SQL注入和XSS的fuzz测试，成功几率大抵会高于前台。&lt;/li&gt;
  &lt;li&gt;对于邀请注册/登出系统/登录跳转/注册验证的链接，都可能存在任意URL跳转的漏洞。&lt;/li&gt;
  &lt;li&gt;如果用户中心未启用token验证，在修改密码/修改用户名和昵称/修改地址/修改邮箱和手机号等地方，很可能存在CSRF漏洞。&lt;/li&gt;
  &lt;li&gt;上传后的地址可能存在列目录漏洞。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;管理后台&quot;&gt;管理后台&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;管理后台可能存在上传的地方，可以尝试能否获取webshell。&lt;/li&gt;
  &lt;li&gt;上传处，如若能上传文本类型文档，查看是否有预览功能，进而考虑能否写入存储型XSS。&lt;/li&gt;
  &lt;li&gt;上传处，如果能够上传XML文档，或者只是提交XML内容，可以尝试XXE攻击。&lt;/li&gt;
  &lt;li&gt;上传处，如果过滤了大多数动态文件，但并不是使用的白名单，可以考虑上传shtml，同样可能执行命令。&lt;/li&gt;
  &lt;li&gt;配置处可能存在未过滤的地方，可以直接写入存储型XSS。&lt;/li&gt;
  &lt;li&gt;php类的网站，有的可以通过转义和截断，往配置文件里写入webshell。&lt;/li&gt;
  &lt;li&gt;asp类的网站，可以通过写配置，然后数据库备份得到webshell，这里不细说。&lt;/li&gt;
  &lt;li&gt;后台大多验证不严，可以尝试SQL注入和XSS的fuzz测试。&lt;/li&gt;
  &lt;li&gt;后台可能存在执行命令的地方，可进行fuzz，获取敏感配置或者直接反弹shell。&lt;/li&gt;
  &lt;li&gt;如若后台存在通讯录，在不违反合规条款的情况下，可尝试自行处理。&lt;/li&gt;
  &lt;li&gt;部分后台如果存在管理用户表，在不违反合规条款的情况下，可尝试自行处理。&lt;/li&gt;
  &lt;li&gt;管理后台大概率不会验证token的，所以很多网站后台也因此存在CSRF。&lt;/li&gt;
  &lt;li&gt;如果非管理用户，也可以尝试越权修改其他用户的密码。&lt;/li&gt;
  &lt;li&gt;后台有时会存在读取文件内容，或者探测内网存活的功能，抑或是上传特殊文件进行解析，可以尝试是否存在SSRF。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;权宜之计&quot;&gt;权宜之计&lt;/h3&gt;

&lt;p&gt;如果我们无法登陆后台，也无法注册怎么办？我们可以尝试以下几种做法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;评论处可能存在上传和存储型XSS，也可能存在没有验证重复提交的CSRF。&lt;/li&gt;
  &lt;li&gt;提交个人建议等供审核的内容时，有可能后台没有做XSS过滤，这时候就可以进行盲打。&lt;/li&gt;
  &lt;li&gt;某些用户的个人页面可以通过枚举id获得，里面可能存在一些敏感信息。&lt;/li&gt;
  &lt;li&gt;在前台或者用户中心，可能存在星号隐藏一些敏感信息，比如说手机号。这些信息在某些时候，可能在源码里直接或者间接能看到。&lt;/li&gt;
  &lt;li&gt;前台可能存在一些伪静态页面，建议随手尝试注入，虽然现在这样的案例已经比较少了。&lt;/li&gt;
  &lt;li&gt;在尝试列目录漏洞时，可以在目录后面加上XSS字符尝试是否有反馈，也可以尝试不存在的文件名（hacksb.php）或者错误的文件名（~list.aspx、error.jspx）进行报错。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;端口入侵&quot;&gt;端口入侵&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;某些端口存在服务认证，如果有弱口令/默认口令，可以爆破进入。&lt;/li&gt;
  &lt;li&gt;某些WebServer在特定端口，同样存在单独的WEB登陆口，如果突破进入后台，可以部署包拿webshell，也可能执行命令反弹shell。&lt;/li&gt;
  &lt;li&gt;代理端口，这类服务比较少，如果找到可以尝试连接，设法将其作为跳板，切入内网。&lt;/li&gt;
  &lt;li&gt;特定端口开启的服务，可能存在RCE，结合MSF可以反弹shell。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;细节思考&quot;&gt;细节思考&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;测试敏感点注入时，最好多尝试几遍，加上risk、level、random-ua以及tamper，不要过于依赖扫描器或者fuzz工具。&lt;/li&gt;
  &lt;li&gt;测试XSS的时候，最好先用字符试水查看过滤情况，有时候不能直接使用payload的时候，可以尝试绕过。&lt;/li&gt;
  &lt;li&gt;上传时可能存在黑/白名单时，每次手动改包会很麻烦。最好是通过字典批量发包进行fuzz。另外，%00不管用时，可以替换成类似的空白字符段（以后补上细节）。&lt;/li&gt;
  &lt;li&gt;绕过逻辑改包，无论是上传还是找回密码，记得每步截图，也许有时候只有一次机会。&lt;/li&gt;
  &lt;li&gt;修改状态码时，也许能够通过辅助插件，自动化fuzz判断。&lt;/li&gt;
  &lt;li&gt;调用callback内容时，其内容也许可控，我们可以尝试往里面注入污染语句。最后，再尝试寻找调用callback的地方。&lt;/li&gt;
  &lt;li&gt;找后台一般有几种情况，可以是子目录，也可以是同域名下其他端口，甚至是其他子域名。比如统一SSO登录的情况下，一套网站总会有一个唯一入口。但由于历史的原因，总会遗漏和不统一的地方，那就是我们的突破口。&lt;/li&gt;
  &lt;li&gt;待续。&lt;/li&gt;
&lt;/ul&gt;</content><author><name>demon</name></author><category term="pentest" /><summary type="html">在安全测试中，我们常常会遇到一些重复性工作。而鄙人记性不佳，有时候并不能高效的进行工作。因此，自己萌生了撰写工作总结的想法，将以往的工作列清单，做一个总结，以作查阅之用。</summary></entry><entry><title type="html">域名采集爬虫</title><link href="/spider/2017/10/19/domain-spider/" rel="alternate" type="text/html" title="域名采集爬虫" /><published>2017-10-19T20:59:18+08:00</published><updated>2017-10-19T20:59:18+08:00</updated><id>/spider/2017/10/19/domain-spider</id><content type="html" xml:base="/spider/2017/10/19/domain-spider/">&lt;p&gt;
    &lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;
    前段时间在研究扫描器的问题，在涉及域名采集这块时， 突然有了一些特别的想法，所以单独将这个模块提取出来，做成了一套独立系统。
    &lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;对于域名采集技术，市面上大概有这样几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DNS域传送泄露&lt;/li&gt;
  &lt;li&gt;接口查询（包括aizhan，shodan等）&lt;/li&gt;
  &lt;li&gt;字典枚举&lt;/li&gt;
  &lt;li&gt;Github泄露&lt;/li&gt;
  &lt;li&gt;搜索引擎泄露&lt;/li&gt;
  &lt;li&gt;SSL/TLS证书泄露&lt;/li&gt;
  &lt;li&gt;DNS历史记录泄露&lt;/li&gt;
  &lt;li&gt;置换扫描枚举法&lt;/li&gt;
  &lt;li&gt;互联网自治系统号码(ASN)枚举&lt;/li&gt;
  &lt;li&gt;NSEC记录枚举&lt;/li&gt;
  &lt;li&gt;互联网项目数据集枚举&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不过这里要谈的显然不是这种，尽管自研的扫描器里已经集成了前面说的这些，今天的主角是域名采集爬虫。&lt;/p&gt;

&lt;p&gt;那么何谓域名采集爬虫呢，其实也是老概念。我们要先从一个URL或者多个URL入口进行爬取。在开始，需要设定好爬行深度和允许爬行的根域名，通过不断地迭代存取和过滤，直到爬取完所有可触及的域名为止。&lt;/p&gt;

&lt;p&gt;这样的域名采集方法，较其他技术优势在于：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;能采到不被搜索引擎收录的可用域名。&lt;/li&gt;
  &lt;li&gt;囊括范围广，除去CDN影响，能更多地去收集目标资产。&lt;/li&gt;
  &lt;li&gt;细致入微，在域名采集普及的时代，厂商一般子域名都会多加注意起名，这样做能多收获一些隐藏域名。&lt;/li&gt;
  &lt;li&gt;可分布式，相对于普通的单机大字典爆破，可以选择性节省更多的时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然讲了这么多，其劣势也很明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将耗费更多的时间和各类资源，即使采用了分布式，由于需要考虑部分目标主机的脆弱性，建立的连接数还是有限的。&lt;/li&gt;
  &lt;li&gt;容易被封，WAF和各种IDS的规则在检测到爬虫后，可能会直接封禁IP，不过这点可以通过技术手段缓解。&lt;/li&gt;
  &lt;li&gt;不易部署，相对于常用的域名接口查询和爆破，这点的劣势还是相当明显的。&lt;/li&gt;
  &lt;li&gt;比较鸡肋，没有太多Team会专门为了爬取域名单独建立一个可用的爬虫体系，一般是扫描器只兼容了传统接口。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优劣对比还是比较明显的，这里不多做探讨，下面纯技术介绍。&lt;/p&gt;

&lt;p&gt;这里的系统采用的是scrapy+django+mongodb，每采集好一批数据后，将存入mongodb，其中的数据包括：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;页面源URL
子域名
根域名

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;后续其他项目将会解析生成的所有子域名，最后再进行资产汇总计算。这些本不属于此项目，不再细说。&lt;/p&gt;

&lt;p&gt;另外，在采集过程中，爬虫会在中间件（合规的做法如此，笔者则是直接在爬虫文件里进行的过滤），根据条件过滤重复和不合规的URL。&lt;/p&gt;

&lt;p&gt;代码其实很简单，现在已有的是前台查询+后台爬虫（命令行）的形式。后期如果项目可用，会加上前台scrapyd发布任务+MMQ分布式队列缓存。&lt;/p&gt;

&lt;p&gt;另外，以后也考虑集成传统接口、泄露查询和枚举模块。&lt;/p&gt;

&lt;p&gt;至于为啥现在没有做，因为懒。&lt;/p&gt;

&lt;p&gt;代码由于还需要优化，暂时没有放出，以后会补上。&lt;/p&gt;

&lt;p&gt;待续。&lt;/p&gt;</content><author><name>demon</name></author><category term="spider" /><summary type="html">前段时间在研究扫描器的问题，在涉及域名采集这块时， 突然有了一些特别的想法，所以单独将这个模块提取出来，做成了一套独立系统。</summary></entry><entry><title type="html">爬虫去重优化</title><link href="/spider/2017/10/14/url-duplicate-removal/" rel="alternate" type="text/html" title="爬虫去重优化" /><published>2017-10-14T08:05:18+08:00</published><updated>2017-10-14T08:05:18+08:00</updated><id>/spider/2017/10/14/url-duplicate-removal</id><content type="html" xml:base="/spider/2017/10/14/url-duplicate-removal/">&lt;p&gt;
    &lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;以前在做漏洞Fuzz爬虫时，曾做过URL去重相关的工作，当时是参考了seay法师的文章以及网上零碎的一些资料，感觉做的很简单。近来又遇到相关问题，于是乎有了再次改进算法的念头。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;首先，针对URL本身的去重，可以直接对整块URL进行处理。在参考网上的一些文章时，发现它们大多做了URL压缩存储。使用这些算法在数据量较大的时候，诚然能大幅减小存储的空间：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于磁盘的顺序存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于Hash算法的存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于MD5压缩映射的存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于嵌入式Berkeley DB的存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于布隆过滤器（Bloom Filter）的存储。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而对于URL逻辑上的去重，这里先给出seay文章中的相似度去重算法，大致是下面这样的：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def urlsimilar(url):
        hash_size=199999
        tmp=urlparse.urlparse(url)
        scheme=tmp[0]
        netloc=tmp[1]
        path=tmp[2][1:]
        query=tmp[4]
        #First get tail
        if len(path.split('/'))&amp;gt;1:
            tail=path.split('/')[-1].split('.')[-1]
            #print tail
        elif len(path.split('/'))==1:
            tail=path
        else:
            tail='1'
         #Second get path_length
        path_length=len(path.split('/'))-1
        #Third get directy list except last
        path_list=path.split('/')[:-1]+[tail]
        #Fourth hash
        path_value=0
        for i in range(path_length+1):
            if path_length-i==0:
                path_value+=hash(path_list[path_length-i])%98765
            else:
                path_value+=len(path_list[path_length-i])*(10**(i+1))

        #get host hash value
        netloc_value=hash(hashlib.new(&quot;md5&quot;,netloc).hexdigest())%hash_size
        url_value=hash(hashlib.new(&quot;md5&quot;,str(path_value+netloc_value)).hexdigest())%hash_size

        return url_value
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这段函数的大概作用是，最后它会根据算法返回一个hash值，这个hash值也就是该URL的hash相似度。如果两个URL计算出的hash值最后比较相等，我们则可以判断两个URL是具有较高的相似度的。&lt;/p&gt;

&lt;p&gt;但是这个函数应该是seay举例时随手提出的（这里强调下，免得被喷，后文不再细说），只是简单做了demo，并没有进行细化检验。在比较粗糙的情况下，该算法确实能剔除一些简单的参数重复的情况，但一旦参数复杂或者url不规范，是不太能很好的进行去重的。&lt;/p&gt;

&lt;p&gt;那么在针对URL获取的过程中，我们还可以做的小优化有哪些呢？&lt;/p&gt;

&lt;h3 id=&quot;日期时间命名&quot;&gt;日期时间命名&lt;/h3&gt;
&lt;p&gt;首先，我们可以根据日期来去重。我们知道，在爬取一些Blog和和门户等系统时，经常会遇到以日期命名的目录。&lt;/p&gt;

&lt;p&gt;这些目录大概归纳起来，存在类似下面的形式：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
2010-11-11
10-11-11
20101111

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;当然，还有些文件会以时间+随机值命名，也可能是用unix时间戳命名，这些可能是根据上传和编辑时间来定义的。&lt;/p&gt;

&lt;p&gt;笔者建议是，使用redis或者memcache之类等缓存型数据库，将其直接存储;或者在数据大的时候，考虑作为临时hash set存储。&lt;/p&gt;

&lt;p&gt;比如，一旦出现日期时间命名的目录或静态文件，我们可以将其URL地址作为样本存储在数据库里，存储的内容可以是：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;目录层级
命名格式
URL地址(或hash值)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;有人可能说，在前面seay提出的那个案例里，好像是可以解决类似日期相似度的问题。那我们先看看下面的例子，此处输出仍然基于上面那个函数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print urlsimilar('http://www.baidu.com/blog/2010-10-11/')
print urlsimilar('http://www.baidu.com/blog/2010-10-13/')
print urlsimilar('http://www.baidu.com/blog/2010-9-13/')
print urlsimilar('http://www.baidu.com/whisper/2010-10-11/')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;输出结果如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;110086
110086
37294
4842

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;我们可以看到，在普通情况下，确实于相同父级目录下，相似度算法是可以判断正确的。
但是一旦日期格式不规范，或者父级目录存在一定的差异，这里是不能很好的判断的。&lt;/p&gt;

&lt;p&gt;当然，我们也可以通过机器学习来完成去重的工作。不过就简化工作而言，还是可以使用一些小Tips，根据规则匹配来做到。&lt;/p&gt;

&lt;h3 id=&quot;静态文件的去重&quot;&gt;静态文件的去重&lt;/h3&gt;

&lt;p&gt;我们知道，在爬取URL的过程中，也会遇到许多静态文件，如shtml、html、css等等。这些文件在大多数的情况下，是没有太大意义的。除非测试者倾向于使用“宁可错杀一百，绝不放过一个”的全量采集手法。&lt;/p&gt;

&lt;p&gt;这时候，我们可以配置黑名单，建立文件后缀规则库进行过滤。&lt;/p&gt;

&lt;p&gt;当然，在这些静态后缀的URL链接，也可能带上参数混淆的情况。
个人建议是，用于回调的json、xml等URL，里面可能储存敏感内容，尽量别动；其他类型的静态文件，仍然采取将参数分离的方式，最后对URL进行去重存储。&lt;/p&gt;

&lt;h3 id=&quot;特定情况的过滤&quot;&gt;特定情况的过滤&lt;/h3&gt;

&lt;p&gt;在爬取特定网站时，我们可以预先做好配置，指定过滤一些目录和页面，以节省大量时间资源。&lt;/p&gt;

&lt;p&gt;反过来，我们也可以指定只爬取指定目录下的页面，定向获取我们想要的内容。&lt;/p&gt;

&lt;h3 id=&quot;敏感页面的感知&quot;&gt;敏感页面的感知&lt;/h3&gt;

&lt;p&gt;在seay提出的demo算法中，是有一定局限的。比如我们需要在敏感目录下，尽可能多的拿到文件信息。比如我们爬取到了后台管理目录，可能会遇到下面的情况：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print urlsimilar('http://www.baidu.com/blog/admin/login.php')
print urlsimilar('http://www.baidu.com/blog/admin/manage_index.php')
print urlsimilar('http://www.baidu.com/blog/admin/test.css')

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;输出结果如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;40768
40768
40768
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;很明显有问题不是么？&lt;/p&gt;

&lt;p&gt;当然，我们可以通过对敏感页面关键词进行监控；或者也可以指定后缀文件，进行白名单监控。&lt;/p&gt;

&lt;p&gt;但是一旦这样做，而且还想采用前面的hash算法的话，大家定义的过滤函数的优先级，肯定需要大于该算法。并且，我们在这样做的过程中，也应该考虑过滤成本的问题，建议采用选择性启用。&lt;/p&gt;

&lt;h3 id=&quot;高频敏感目录的优待&quot;&gt;高频敏感目录的优待&lt;/h3&gt;

&lt;p&gt;可能在爬取的过程中，部分爬虫是并用了目录爆破的手段的。如果采用了这种手法并且匹配成功后，我们可以将该目录下的内容单独使用一份过滤规则，从而避免去重算法的误判。&lt;/p&gt;

&lt;h3 id=&quot;响应页面的过滤&quot;&gt;响应页面的过滤&lt;/h3&gt;

&lt;p&gt;对于某些网站来讲，可能有不少页面因为链接是失效的，会被冠以404页面和50x错误。另外，在无权访问的时候，可能网站会做30x跳转和403目录限制。&lt;/p&gt;

&lt;p&gt;这些页面没有实质性内容，在大多数时候是没有意义的，我们可以在配置文件里对需要爬取的这类页面做白名单，比如保留403页面，或者存取30x跳转前（后）的页面。&lt;/p&gt;

&lt;h3 id=&quot;waf警告页面过滤&quot;&gt;WAF（警告）页面过滤&lt;/h3&gt;

&lt;p&gt;某些网站可能被装上了WAF，在访问频率过快时，可能会得到一个WAF的警告页面。而在CMS本身就做了限制的情况下，会以20x的响应码展示一些没有不存在的页面。&lt;/p&gt;

&lt;p&gt;当然，我们可以通过分布式换代理的方式，去解决部分这样的问题，这里先不多做讨论。&lt;/p&gt;

&lt;p&gt;这时候，我们可以配置相应的次数阈值，如果某些页面出现的次数过多，可以将其标记为警告（WAF）页面，进而做出过滤处理。这里对某页面的识别，可以通过黑名单关键字标记，或者计算页面hash值，比如下面这样：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;content = urllib2.urlopen('http://www.test.com/').read()
md5_sum = hashlib.md5()
md5_sum.update(content)
print md5_sum.hexdigest()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当然，我们在实际计算页面hash值和取关键字（做分词或黑名单）时，也可能由于反爬虫机制的存在（如添加随机值），需要适时调整相似度来计算hash值或者采用其他手段。当然这也会消耗更多的时间和机器资源。但某些特定的情况下，可能也会带来意想不到的收获。&lt;/p&gt;

&lt;h3 id=&quot;无意义参数页面去重&quot;&gt;无意义参数页面去重&lt;/h3&gt;

&lt;p&gt;我们在采集页面的过程中，同样有可能会遇到一些毫无意义的、高频出现的、多参数页面。这类页面可能是回调页面，也可能是临时渲染的随机页面。&lt;/p&gt;

&lt;p&gt;在这里，大家可以通过前面处理WAF（警告）的方法进行过滤。当然，使用前面的hash算法也是可以应对大部分情况的。毕竟网站的这类的URL有限，不必为了几种特型去消耗更多的资源，这样得不偿失。&lt;/p&gt;

&lt;h3 id=&quot;js代码中的url&quot;&gt;JS代码中的URL&lt;/h3&gt;

&lt;p&gt;在我们提取js代码，也就是遇到ajax之类的交互情况时，可能会遇到需要拼接的GET请求，或者直接可以取用的POST请求。&lt;/p&gt;

&lt;p&gt;这类的URL地址，最好是结合phantomjs等webkit，更方便地进行动态拼接。&lt;/p&gt;

&lt;p&gt;它们会显得比较特殊，可能仅仅返回状态码，也可能会返回实质性的敏感内容。这种情况，就需要根据爬取者的要求，对爬取的过滤规则进行适应性调整。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;这里旨在提出一些对相似URL去重的小优化，可能效果有限，也可能存在未尽人意之处。欢迎大家提出建议，诸君共勉。&lt;/p&gt;

&lt;h3 id=&quot;参考文章&quot;&gt;参考文章&lt;/h3&gt;

&lt;p&gt;如何避免重复抓取同一个网页
https://segmentfault.com/q/1010000002664904&lt;/p&gt;

&lt;p&gt;浅谈动态爬虫与去重
http://bobao.360.cn/learning/detail/3391.html&lt;/p&gt;

&lt;p&gt;网络爬虫：URL去重策略之布隆过滤器(BloomFilter)的使用
http://blog.csdn.net/lemon_tree12138/article/details/47973715&lt;/p&gt;

&lt;p&gt;实用科普：爬虫技术浅析 编写爬虫应注意的点
http://www.cnseay.com/?p=4102&lt;/p&gt;

&lt;p&gt;网络爬虫 (spider) URL消重设计 URL去重设计
http://woshizn.iteye.com/blog/532605&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">以前在做漏洞Fuzz爬虫时，曾做过URL去重相关的工作，当时是参考了seay法师的文章以及网上零碎的一些资料，感觉做的很简单。近来又遇到相关问题，于是乎有了再次改进算法的念头。</summary></entry><entry><title type="html">迁移博客</title><link href="/jekyll/update/start/2017/09/02/welcome-to-jekyll/" rel="alternate" type="text/html" title="迁移博客" /><published>2017-09-02T08:05:18+08:00</published><updated>2017-09-02T08:05:18+08:00</updated><id>/jekyll/update/start/2017/09/02/welcome-to-jekyll</id><content type="html" xml:base="/jekyll/update/start/2017/09/02/welcome-to-jekyll/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;最近原来的空间服务里到期了，钱包吃紧又想diy，最后就选择了转入Github page。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;在转入时遇到了许多坑，尤其是在ruby gem安装时老遇到依赖问题，下载没代理也老卡住。&lt;/p&gt;

&lt;p&gt;好不容易弄好后，本来想自己改个简单的主题将就用下。结果又遇到分页和一系列插件配置问题，搞得头都大了。&lt;/p&gt;

&lt;p&gt;作为一名老年选手差点崩溃，最后无奈下了个Jekyll主题，稍微改下就全部搞定了。&lt;/p&gt;

&lt;p&gt;好吧，以前用CMS差点被人搞，这次全静态，could you tell me how to play with it?&lt;/p&gt;

&lt;p&gt;If you can…&lt;/p&gt;

&lt;p&gt;Dalao,  ball dai fly…&lt;/p&gt;</content><author><name>demon</name></author><category term="start" /><summary type="html">最近原来的空间服务里到期了，钱包吃紧又想diy，最后就选择了转入Github page。</summary></entry><entry><title type="html">Selenium+phantomjs刷量</title><link href="/spider/2016/11/12/seleniumphantomjs-e5-88-b7-e9-87-8f/" rel="alternate" type="text/html" title="Selenium+phantomjs刷量" /><published>2016-11-12T21:52:00+08:00</published><updated>2016-11-12T21:52:00+08:00</updated><id>/spider/2016/11/12/seleniumphantomjs%E5%88%B7%E9%87%8F</id><content type="html" xml:base="/spider/2016/11/12/seleniumphantomjs-e5-88-b7-e9-87-8f/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;最近有这方面的需求，要帮人在某网站上刷点击量。由于该网站是一家比较知名的大型门户，反作弊机制肯定是有的，所以突发奇想用Selenium试试。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
	与Selenium类似的东西有lxml，它们采用正则xpath路径匹配标签会多一些。当然，也有人会使用beautifulsoup4去解析网页结构，最后再得到需要的标签。达成目的的路不止一条，这点不再赘述。
&lt;/p&gt;

&lt;p&gt;
	本来想用Selenium随便找个浏览器模拟人的浏览网页行为，结果每次需要重新打开一次浏览器，几乎让人抓狂。最后还是采用了phantomjs，这是一种后台浏览器，同样满足模拟行为。
&lt;/p&gt;

&lt;p&gt;
	这里提一下，Selenium需要跟浏览器版本的更新度一致。本来我采用的firefox28.0+Selenium3.01,结果踩了半天坑没找到原因。最后，将火狐更新到最新版47.01才解决。
&lt;/p&gt;

&lt;p&gt;
	像这类调度，我发现的大概需要注意的有两点：
&lt;/p&gt;

&lt;p&gt;
	第一：他们都需要有个浏览器调用的中间件【windows下举例】：
&lt;/p&gt;

&lt;p&gt;
	&lt;span style=&quot;color:#FF0000;&quot;&gt;比如phantomjs.exe（同类的有chromedriver.exe【谷歌】、geckodriver.exe【火狐】），这些需要放在python环境的script目录下。&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
	第二：浏览器需要装在默认目录：
&lt;/p&gt;

&lt;p&gt;
	&lt;span style=&quot;color:#FF0000;&quot;&gt;当然在浏览器目录添加系统环境变量（PATH）也是可以的，这个不是默认添加的，否则会报错。&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
	&lt;span style=&quot;color:#000000;&quot;&gt;完成了这些工作以后，我们就可以后台对&lt;/span&gt;Selenium进行调度了，另外phantomjs具有JS特性，自然也是可以对后台加载的网页进行一些诸如定位、拖动滚动条之类的操作。
&lt;/p&gt;

&lt;p&gt;
	比如下面这段代码片段【案例来自于网络】：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
&amp;nbsp; &amp;nbsp; driver.get(pageURL)&amp;nbsp;
&amp;nbsp; &amp;nbsp; js1 = &amp;#39;return document.body.scrollHeight&amp;#39;
&amp;nbsp; &amp;nbsp; js2 = &amp;#39;window.scrollTo(0, document.body.scrollHeight)&amp;#39;
&amp;nbsp; &amp;nbsp; old_scroll_height = 0
&amp;nbsp; &amp;nbsp; while(driver.execute_script(js1) &amp;gt; old_scroll_height):
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; old_scroll_height = driver.execute_script(js1)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; driver.execute_script(js2)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; time.sleep(1) &amp;nbsp;&lt;/pre&gt;

&lt;p&gt;
	我们刷流量嘛，自然需要代理。这个可以在网上买，大概5块几千个没问题，不保证稳定性。当然，你去其他免费代理网站爬下来也是可以的。
&lt;/p&gt;

&lt;p&gt;
	设置代理的法子大概写一下【案例来自于网络】：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
service_args = [
 &amp;#39;--proxy=127.0.0.1:9999&amp;#39;,
 &amp;#39;--proxy-type=socks5&amp;#39;,
 ]
driver = webdriver.PhantomJS(&amp;#39;../path_to/phantomjs&amp;#39;,service_args=service_args)&lt;/pre&gt;

&lt;p&gt;
	UA在放在list池子里，需要的时候自行启用：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
from random import choice&lt;/pre&gt;

&lt;p&gt;
	header里设置UA【案例来自于网络】：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
from selenium import webdriver
from selenium.webdriver import DesiredCapabilities
driver=webdriver.PhantomJS(executable_path=&amp;#39;存放路径\phantomjs.exe&amp;#39;)
desired_capabilities= DesiredCapabilities.PHANTOMJS.copy()
headers = {&amp;#39;Accept&amp;#39;: &amp;#39;*/*&amp;#39;,
&amp;#39;Accept-Language&amp;#39;: &amp;#39;en-US,en;q=0.8&amp;#39;,
&amp;#39;Cache-Control&amp;#39;: &amp;#39;max-age=0&amp;#39;,
&amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36&amp;#39;,#这种修改 UA 也有效
&amp;#39;Connection&amp;#39;: &amp;#39;keep-alive&amp;#39;
&amp;#39;Referer&amp;#39;:&amp;#39;http://www.baidu.com/&amp;#39;
}
for key, value in headers.iteritems():
    desired_capabilities['phantomjs.page.customHeaders.{}'.format(key)] = value
desired_capabilities['phantomjs.page.customHeaders.User-Agent'] = &amp;#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&amp;#39;
driver= webdriver.PhantomJS(desired_capabilities=desired_capabilities)
driver.get(&amp;quot;http://www.myip.cn/judge.php&amp;quot;)
print driver.page_source&lt;/pre&gt;

&lt;p&gt;
	其他涉及具体细节的，这里就不多谈了，网上很多。
&lt;/p&gt;

&lt;p&gt;
	我这里也是临时用下，没有太高深的技术。希望对某些朋友有帮助，Good luck!
&lt;/p&gt;

&lt;p&gt;
	&lt;strong&gt;&lt;span style=&quot;color:#FFD700;&quot;&gt;[转载请注明来源&lt;/span&gt;&lt;a href=&quot;http://www.dawner.info/&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color:#FFD700;&quot;&gt;本站&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;color:#FFD700;&quot;&gt;,谢谢。]&lt;/span&gt;&lt;/strong&gt;
&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">最近有这方面的需求，要帮人在某网站上刷点击量。由于该网站是一家比较知名的大型门户，反作弊机制肯定是有的，所以突发奇想用Selenium试试。</summary></entry><entry><title type="html">看我如何从邮箱附件的逆向分析到揪出黑客源头</title><link href="/translate/2016/07/06/e7-9c-8b-e6-88-91-e5-a6-82-e4-bd-95-e4-bb-8e-e9-82-ae-e7-ae-b1-e9-99-84-e4-bb-b6-e7-9a-84-e9-80-86-e5-90-91-e5-88-86-e6-9e-90-e5-88-b0-e6-8f-aa-e5-87-ba-e9-bb-91-e5-ae-a2-e6-ba-90-e5-a4-b4/" rel="alternate" type="text/html" title="看我如何从邮箱附件的逆向分析到揪出黑客源头" /><published>2016-07-06T08:55:00+08:00</published><updated>2016-07-06T08:55:00+08:00</updated><id>/translate/2016/07/06/%E7%9C%8B%E6%88%91%E5%A6%82%E4%BD%95%E4%BB%8E%E9%82%AE%E7%AE%B1%E9%99%84%E4%BB%B6%E7%9A%84%E9%80%86%E5%90%91%E5%88%86%E6%9E%90%E5%88%B0%E6%8F%AA%E5%87%BA%E9%BB%91%E5%AE%A2%E6%BA%90%E5%A4%B4</id><content type="html" xml:base="/translate/2016/07/06/e7-9c-8b-e6-88-91-e5-a6-82-e4-bd-95-e4-bb-8e-e9-82-ae-e7-ae-b1-e9-99-84-e4-bb-b6-e7-9a-84-e9-80-86-e5-90-91-e5-88-86-e6-9e-90-e5-88-b0-e6-8f-aa-e5-87-ba-e9-bb-91-e5-ae-a2-e6-ba-90-e5-a4-b4/">&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;这个故事要从一次垃圾邮件攻击事件说起，下图是一个笔者从某封垃圾邮件里提取的可疑附件。至于下面这蹩脚的英语，这也是值得我们注意的地方。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;上图的附件使用了“.doc”作为后缀，但它其实是RTF（富文本）格式的文件。该文件包含了一个针对性的RTF栈溢出exp，它利用了CVE-2010-3333，也就是在微软Word RTF解析器在处理pFragments时会产生的一个漏洞。然而，该漏洞在五年前就已经修补了。&lt;/p&gt;

&lt;p&gt;正如你在上图中看到的那样，该exp和shellcode做了混淆来逃避杀软的检测。经过各种提取整理解密之后，笔者发现该shellcode会从volafile.io上面下载文件来执行。&lt;/p&gt;

&lt;p&gt;下载下来的这个文件是微软.net Win32可执行文件，简单hex dump了这个文件，笔者发现了HawkEyekeylogger字符串。&lt;/p&gt;

&lt;p&gt;在谷歌后发现，它指引笔者找到了开发该键盘记录器的官网。在网站里，他们列出了该键盘记录器一些的特性。&lt;/p&gt;

&lt;p&gt;在笔者动态的分析中，该键盘记录器会把自身复制一份到Application Data（%appdata%）文件夹，并且将复制后的文件命名为WindowsUpdate.exe。同时，它在注册表里设置了开机启动，以实现其持续性攻击。&lt;/p&gt;

&lt;p&gt;并且，它还会在受感染的系统里释放以下文件：&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;%Temp%\Sysinfo.txt – 释放的恶意软件exe路径&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;%Appdata%\pid.txt –恶意软件进程ID&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;%Appdata%\pidloc.txt – 恶意软件进程exe路径&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;接着，笔者观察到该键盘记录器试图去checkip.dyndns.com，获取受感染系统的外网IP。这个合法的网站经常被恶意软件利用，拿来确定受感染系统的IP地址。&lt;/p&gt;

&lt;p&gt;过了一会儿，笔者监控到了SMTP流量，发现了受感染系统发送信息给黑客email的动作。&lt;/p&gt;

&lt;p&gt;里面的信息可能包括：&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;计算机名&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;本地日期和时间&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;系统语言&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;操作系统&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;平台&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;操作系统版本&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;内存&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;.net框架&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;系统权限&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;默认浏览器&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;防火墙&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;内网IP地址&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;外网IP地址&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;恢复邮件设置和密码&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;恢复浏览器和FTP密码&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;正如前面笔者提到的，这款键盘记录器是由.net编译的。所以，笔者接下来需要反编译这个可执行文件。笔者使用了一个开源的.net反编译工具&lt;a href=&quot;https://github.com/icsharpcode/ILSpy&quot;&gt;ILSpy&lt;/a&gt;来完成这个任务。&lt;/p&gt;

&lt;p&gt;笔者反编译出了源代码，并将其与官网的特性列表进行比较，结果表明是完全符合的。笔者发现其代码有以下的特点：&lt;/p&gt;

&lt;p&gt;一个剪贴板记录器&lt;/p&gt;

&lt;p&gt;一个浏览器，FTP和邮件客户端密码记录器。它也会去尝试窃取密码管理器证书和windows密钥。&lt;/p&gt;

&lt;p&gt;蠕虫类的USB感染程序，可以让记录器感染扩散到其他windows机器。&lt;/p&gt;

&lt;p&gt;它也针对一些Steam游戏平台的用户，通过删除配置和登录数据文件，用户会强制再次登录。这就给了键盘记录器窃取用户Steam认证的可乘之机。&lt;/p&gt;

&lt;p&gt;窃取的信息里包括桌面截图，它们会被发送到黑客的邮箱，或者键盘记录器里配好的FTP服务器上。&lt;/p&gt;

&lt;p&gt;黑客貌似也会配置键盘记录器，通过HTTP将窃取的信息上传到PHP服务器上。但是奇怪的是，这里的代码留空了。&lt;/p&gt;

&lt;p&gt;笔者在反编译时，发现最有趣的是一个C#的构造函数Form1()。这是键盘记录器储存配置的地方，但是为了确保黑客电子邮件地址和FTP登录凭证的安全，它们使用了Rijndael算法和Base64加密。&lt;/p&gt;

&lt;p&gt;但是我们知道，这些加密的数据并不一定安全，特别是解密的部分写在了笔者能够反编译的代码里。&lt;/p&gt;

&lt;p&gt;下面这张图是Decrypt（解密）方法，它会接收两个字符串参数：encryptedBytes和secretKey。这个安全密钥恰好是硬编码字符串HawkSpySoftwares。&lt;/p&gt;

&lt;p&gt;正如提到的那样，该键盘记录器使用了Rijndael算法，安全密钥用了Unicode字符串“099u787978786”进行加盐，也是硬编码。&lt;/p&gt;

&lt;p&gt;处于好奇，笔者复制了这部分代码，简单修改适应后，在MS Visual Studio里面去进行编译。当然，最后笔者应该是解密成功了（待验证）。&lt;/p&gt;

&lt;p&gt;最后，笔者拿着邮箱认证信息去登陆尝试。&lt;/p&gt;

&lt;p&gt;这些似乎是感染系统上的电子邮件地址。所以笔者检查了邮件设置，结果发现了意外之喜！发送到这个邮箱的电子邮件会自动转发到黑客的Gmail账户里。你可以在下面截图里看到黑客的Gmail地址。&lt;/p&gt;

&lt;p&gt;也许黑客知道HawkEye容易被破解，所以为了保护他们自己的电子邮件认证信息，就劫持了一个无辜的电子邮件账户作为初始的接收器，最后它会把收到的内容统统转给黑客的真实电子邮件地址。&lt;/p&gt;

&lt;p&gt;最终，笔者把受害的电子邮件帐户还给了失主，并为他们修改了密码，移除了黑客的电子邮件重定向设置。&lt;/p&gt;

&lt;p&gt;如文中所写的，笔者也收到了包含CVE-2012-0158的exp附件，里面是同一款键盘记录器，但是却配置了另一个电子邮箱账户作为窃取数据的初始接收邮箱。&lt;/p&gt;

&lt;p&gt;攻击中的这两个漏洞虽然已经比较早了，但是仍广泛用于电子邮件攻击之中，这里建议读者更新好补丁，使用好相应的杀软，从而防御黑客的攻击。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;strong&gt;[参考来源&lt;a style=&quot;color: #ff0000;&quot; href=&quot;https://www.trustwave.com/Resources/SpiderLabs-Blog/How-I-Cracked-a-Keylogger-and-Ended-Up-in-Someone-s-Inbox/&quot; target=&quot;_blank&quot;&gt;trustwave&lt;/a&gt;，转载请注明本站翻译]&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">这个故事要从一次垃圾邮件攻击事件说起，下图是一个笔者从某封垃圾邮件里提取的可疑附件。至于下面这蹩脚的英语，这也是值得我们注意的地方。</summary></entry><entry><title type="html">Asacub：从间谍软件到银行木马</title><link href="/translate/2016/01/25/asacub-ef-bc-9a-e4-bb-8e-e9-97-b4-e8-b0-8d-e8-bd-af-e4-bb-b6-e5-88-b0-e9-93-b6-e8-a1-8c-e6-9c-a8-e9-a9-ac/" rel="alternate" type="text/html" title="Asacub：从间谍软件到银行木马" /><published>2016-01-25T00:17:00+08:00</published><updated>2016-01-25T00:17:00+08:00</updated><id>/translate/2016/01/25/asacub:%E4%BB%8E%E9%97%B4%E8%B0%8D%E8%BD%AF%E4%BB%B6%E5%88%B0%E9%93%B6%E8%A1%8C%E6%9C%A8%E9%A9%AC</id><content type="html" xml:base="/translate/2016/01/25/asacub-ef-bc-9a-e4-bb-8e-e9-97-b4-e8-b0-8d-e8-bd-af-e4-bb-b6-e5-88-b0-e9-93-b6-e8-a1-8c-e6-9c-a8-e9-a9-ac/">&lt;p&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;strong&gt;我们最近分析了一个系列银行木马Trojan-Banker.AndroidOS.Asacub，发现了其中在用的一台CC服务器chugumshimusona.com，也在为一款名为CoreBot的Windows木马所使用,这让我们起了对这款移动端银行木马进行分析的心思。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;!--more--&gt;
&lt;p&gt;据我们所知，最初版本的Asacub木马出现在2015年6月。与其说当时的Asacub是银行恶意软件，不如说它是一款木马。早期的Asacub会窃取用户收到的所有短信，并上传到黑客的服务器。这款木马能够从CC服务器接收和处理下面的命令：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;get_history: 将浏览器历史上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_contacts: 将联系人列表上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_listapp: 将安装的应用列表上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; block_phone: 锁屏。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; send_sms: 发送带有指定内容的短信到指定号码。&lt;/span&gt;
而新版本的Asacub出现在2015年7月，这款恶意软件在接口使用了欧洲银行的logo，取代了早期版本的美国银行的logo。
与此同时，它可以执行的命令数量也有了很大的增长：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;get_sms: 将所有短信上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; del_sms: 删掉指定的短信。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; set_time: 为CC服务器的联系设定新的时间间隔。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_time: 指定CC目标与CC服务器联系的时间间隔。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; mute_vol: 静默电话模式。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; start_alarm: 启用设备白屏处理器继续运行模式。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; stop_alarm: 禁用设备白屏处理器继续运行模式。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; block_phone: 锁屏。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; rev_shell: 反弹shell执行命令。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; intercept_start: 启用短信拦截。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; intercept_stop: 关闭短信拦截。&lt;/span&gt;
上述的远程命令执行（反弹shell）功能，其实对这类恶意软件的来讲是不太正常的。在接收到命令后，木马会主动将远程服务器接入肉鸡设备的控制台，以便黑客在设备上执行命令和获取输出的结果。这个功能是典型的后门功能，我们其实很少发现银行类恶意软件会使用它。因为大多数银行类恶意软件，旨在从受害者银行账户里窃取资金，而不是控制设备本身。
最新版本的Asacub出现在2015年9月之后，这里的功能比起早期版本来更加关注窃取银行的敏感信息。早期版本只是使用了银行的logo图标，新版本中我们则发现了一些带着银行logo的钓鱼页面：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;alignnone&quot; src=&quot;https://cdn.securelist.com/files/2016/01/blog_corebot_1nn-768x698.jpg&quot; alt=&quot;&quot; width=&quot;768&quot; height=&quot;698&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该木马名叫“ActivityVTB24”。这听起来似乎是一家俄罗斯的大型银行，但是其却自称为乌克兰银行。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;alignnone&quot; src=&quot;https://cdn.securelist.com/files/2015/12/blog_corebot_2-768x683.jpg&quot; alt=&quot;&quot; width=&quot;768&quot; height=&quot;683&quot; /&gt;
自去年9月Asacub改版以来，钓鱼窗口出现在所有变种之中，但是其中只有银行卡的输入框可用。这意味着黑客可能只攻击他们所使用银行的客户，当然也可能这只是其中一个版本。
在启动后，“秋日版本（autumnal version）”的木马就开始窃取短信，并且还能执行下面的命令：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;get_history: 上传浏览器历史记录到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_contacts: 上传联系人列表到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_cc: 弹出钓鱼窗口来窃取银行卡数据。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_listapp: 上传已安装程序列表到服务器。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; change_redir: 转发所有来电到指定手机号码。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; block_phone: 锁屏。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; send_ussd: 运行指定的USSD请求。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; update:下载指定的文件并安装。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; send_sms: 短信发送指定内容到指定号码。&lt;/span&gt;
虽然目前我们并没有注意到有美国用户受到它的攻击，但是黑客对美国银行logo的使用应该是个危险的信号。该木马正在迅速发展，随时有新的特性可能会激活，然后添加进木马里。
&lt;strong&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;今天的Asacub&lt;/span&gt;&lt;/strong&gt;
2015年末，我们发现了一个新的Asacub，它增添了下面的命令：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;GPS_track_current – 获取设备的坐标定位，发送给攻击者。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; camera_shot – 使用设备的相机拍照。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; network_protocol – 目前我们不知道它有任何用处，但应该在未来会和CC服务器产生交互。&lt;/span&gt;
这些变种里没有钓鱼功能，但代码中涉及到了银行关键词。有意思的是，该木马一直试图关闭乌克兰银行的官方应用：
&lt;img class=&quot;alignnone&quot; src=&quot;https://cdn.securelist.com/files/2016/01/blog_corebot_3.png&quot; alt=&quot;&quot; width=&quot;838&quot; height=&quot;207&quot; /&gt;
此外，我们还分析了该木马和CC服务器的通信，它似乎对俄罗斯手机银行服务特别感兴趣，
在新年假期，新的改动在俄罗斯通过短信疯狂传播。短短一个星期内，从2015.12.28到2016.01.04，我们已发现6500名感染的用户，该木马由此跻身最活跃的恶意程序TOP5.。Asacub改版后发展的速度才有所减缓，我们将继续跟踪这类恶意软件。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[参考来源&lt;a href=&quot;https://securelist.com/blog/research/73211/the-asacub-trojan-from-spyware-to-banking-malware/&quot; target=&quot;_blank&quot;&gt;securelist&lt;/a&gt;，转载请注明本站翻译]&lt;/strong&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">我们最近分析了一个系列银行木马Trojan-Banker.AndroidOS.Asacub，发现了其中在用的一台CC服务器chugumshimusona.com，也在为一款名为CoreBot的Windows木马所使用,这让我们起了对这款移动端银行木马进行分析的心思。</summary></entry><entry><title type="html">python字符编码处理</title><link href="/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/2015/08/04/python-e5-ad-97-e7-ac-a6-e7-bc-96-e7-a0-81-e5-a4-84-e7-90-86/" rel="alternate" type="text/html" title="python字符编码处理" /><published>2015-08-04T18:58:00+08:00</published><updated>2015-08-04T18:58:00+08:00</updated><id>/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/2015/08/04/python%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E5%A4%84%E7%90%86</id><content type="html" xml:base="/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/2015/08/04/python-e5-ad-97-e7-ac-a6-e7-bc-96-e7-a0-81-e5-a4-84-e7-90-86/">&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;最近因业务需求在写爬虫时，遇到点编码的问题，加上以前曾被类似问题困扰过，特此记录一下。&lt;/span&gt;&lt;/strong&gt;
由于开发和使用环境常在Linux和Win下切换，常遇到字符处理错误，总结一些问题如下：&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;1.如何中文匹配网页内容，而不会被各种编码扰乱&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;s=&quot;编码&quot;
rs=unicode(s, &quot;utf8&quot;)
u'\u7f16\u7801'&lt;/pre&gt;
&lt;p&gt;很简单普遍的做法，unicode编码能很好的转码中文，平时储存中文字符串时可以：&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;x=u'编码'&lt;/pre&gt;
&lt;p&gt;但是这有可能会报错，python在字符处理时忒蛋疼，怎么办？&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;reload(sys)
sys.setdefaultencoding('utf-8')&lt;/pre&gt;
&lt;p&gt;又是非常简单而万精油的两句代码，默认把文件编码设成utf-8，在这时光是在代码头部写上#coding=utf-8之类的是不太管用的。
&lt;span style=&quot;color: #ff0000;&quot;&gt;2.网上摘抄一段内容，也是自己遇到过的，关于把文字直接解码。&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;Traceback (most recent call last):
File &quot;ChineseTest.py&quot;, line 3, in 
print open(&quot;Test.txt&quot;).read().decode(&quot;utf-8&quot;)
UnicodeEncodeError: 'gbk' codec can't encode character u'\ufeff' in position 0: illegal multibyte sequence&lt;/pre&gt;
&lt;p&gt;原来，某些软件，如notepad，在保存一个以UTF-8编码的文件时，会在文件开始的地方插入三个不可见的字符（0xEF 0xBB 0xBF，即BOM）。
因此我们在读取时需要自己去掉这些字符，python中的codecs module定义了这个常量：&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;# coding=gbk
import codecs
data = open(&quot;Test.txt&quot;).read()
if data[:3] == codecs.BOM_UTF8:
data = data[3:]
print data.decode(&quot;utf-8&quot;)
结果：abc中文&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;3.在Win下保存文件时，注意下保存文件的编码，鄙人一般选UTF8，默认的ANSI和UNICODE感觉不太好用。里面的内容换到Linux下容易报错。&lt;/span&gt;
&lt;span style=&quot;color: #ff0000;&quot;&gt;4.编码和解码，这样的姿势也是可以的：&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;x=r'\u7f16\u7801'
print x.decode(&quot;unicode_escape&quot;)
编码&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;
5.话不多说看图，中文字符串的比较:&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;'编码'.decode('utf-8') == u'编码'
True

'编码' == u'编码'
__main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal
False

unicode('编码') == u'编码'
Traceback (most recent call last):
  File &quot;&quot;, line 1, in 
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe7 in position 0: ordinal not in range(128)&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;6.附上一个链接，本文上述的问题里面多有讲述，但鄙人没有一一验证过，但确实很详细。&lt;/span&gt;
&lt;span style=&quot;color: #ff00ff;&quot;&gt;&lt;a title=&quot;Python、Unicode和中文&quot; href=&quot;http://blog.csdn.net/summerhust/article/details/6654150&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff00ff;&quot;&gt;Python、Unicode和中文&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;文章是记录型，很简单的东西，不喜勿喷。
&lt;span style=&quot;color: #00B050;&quot;&gt;[转载请注明来源&lt;a href=&quot;http://www.dawner.info&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;本站&lt;/span&gt;&lt;/a&gt;,谢谢。]&lt;/span&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">最近因业务需求在写爬虫时，遇到点编码的问题，加上以前曾被类似问题困扰过，特此记录一下。 由于开发和使用环境常在Linux和Win下切换，常遇到字符处理错误，总结一些问题如下：</summary></entry><entry><title type="html">Ubuntu更新显卡驱动失败解决办法</title><link href="/system/2014/10/13/ubuntu-e6-9b-b4-e6-96-b0-e6-98-be-e5-8d-a1-e9-a9-b1-e5-8a-a8-e5-a4-b1-e8-b4-a5-e8-a7-a3-e5-86-b3-e5-8a-9e-e6-b3-95/" rel="alternate" type="text/html" title="Ubuntu更新显卡驱动失败解决办法" /><published>2014-10-13T00:32:00+08:00</published><updated>2014-10-13T00:32:00+08:00</updated><id>/system/2014/10/13/ubuntu%E6%9B%B4%E6%96%B0%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95</id><content type="html" xml:base="/system/2014/10/13/ubuntu-e6-9b-b4-e6-96-b0-e6-98-be-e5-8d-a1-e9-a9-b1-e5-8a-a8-e5-a4-b1-e8-b4-a5-e8-a7-a3-e5-86-b3-e5-8a-9e-e6-b3-95/">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;前段时间因为做项目，一直整夜整夜地挂机爬站，笔记本用成台式机的负荷也是醉了-_-&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;由此出现了一个问题，鄙人笔记本物理机装的Ubuntu13.04，在挂机或者待机一段时间不能恢复登录状态，一直黑屏。对于这种情况本屌只有一个劲儿的敲键盘试图唤醒，结果只能弹出一个纯命令行，切回图形界面后控制面板却没了。&lt;/p&gt;

&lt;p&gt;于是乎，本屌在网上查了下，似乎是因为显卡驱动的原因，好嘛，那就更新显卡驱动，在Ubuntu软件更新器里面（图形化的）点了一个最新版的，兴冲冲地重启然后登陆，啪，尼玛，黑屏。。不死心再来一遍，还是那样，差点就泪奔了，这项目还没做完呢，一会儿搞半天没时间了咋办  T_T&lt;/p&gt;

&lt;p&gt;找大牛求助，指点去Ubuntu社区翻了翻，找出一篇&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?t=217062&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;帖子&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，说是可以恢复模式切回原来的状态，重启看了下，似乎需要安装盘，我去，屌丝表示没准备Ubuntu的安装盘，这条果断Pass。&lt;/p&gt;

&lt;p&gt;接着找到一篇&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?f=94&amp;amp;t=140531&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;帖子&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，让本屌在错误日志里面，找到安装新显卡驱动时的报错，查了下并没有出错，估计只是不兼容？然后在安装日志里试图直接进行卸载，未果。&lt;/p&gt;

&lt;p&gt;网上还有坑爹&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;a href=&quot;http://blog.csdn.net/crazyboy2009/article/details/8232158&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;教程&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;让直接卸载显卡驱动，然后后来就没后文了，尼玛，当本屌傻麽，直接卸载了不管依赖关系想重装系统麽，最好别酱紫做！&lt;/p&gt;

&lt;p&gt;后来本屌的解决办法是通过&lt;strong&gt;&lt;span style=&quot;color: #ff00ff;&quot;&gt;&lt;a href=&quot;http://jishu.zol.com.cn/140483.html&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff00ff;&quot;&gt;更新软件列表源&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/strong&gt;，搞一个叫Paa的玩意儿，将所有的包更新到了最新版本（还是13.x），然后才解决了。&lt;/p&gt;

&lt;p&gt;附上：&lt;span style=&quot;color: #ff00ff;&quot;&gt;&lt;strong&gt;sudo apt-get dist-upgrade&lt;/strong&gt;&lt;/span&gt;（里面的上两条其实不用）&lt;/p&gt;

&lt;p&gt;顺便附上一个没有成功的&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?f=42&amp;amp;p=2973310&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;案例&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，以及后来看到的一个复杂&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?t=384333&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;例子&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;。&lt;/p&gt;

&lt;p&gt;现在暂时用着笔记本还没出问题，如果再出现相应情况，打算试试这个&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a href=&quot;http://zhidao.baidu.com/link?url=rBeSSVPGK3lwAQVX3Rlj6oHY6_8bUiz-oY8CsIJAs4gTHg747YRiG8OVxSU21jS0GX7DqC4XVDO1EKlvBZViHK&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;案例&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，似乎是啥笔记本模式，看起来有点靠谱。其实最好的法子就是设置电源选项让它不挂起，不过在公司有闲杂人员出入，也只能将就了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;还有，大家没事不要乱更新卸载驱动之类的，坑爹的一B。。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;===================&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;Enjoy yourself！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">前段时间因为做项目，一直整夜整夜地挂机爬站，笔记本用成台式机的负荷也是醉了-_-  </summary></entry><entry><title type="html">对于python框架Scrapy+Gevent的研究</title><link href="/other/2014/09/20/e5-af-b9-e4-ba-8epython-e6-a1-86-e6-9e-b6scrapygevent-e7-9a-84-e7-a0-94-e7-a9-b6/" rel="alternate" type="text/html" title="对于python框架Scrapy+Gevent的研究" /><published>2014-09-20T21:26:00+08:00</published><updated>2014-09-20T21:26:00+08:00</updated><id>/other/2014/09/20/%E5%AF%B9%E4%BA%8Epython%E6%A1%86%E6%9E%B6scrapygevent%E7%9A%84%E7%A0%94%E7%A9%B6</id><content type="html" xml:base="/other/2014/09/20/e5-af-b9-e4-ba-8epython-e6-a1-86-e6-9e-b6scrapygevent-e7-9a-84-e7-a0-94-e7-a9-b6/">&lt;p&gt;&lt;span style=&quot;color: #00ffff;&quot;&gt;&lt;strong&gt;没错，标题党君又来了！文章只是做些第三方评论，不喜勿喷。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;前几天看到&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a title=&quot;Freebuf&quot; href=&quot;http://www.freebuf.com/tools/43194.html&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;FB上的一篇文章&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，是将用Scrapy爬虫框架加以关键词词尝试，从而将可用的关键词与相应的URL返回存到结果里，个人感觉还是有比较大的改进空间的。覆盖攻击向量字段如下：&lt;/p&gt;

&lt;p&gt;Http头中的Referer字段
User-Agent字段
Cookie
表单（包括隐藏表单）
URL参数
RUL末尾，如 www.example.com/&amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;
跳转型XSS&lt;/p&gt;

&lt;p&gt;由于英文说明书的原作者说该Scrapy的XSS延伸版不能进行Ajax判断，还是有点小遗憾，希望日后改进。&lt;/p&gt;

&lt;p&gt;感觉其提供的测试页面爬下来效果不错，我这儿没有图床可用，就不展示了。（&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;话说有朋友可以给鄙人一免费图床地址么，2333333&lt;/span&gt;&lt;/strong&gt;）&lt;/p&gt;

&lt;p&gt;附上该Git的地址：&lt;a title=&quot;xsscrapy&quot; href=&quot;https://github.com/DanMcInerney/xsscrapy&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;下载&lt;/span&gt;&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;还有就是，本人测试用的是Ubuntu13.04，作者建议pip安装，鄙人使用自带的安装包pip，表示有不少问题，后来在网上下了一个&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a title=&quot;pip1.5&quot; href=&quot;https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;1.5版本&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;的才成功。此外，个人感觉apt-get安装确实挺给力，除了没有的包以外，基本很难报错（知道肯定有人吐槽这B说的不是废话么^_^，以前一个朋友就挺爱骂我SB，不过可惜再也没能见到他了）。&lt;/p&gt;

&lt;p&gt;PS：里面所需的BeautifulSoup最好用BeautifulSoup3.2.1，BeautifulSoup4.x版本已改名为bs4，坑惨小弟了，半天没反应过来。其他的pip安装（如pybloom），也可apt-get安装（如py-requests）,最后，对付某&lt;span style=&quot;color: #00B050;&quot;&gt;error: command ‘x86_64-linux-gnu-gcc’ failed with exit status 1&lt;/span&gt;错误时，&lt;span style=&quot;color: #00B050;&quot;&gt;sudo apt-get install python-twisted-web python2.7-dev&lt;/span&gt;，可能会用的上。&lt;/p&gt;

&lt;p&gt;Scrapy是个不错的爬虫框架，最近笔者自己打算好好研究一下，结合注入工具进行爬虫式扫描，感觉应该不错的样子。如果有朋友有兴趣，或者有现货，欢迎提出宝贵建议，不胜感激。&lt;/p&gt;

&lt;p&gt;另外，前面提到的Gevent是一名访客告诉小弟的，查看了下，是Python的一个高并发框架（高级术语名为协程）。没记错的话，还是以前那位爱骂我SB的朋友告诉我的（小感伤一下），因为以前有做分布式监控的想法。以后考虑将其纳入做项目的计划范围。&lt;/p&gt;

&lt;p&gt;本文很水，不过以后有心得会更新的，除非有单独料，我会单独提出。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #00ffff;&quot;&gt;Enjoy yourself.^_^&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">没错，标题党君又来了！文章只是做些第三方评论，不喜勿喷。</summary></entry></feed>