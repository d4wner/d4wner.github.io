<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-23T23:25:08+08:00</updated><id>http://localhost:4000/</id><title type="html">HellSec</title><subtitle>原创网络安全博客&lt;-|-&gt;匠心独运，做有深度的良品。</subtitle><author><name>HellSec</name></author><entry><title type="html">IAST重构实录(一)</title><link href="http://localhost:4000/scanner/2021/07/22/iast-record-1/" rel="alternate" type="text/html" title="IAST重构实录(一)" /><published>2021-07-22T21:13:00+08:00</published><updated>2021-07-22T21:13:00+08:00</updated><id>http://localhost:4000/scanner/2021/07/22/iast-record-1</id><content type="html" xml:base="http://localhost:4000/scanner/2021/07/22/iast-record-1/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;最近接了个新的坑，准备重构某个IAST的项目。由于以前针对集群和大数据的接入使用，都是对二次封装平台接触的多。然鹅这次需要自己上服务器完整趟坑，故而记录下流水账，给连续加的班留个纪念。
    &lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;h3 id=&quot;redis脏数据的问题&quot;&gt;redis脏数据的问题&lt;/h3&gt;

&lt;p&gt;在接入redis集群时，开始测的时候只做了单元测试，处理时基本没遇到啥问题。&lt;/p&gt;

&lt;p&gt;但在整体联动试运行时，发现原来服务器集群上留存有原来测试脚本和agent，在往redis集群里打脏数据，格式与现有的不一致，导致连连报错。&lt;/p&gt;

&lt;p&gt;后来通过定位crontab和现有进程，找出了运行的几个测试脚本和agent，将它们干掉做了缓释。&lt;/p&gt;

&lt;h3 id=&quot;redis集群配置问题&quot;&gt;redis集群配置问题&lt;/h3&gt;

&lt;p&gt;我们知道，redis集群上执行一些命令，如&lt;code class=&quot;highlighter-rouge&quot;&gt;flushall&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;keys*&lt;/code&gt;等等，对线上环境可能会造成影响，所以一般会重命名，也就会用到&lt;code class=&quot;highlighter-rouge&quot;&gt;rename-command&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;但是我搞的时候不太熟，结果整的无论是redis-cli练连上去，还是开发急脚本用库去连，都不好使会显示：&lt;code class=&quot;highlighter-rouge&quot;&gt;command not found&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;所以后来我在redis配置文件里找到重命名的命令后，直接在任务函数里封装了一层远程cli命令，动态配置账密和rename以后的命令，引用配置来写死来执行危险操作（屏蔽了logging），如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;redis-cli -h x.x.x.x -p xxx -a xxx commandxxx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;redis持久化异常&quot;&gt;redis持久化异常&lt;/h3&gt;

&lt;p&gt;一个异常提示的是：
&lt;code class=&quot;highlighter-rouge&quot;&gt;MISCONF Redis is configured to save RDB snapshots...&lt;/code&gt;，这个其实就是持久化问题，一般不建议配置太长时间，该清理就清理，或者设置好expire时间。&lt;/p&gt;

&lt;p&gt;还有个提示是redis达到max records还是啥玩意儿，提示的大概是  &lt;code class=&quot;highlighter-rouge&quot;&gt;OOM command not allowed when used memory &amp;gt; 'maxmemory'&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;大概说是达到上限了，连接集群时，我尝试捕获错误也没捕获到。所以也不好根据这个来判定特征，对方只是直接拒绝连接了。&lt;/p&gt;

&lt;p&gt;我处理这类情况一般是，针对键值配置好足额的expire时间，然后定期去判定dbszie，设定一个肩部峰值，暂定的是70%-80%左右。最后，再加上我之前cli版本的主动清理，事实上还是比较好处理这类问题的。&lt;/p&gt;

&lt;h3 id=&quot;kafka-rebalancing解决&quot;&gt;kafka rebalancing解决&lt;/h3&gt;

&lt;p&gt;注意，kafka如果读取时切换任务比较快，可以设置较短的时间避免rebalancing。&lt;/p&gt;

&lt;p&gt;但是如果本身消费的时候，需要进行延时逻辑判定，或者需要等待kafka消息流等情况，就需要其他情况来控制了。&lt;/p&gt;

&lt;p&gt;相关配置参数如下（python版本）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;session_timeout_ms(会话池过期时间，维持活跃)
heartbeat_interval_ms（心跳时间，涉及rebalancing）
max_poll_interval_ms（总过期时间，设置过短容易rebalancing）
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;kafka时间段跳空后开&quot;&gt;kafka时间段跳空后开&lt;/h3&gt;

&lt;p&gt;我遇到的情况是，同一个groupid的情况下，在kafka topicA读数据流，选取了一个权重最大的时间点作为锚点。&lt;/p&gt;

&lt;p&gt;然后我又以此时间点，去kafka topicB读取数据存入redis缓存，作为基准分析数据。&lt;/p&gt;

&lt;p&gt;等我回过头再去kafka topicA读数据流时，发现新读取的数据流时间点，居然比存入缓存的topicB时间更晚。&lt;/p&gt;

&lt;p&gt;我仔细想了想，是不是因为在我读取kafka topicB去存的时候产生了延时，然后kafka topicA数据流的offset其实依然在跑。&lt;/p&gt;

&lt;p&gt;但是我消费的时候，设置了auto commit和手动commit和close，kafka topicA的二次锚点还是会跳空比缓存数据后开，这个问题目前偶尔会跳出来，留待之后观察。&lt;/p&gt;

&lt;h3 id=&quot;批量插入和消费问题&quot;&gt;批量插入和消费问题&lt;/h3&gt;

&lt;p&gt;主要聊聊kafka消费和redis的读插，两个比较简单的问题，放在一起聊了。&lt;/p&gt;

&lt;h4 id=&quot;kafka读取&quot;&gt;kafka读取&lt;/h4&gt;

&lt;p&gt;本身kafka属于流数据，通过offset控制读取的点位，竞争消费的问题由groupid控制解决。&lt;/p&gt;

&lt;p&gt;但是在竞争读取数据时，要知道redis缓存上限原来是按单进程进行窗口结算的。&lt;/p&gt;

&lt;p&gt;我在开多线程以后，如果从某个线程读取窗口数据，mset压入redis集群触发存储告警上限。&lt;/p&gt;

&lt;p&gt;这时候我们需要主动杀死线程，这样可能会丢失另外几个相当多的正在压入redis的数据。&lt;/p&gt;

&lt;p&gt;解决方案的话，我这边考虑的是多起几个节点和或者线程，减少单个单线程和节点的数据集。&lt;/p&gt;

&lt;p&gt;一旦停止，能通过少量时间的time-sleep来缓释节点的数据处理压力，也能避免&lt;code class=&quot;highlighter-rouge&quot;&gt;max_poll_interval_ms&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;session_timeout_ms&lt;/code&gt;设置的时间过长的问题。&lt;/p&gt;

&lt;h4 id=&quot;redis读写之殇&quot;&gt;redis读写之殇&lt;/h4&gt;

&lt;p&gt;在取redis数据的时候，读的动作和速度肯定是延后于写的。本身这事儿就不应该用消息队列型存储来搞，在数据量堆积到时候读压力巨tm大。&lt;/p&gt;

&lt;p&gt;实时规则处理可以用spark或者flink框架，离线的话完全可以采用hive（kafka2hive）存储kafka结果，定时拉取表对比生成临时hive表，再用脚本规则过滤去输出结果。&lt;/p&gt;

&lt;p&gt;这里因为一些限制借助不了大数据平台的能力，也不能指望我为一个项目搞个一套平台框架。&lt;/p&gt;

&lt;p&gt;所以呢，还是沿用的原来的思路，借助redis缓存，形成时间窗口。&lt;/p&gt;

&lt;p&gt;然后依据单个kafka数据的index_id和时间点作为锚定标准，去对比分析另一个kafka数据流，最后完成阶段性循环。&lt;/p&gt;

&lt;p&gt;但是在读redis的时候，批量mget数据意义不大，貌似提速不了多少。&lt;/p&gt;

&lt;p&gt;我这边是根据两个kafka队列的index_id去做判定的。&lt;/p&gt;

&lt;p&gt;如果一次性mget多个数据，通过多线程去做index_id的关联逻辑判定时，如果规则判定本身不怎么耗时的话，你会发现并没有快多少。&lt;/p&gt;

&lt;p&gt;所以结论是，大头还是落在kafka读取的压力上，取了以后就能去做分布式匹配。&lt;/p&gt;

&lt;h3 id=&quot;结语&quot;&gt;结语&lt;/h3&gt;

&lt;p&gt;本小节只简单聊了下之前踩过的准备坑，针对IAST本身没有聊太多，后面的续篇会简单介绍下。&lt;/p&gt;</content><author><name>HellSec</name></author><category term="scanner" /><summary type="html">最近接了个新的坑，准备重构某个IAST的项目。由于以前针对集群和大数据的接入使用，都是对二次封装平台接触的多。然鹅这次需要自己上服务器完整趟坑，故而记录下流水账，给连续加的班留个纪念。</summary></entry><entry><title type="html">业务SDL的上岸体历（二）</title><link href="http://localhost:4000/operation/2021/03/09/sdl-result2/" rel="alternate" type="text/html" title="业务SDL的上岸体历（二）" /><published>2021-03-09T21:46:00+08:00</published><updated>2021-03-09T21:46:00+08:00</updated><id>http://localhost:4000/operation/2021/03/09/sdl-result2</id><content type="html" xml:base="http://localhost:4000/operation/2021/03/09/sdl-result2/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;之前已经聊过在甲方团队做DevSecOps能力建设的零碎轨迹，但是针对项目级安全治理的关键细节，我们还不曾详述，那么接下来简单讲一讲。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;h3 id=&quot;安全合规评审&quot;&gt;安全合规评审&lt;/h3&gt;

&lt;p&gt;对于重要的基础系统或者服务，我们如何做好完整的评审闭环呢？&lt;/p&gt;

&lt;p&gt;这里我们分三个阶段去达成这个目标：&lt;/p&gt;

&lt;h4 id=&quot;信息采集阶段&quot;&gt;信息采集阶段&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;问卷和自查&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先，最了解系统的人肯定是相关开发人员和产品经理。那么整个企业或者整个业务线，拥有相似的研发习惯或者通病的话，批量进行问卷和自查能帮我们解决掉很多事情。&lt;/p&gt;

&lt;p&gt;当然，很多人可能会说，这种东西业务方不一定会配合你去做，安全人员在大部分企业内的地位是相对低的。&lt;/p&gt;

&lt;p&gt;好一点的可能是独立出来，直线向CTO汇报；次一点的挂在强势的运维总监下面，好赖能在平时做做事务推动。再次的话，就是那种一个人的安全部，除了怼二开/购置商业产品，还有日常和业务线撕逼外，其实存在感不是很大。&lt;/p&gt;

&lt;p&gt;但是话又说回来，做不做是一回事，如何去达成是另一回事。&lt;/p&gt;

&lt;p&gt;能用点手段，自上而下去推动自然是最好的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;文档发掘&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果是规范完备的研发线，通常会存在相关的研发说明和产品文档。&lt;/p&gt;

&lt;p&gt;我们做风险评估之前，也会从大量的信息中主动提取疑似脆弱点，作为后面评估的参考依据。&lt;/p&gt;

&lt;p&gt;后面的工作中，我们能通过对比记录，更加快速的定位到风险点。&lt;/p&gt;

&lt;p&gt;同时也能在一定程度上，避免在过长的评审时间跨度之后，产生不必要的风险遗漏。&lt;/p&gt;

&lt;h4 id=&quot;主动评估阶段&quot;&gt;主动评估阶段&lt;/h4&gt;

&lt;p&gt;在这个阶段，我们主要会对下面三个风险点进行分析。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;接入风险：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在使用某个系统的时候，可能系统本身是不具有高风险的。&lt;/p&gt;

&lt;p&gt;但是由于业务方操作不当，或者配置失误，有可能会导致产生接入性风险。&lt;/p&gt;

&lt;p&gt;所以这块儿的治理，一般是针对中台或者基础服务，需要提出规范制度进行管控。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;架构风险：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在架构设计之初，很多产品的安全评审是不足的。所以在架构设计中会出现潜在的风险。&lt;/p&gt;

&lt;p&gt;在设计逻辑架构时，我们会考虑到数据流转和业务场景转换，在关键流程节点上往往需要制定审计策略。&lt;/p&gt;

&lt;p&gt;在设计系统架构时，我们需要考虑到是否接入了安全能力组件或者安全服务中台；是否符合固有安全架构设定；是否符合冗余灾备法则。&lt;/p&gt;

&lt;p&gt;在设计网络架构时，我们需要考虑是否进行了线上线下隔离；所在网络区域是否对外开放；如果接入了外网，是否接入了SSO+IAM+VPN；是否会留存重要的操作日志；是否能做流量安全审计。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基础风险：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;运行时系统评估：WEB层面主要针对OWASP TOP风险进行核查；系统本身层面会审计是否引用了不安全的第三方服务（组件）；主机层面是否对基线（端口、配置、内核等）的进行了加固核查。&lt;/p&gt;

&lt;p&gt;代码评估：分析代码中是否存在硬编码和配置风险；是否引用了不安全的第三方包；是否存在常见的漏洞；是否存在明显的逻辑链路失误。&lt;/p&gt;

&lt;p&gt;数据评估：是否存在未脱敏数据的批量开放展示；是否存在敏感数据存储未加密，是否存在第三方数据不合规引用，是否接入了数据溯源和加密分发等措施。&lt;/p&gt;

&lt;h4 id=&quot;自动化覆盖阶段&quot;&gt;自动化覆盖阶段&lt;/h4&gt;

&lt;p&gt;在完成风险评估后，我们需要有一套完整的风险验收的闭环。&lt;/p&gt;

&lt;p&gt;在我们完成主动评估阶段的工作后，通过紧急程度分级进行修复推进。&lt;/p&gt;

&lt;p&gt;而后，我们再结合基础CMDB的数据，借助自动化能力，进行定期巡检复核。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;接入风险覆盖：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以通过注册依赖和代码层面，进行特征对比检查，校验是否存在接入失衡导致的风险。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;架构风险覆盖：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自动化核查供应链，实现第三方对接监测，枚举安全中台能力覆盖，枚举边界兜底措施覆盖等等。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基础风险覆盖：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接入黑盒漏扫、基线、白盒规则，后期通过自动化观测，进行变更管控和风险复核。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;基础能力兜底&quot;&gt;基础能力兜底&lt;/h3&gt;

&lt;p&gt;对于整个企业、业务线、乃至具体到单个系统。除了本身固有的风险以外，我们更需要从宏观的架构出发，使用基础安全的能力进行兜底。&lt;/p&gt;

&lt;h4 id=&quot;组件管理&quot;&gt;组件管理&lt;/h4&gt;

&lt;p&gt;首先，企业的研发线通常会引用开源组件，这批组件相对固定，所以维护起来有一定价值。&lt;/p&gt;

&lt;p&gt;从安全的角度来讲，我们可以通过接入CMDB库来管理组件版本，引入对CVE官方或者开源组件官方的漏洞舆情感知，方便我们在紧急情况进行有序的版本推修。&lt;/p&gt;

&lt;p&gt;当然，我们也可以通过脚手架的形式，把常用的组件进行打包，进行统一的版本升级管理。&lt;/p&gt;

&lt;p&gt;其次，我们还可以对于常见的开源组件做二改加固，单独维护一套相对安全的组件版本。&lt;/p&gt;

&lt;h4 id=&quot;能力引入集成&quot;&gt;能力引入集成&lt;/h4&gt;

&lt;p&gt;这个也是老生常谈了，对于安全能力的引入和集成，可以分为下面几个点去实施。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;安全组件：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果我们拥有维护安全组件的能力，要么指望通过规范，推广让研发线做引入。要么就直接在流水线中进行默认集成，如果上线出现问题再做版本回退。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;黑白盒：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在流水线发布的时候，会自动化进行增量的黑白盒扫描，此时的优先级调度，建议排在队列前面。&lt;/p&gt;

&lt;p&gt;然而这时候如果测出问题，并不建议直接阻断构建，因为我们一切的工作都是为了保障业务稳定性而服务的。&lt;/p&gt;

&lt;p&gt;如果出现高危漏洞，可直接推送告警给安全人员，审核确认后可以跟研发人员沟通紧急修复。如果紧急程度不高，我们可通过邮件或者工单沟通好，徐徐图之。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IDE插件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在代码交付之前，业务方使用IDE安全插件，能很好的对代码做预修正。&lt;/p&gt;

&lt;p&gt;常见的问题，能在自核验的过程中解除，减少其他环节的耗时。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;代理插件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在质量测试和开发中，一般会在本地PC进行调试，也会访问一些搭建好的新服务。&lt;/p&gt;

&lt;p&gt;这时候如果本地agent（浏览器）设置了相关代理，将测试流量转发到安全中心。就能很好的对增量的服务和接口进行风险确认，及时防患于未然。&lt;/p&gt;

&lt;p&gt;当然这个需要配置代理白名单，区分测试环境和线上环境，避免流量互串。&lt;/p&gt;

&lt;h4 id=&quot;底层服务化&quot;&gt;底层服务化&lt;/h4&gt;

&lt;p&gt;除了必要的安全检测能力，其他底层支撑能力也需要实现服务化。&lt;/p&gt;

&lt;p&gt;很多企业目前的安全建设进度，其实还处在做二开或者怼商业产品的节点，不定时做做渗透和众测，但自身并没有精力把底层的安全能力树立起来做支撑。&lt;/p&gt;

&lt;p&gt;因此，我们需要根据企业目前的情况进行分期建设，针对各个产品线进行安全缺失度评测和适度补全。&lt;/p&gt;

&lt;p&gt;首先，我们需要强化底层的认证机制，借助内部的证书分发中台，将鉴权认证细粒度到业务方的IDC、服务、接口，尽可能实现非敏感区域全覆盖。&lt;/p&gt;

&lt;p&gt;其次，需要实现存储能力的标准化，自适应调整敏感数据和文件的存储结构。将失衡的加密手段，以及错误的存储桶访问配置，实现无感知的安全强化加固。&lt;/p&gt;

&lt;p&gt;此外，我们还需要对业务容器化的流程做融合，把相关能力嵌入到云管平台及配套体系中，在企业“云化”的发展中形成安全助力。&lt;/p&gt;</content><author><name>HellSec</name></author><category term="operation" /><summary type="html">之前已经聊过在甲方团队做DevSecOps能力建设的零碎轨迹，但是针对项目级安全治理的关键细节，我们还不曾详述，那么接下来简单讲一讲。</summary></entry><entry><title type="html">业务SDL的上岸体历（一）</title><link href="http://localhost:4000/operation/2021/02/14/sdl-result1/" rel="alternate" type="text/html" title="业务SDL的上岸体历（一）" /><published>2021-02-14T21:46:00+08:00</published><updated>2021-02-14T21:46:00+08:00</updated><id>http://localhost:4000/operation/2021/02/14/sdl-result1</id><content type="html" xml:base="http://localhost:4000/operation/2021/02/14/sdl-result1/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;在甲方做业务SDL的几年，在落地方面也做了不少努力。一是得看着业务大爷们的脸色，二是得假装硬气的扮猪吃老虎，一路走来可谓是一把辛酸泪。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;系列文章，针对业务安全的从0-1的介绍，从小团队到大团队的风格转变，&lt;/p&gt;

&lt;p&gt;闲话不多扯，我们团队在SDL方向，针对公司现状进行了多个维度的治理。&lt;/p&gt;

&lt;p&gt;在本文笔者会根据过往的治理工作，在业务建设从0到1进行剖析，给大家落地经验的参考。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-12-7/1607328853415-SDLGO.png&quot; alt=&quot;SDL建设落地&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;基础审计运营&quot;&gt;基础审计运营&lt;/h3&gt;

&lt;p&gt;在安全建设初期，我们会着重针对事前、事中、事后为单位，进行安全审计运营。&lt;/p&gt;

&lt;h4 id=&quot;事前审计&quot;&gt;&lt;em&gt;事前审计&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在事前的维度，我们会以域名和服务Key为单位，对前后端的资产进行梳理，嵌入SOC进行运营管理，并进行安全审计流程闭环。&lt;/p&gt;

&lt;p&gt;在项目上线前，我们会对提测前的代码进行人工Review，进行安全合规的风险评估。&lt;/p&gt;

&lt;p&gt;对于安全检查大致包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OWASP TOP 10&lt;/li&gt;
  &lt;li&gt;OWASP API Security TOP 10&lt;/li&gt;
  &lt;li&gt;常见的逻辑漏洞&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此外，我们还会根据研发同学的习惯，制定相关核查制度，进行重点检查，提升检察效率。&lt;/p&gt;

&lt;p&gt;对于数据合规会重点检查：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据接口是否脱敏&lt;/li&gt;
  &lt;li&gt;数据存储是否加密&lt;/li&gt;
  &lt;li&gt;传输是否加密&lt;/li&gt;
  &lt;li&gt;数据关联性是否解耦&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有部分没有全部例举出来，会严格的按照合规白皮书的标准来实施。&lt;/p&gt;

&lt;p&gt;在审计完毕后，会在SOC平台进行数据留存，发工单向业务同学督促修复。&lt;/p&gt;

&lt;p&gt;如果出现了延期，系统工单会进行逐级递进，通知其上级，直到业务方响应闭环为止。&lt;/p&gt;

&lt;p&gt;虽然这些制度可能比较难推行，但确实是非常有必要的，这需要自上而下的认可才能完成。&lt;/p&gt;

&lt;p&gt;只有把代码安全的指标，像代码质量考核那样，加入业务研发同学的KPI之中，才能真正引起大家对安全的重视。&lt;/p&gt;

&lt;h4 id=&quot;事中监督&quot;&gt;&lt;em&gt;事中监督&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在每个项目的大型迭代时，我们也会进行人工安全审计。&lt;/p&gt;

&lt;p&gt;除了针对迭代出现的安全问题以外，主要还会检查部分误添加的线上测试数据，以及研发同学无意中进行的硬编码、弱口令配置、敏感数据打印等等。&lt;/p&gt;

&lt;p&gt;由于当时工具比较原始，除了借助半自动化的代码审计工具（如IDE、Fortify）以外，也会读取第三方设备收集入库的日志，自行写脚本筛选核查。&lt;/p&gt;

&lt;p&gt;当然这些工作，后面也逐步被接入CICD流水线的自动化平台所替代。&lt;/p&gt;

&lt;h4 id=&quot;事后管控&quot;&gt;&lt;em&gt;事后管控&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在各个系统正常运营期间，我们需要做的是借助各种安全入侵防护产品（自研or商采），譬如IDS、WAF、RASP之类的进行纵深体系检测，再借助风控与法务部门的力量，对出现的风险事件进行溯源和应急响应。&lt;/p&gt;

&lt;p&gt;为啥这里说的是纵深检测而不是纵深防御，其实是早期在使用安全产品的时候，更多的需要去借助人工去复核，没有很好的进行产品联动。&lt;/p&gt;

&lt;p&gt;当然大家可能觉得是因为没有采购同一家产品，造成了兼容性缺失的问题，才会导致工作量指数级别上升。&lt;/p&gt;

&lt;p&gt;其实笔者觉得这块儿的工作，无论是厂商本身来做，还是依靠团队人工对信息流进行补齐，都是相对次要的。&lt;/p&gt;

&lt;p&gt;更关键的点在于，是我们需要优化好安全运营规则，让更严重的事务和更优先的级别，及早的被我们感知到并进行应急处理，而不是淹没在海量告警和镭射动态展示大盘之中。&lt;/p&gt;

&lt;h3 id=&quot;自动化能力建设&quot;&gt;自动化能力建设&lt;/h3&gt;

&lt;p&gt;在这个阶段，我们投入了更多的精力去建设团队的安全自动化能力，以求解放人力去搞研究工作。&lt;/p&gt;

&lt;p&gt;在此期间，我们针对性的对缺失的安全能力，对标业内互联网企业的标杆进行剖析，发现主要有几个点是亟待改善的。&lt;/p&gt;

&lt;h4 id=&quot;黑盒安全能力&quot;&gt;&lt;em&gt;黑盒安全能力&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;我们利用线下测试环境和镜像流量，重点建设了主被动扫描器（DAST），其中包含黑盒漏扫和基线核查平台；同时，也针对交互式扫描（IAST）能力进行强化，对代码执行进行污点插桩和编号，监控系统的输出和落地执行结果，方便向源头进行追溯。&lt;/p&gt;

&lt;h4 id=&quot;白盒安全能力&quot;&gt;&lt;em&gt;白盒安全能力&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;针对白盒扫描（SAST），我们接入了多个扫描引擎。除了质量CICD流水线平台自带了安全插件进行优化外，还对商业化的白盒安全产品进行了外采接入；除此之外，自研的引擎里，还加入了针对Git平台的关键词和配置、组件版本依赖检查（支持分支），保障在接入流水线后，能尽可能完整的覆盖到所有代码的检查。&lt;/p&gt;

&lt;h4 id=&quot;数据合规检查&quot;&gt;&lt;em&gt;数据合规检查&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;针对这块儿，我们着重对数据库里存储敏感数据字段进行核查，也结合XIDS对流量里包含的敏感数据进行了监控。&lt;/p&gt;

&lt;p&gt;同时，我们还结合DLP监管的记录，以及对明暗水印机制的设计，对数据外泄事件起到了一定的防控作用。&lt;/p&gt;

&lt;h4 id=&quot;安全sdk治理&quot;&gt;&lt;em&gt;安全SDK治理&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;每个公司的技术栈是不同的，所以针对内部应用最多的技术栈代码，针对性的开发安全SDK也显得比较重要。&lt;/p&gt;

&lt;p&gt;除了本身对研发团队提出安全编码制度以外，我们还对研发团队提供了安全SDK，也提供了IDE安全检查插件，通过微侵入式的方法，在一定程度上保障了代码的安全。&lt;/p&gt;

&lt;p&gt;当然，这样产出的代码，可能在交付给第三方合作厂商时，会单独进行脱敏分支开发，在一定程度上会提升成本。但考虑到这方面的业务量不多，还是合算的。&lt;/p&gt;

&lt;h3 id=&quot;风险链路治理&quot;&gt;风险链路治理&lt;/h3&gt;

&lt;p&gt;在SDL建设到了这个阶段，我们已经有了一定的安全自动化能力，那么如何去实现突破呢？&lt;/p&gt;

&lt;p&gt;在梳理业务链路中的风险后，我们开始针对威胁进行建模，从体系架构上自上而下进行治理。&lt;/p&gt;

&lt;p&gt;其中，我们重点关注了IPDRR和STRIDE模型，并辅以DREAD模型进行威胁评级，针对原有安全能力的缺失进行优化联动，也对现有的安全风险点进行收敛治理。&lt;/p&gt;

&lt;p&gt;从不同的业务场景，以及不同的数据流程，分别对风险定制了削减措施，分配人力形成项目组，推进治理优化工作。&lt;/p&gt;

&lt;p&gt;具体的风险链路大盘，这里由于内容敏感不方便直接发出来，我们就简单谈谈重点项目的建设工作。&lt;/p&gt;

&lt;h4 id=&quot;资产库的收敛&quot;&gt;&lt;em&gt;资产库的收敛&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在原先的工作中，我们自己的资产库更多的会去依赖于运维部门的注册信息，然后我们内部通过扫描器进行的资产补充。&lt;/p&gt;

&lt;p&gt;后来发现这样做有个问题，我们在针对细粒度的资产标签，譬如新增端口、机器所属域、服务类型列表、迭代接口等等，无法及时的掌控，这样对我们的纵深体系的监测是不利的。&lt;/p&gt;

&lt;p&gt;所以在我们着重进行了这方面能力的补充，通过被动监控流量，以及主动对代码AST树进行梳理的方式，对复杂的调用进行网状关系绘制，以及对标签关联的资产进行聚类。&lt;/p&gt;

&lt;p&gt;这样，在出现问题后，我们也能很快的进行阻断和溯源定位。&lt;/p&gt;

&lt;h4 id=&quot;内部巡检&quot;&gt;&lt;em&gt;内部巡检&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;针对内部的一些通用型漏洞，由于信息关联度问题，我们在原来的工作中没有很好的进行横向打通。&lt;/p&gt;

&lt;p&gt;在不断加强内部对于0day、1day的识别能力后，我们能更好的去识别入侵事件。&lt;/p&gt;

&lt;p&gt;在后面的SOC运营平台的建设工作中，会针对漏洞元素进行细粒度的手动标签，也支持了自动化识别，加强了针对资产的测试环境和边界环境，及时进行横向巡检，将风险扼杀于摇篮之中。&lt;/p&gt;

&lt;h4 id=&quot;边界治理&quot;&gt;&lt;em&gt;边界治理&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在针对诸多风险进行治理时，必不可少的需要考虑到对边界安全进行收敛。&lt;/p&gt;

&lt;p&gt;在这方面的工作中，我们可以考虑做以下的工作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;针对出入的流量进行重点核查&lt;/li&gt;
  &lt;li&gt;对开放的外网服务进行一键式深度防护&lt;/li&gt;
  &lt;li&gt;启用备份机制，随时对出现漏洞升级故障的边界服务进行替换&lt;/li&gt;
  &lt;li&gt;对边界资产进行即时扫描防控&lt;/li&gt;
  &lt;li&gt;自动化核查应该把边界的优先级提升到最高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们前面对于资产库的优化工作，也是能为边界安全的治理提供效能的。&lt;/p&gt;

&lt;h4 id=&quot;接口人制度&quot;&gt;&lt;em&gt;接口人制度&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;我们原来的业务方接口人，都是资产定位到项目组为止。&lt;/p&gt;

&lt;p&gt;这样的话，很难在具体风险发生时，最快速度联系到个体接洽解决，在推进修复的过程中也很容易扯皮。&lt;/p&gt;

&lt;p&gt;所以后来我们采用了业务方接口注册制度，让业务方主动去做项目归属个体和人员backup的填写，并且结合内部的人力资源存留的注册信息，定期向业务方确认项目变动情况。&lt;/p&gt;

&lt;p&gt;总之，我们需要保障在项目存活的安全生命周期中，实现人员响应可控、权限周期可控、人员备用可控。&lt;/p&gt;

&lt;h4 id=&quot;第三方合作建设&quot;&gt;&lt;em&gt;第三方合作建设&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在第三方合作体系的建设过程中，我们也参照业内的情况，做了不少工作。&lt;/p&gt;

&lt;p&gt;在这方面的治理上，虽然有不少标准可以参考，但并没有统一的方案。&lt;/p&gt;

&lt;p&gt;其中需要治理的内容有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第三方合作商的业务&lt;/li&gt;
  &lt;li&gt;第三方SDK的引入核查&lt;/li&gt;
  &lt;li&gt;第三方流量接入时的监控前置&lt;/li&gt;
  &lt;li&gt;第三方账号监控的核查&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些东西太多，只能稍微例举几项。真正想要做好，需要对各业务线的情况进行深入了解，结合企业自身的情况进行定制。&lt;/p&gt;

&lt;p&gt;同时，这也是外部审计公司很难在短时间内给出完整方案的，需要内部的安全人员去做更多的探索。&lt;/p&gt;

&lt;p&gt;后面的话，针对这块儿可能会单独提出来跟大家聊聊，讨论下有哪些通用的解决方案。&lt;/p&gt;

&lt;h3 id=&quot;指标订立和回归&quot;&gt;指标订立和回归&lt;/h3&gt;

&lt;p&gt;在针对SDL实现流程化管理，对治理成果进行验收时，我们需要重点注意的是，得对前期订立的指标做验证回归。&lt;/p&gt;

&lt;p&gt;这些无论是在安全工作的KPI中体现，还是向业务方进行成果透明化展示，都是有意义的。&lt;/p&gt;

&lt;p&gt;这里举例几个方面，给大家参考下指标订立的原则：&lt;/p&gt;

&lt;h4 id=&quot;风险收敛&quot;&gt;&lt;em&gt;风险收敛&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;我们针对需要重要防控的项目，在风险收敛比例和收敛耗时方面，是需要订立一定的指标的。&lt;/p&gt;

&lt;p&gt;如果在这期间发生了安全风险引起的资损事件，会侧重考核我们是否做出了合适的应对措施，针对资损事件进行弥补和挽回。&lt;/p&gt;

&lt;h4 id=&quot;漏洞回归&quot;&gt;&lt;em&gt;漏洞回归&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;对于漏洞指标订立，我们会针对高危漏洞的数量、漏洞类型趋势进行统计。&lt;/p&gt;

&lt;p&gt;其中，会区分外部SRC提交和内部发现情况，从不同维度进行趋势计算。&lt;/p&gt;

&lt;p&gt;同时，我们针对黑白盒自动化漏扫的召回率和精确率，也会做同期的月度、季度、年度对比。&lt;/p&gt;

&lt;p&gt;最终，我们也会添加插件数、服务数、接口数等作为变量，从综合层面去判定成果，避免在判定权重方面发生缺失。&lt;/p&gt;

&lt;h4 id=&quot;服务健康&quot;&gt;&lt;em&gt;服务健康&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;这个有点类似于QA的质量考核指标，如果一个服务经常出现代码风险，经常出现漏洞，通常这个服务的健康评分会比较低。&lt;/p&gt;

&lt;p&gt;这个是针对业务方的，也是对我们治理结果的的反馈之一。&lt;/p&gt;

&lt;p&gt;对于服务安全，我们订立了几个指标，譬如漏洞修复率、安全产品接入率、漏洞反复率、风险发生率、鉴权接入率等等。&lt;/p&gt;

&lt;p&gt;最后，通过单个服务乃至业务线作为基准元素，我们也会反过来对比考核各个业务线安全的质量，从而在更高的维度评价业务安全的产出。&lt;/p&gt;</content><author><name>HellSec</name></author><category term="operation" /><summary type="html">在甲方做业务SDL的几年，在落地方面也做了不少努力。一是得看着业务大爷们的脸色，二是得假装硬气的扮猪吃老虎，一路走来可谓是一把辛酸泪。 系列文章，针对业务安全的从0-1的介绍，从小团队到大团队的风格转变，</summary></entry><entry><title type="html">被动漏扫系统实践</title><link href="http://localhost:4000/scanner/2021/02/05/axe-scan/" rel="alternate" type="text/html" title="被动漏扫系统实践" /><published>2021-02-05T20:46:00+08:00</published><updated>2021-02-05T20:46:00+08:00</updated><id>http://localhost:4000/scanner/2021/02/05/axe-scan</id><content type="html" xml:base="http://localhost:4000/scanner/2021/02/05/axe-scan/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;做被动漏扫也有很长一段时间了，期间自用的版本和企业公用的版本，各单独起分支做持续迭代。目前市面上优秀的轮子已经不少，所以这里只简单讲讲企业内部漏扫设计的思路。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-12-9/1607503446666-%E8%A2%AB%E5%8A%A8%E6%BC%8F%E6%89%AB%20(1).jpg&quot; alt=&quot;被动漏扫全景图&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;流量来源&quot;&gt;流量来源&lt;/h3&gt;

&lt;p&gt;在平时的漏扫使用中，代理流量来源大概有这么几个：&lt;/p&gt;

&lt;h4 id=&quot;browser_plugin&quot;&gt;&lt;em&gt;Browser_Plugin&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;主要代表有chrome插件，对测试环境进行访问时，会把PC端流量传输到流量center。&lt;/p&gt;

&lt;h4 id=&quot;类burp插件&quot;&gt;&lt;em&gt;类Burp插件&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在进行测试的过程中，我们需要设置多层代理。&lt;/p&gt;

&lt;p&gt;将通过类burp软件的流量，通过代理agent中转，再传输到流量center。&lt;/p&gt;

&lt;h4 id=&quot;nids&quot;&gt;&lt;em&gt;Nids&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;针对节点主机部署agent，包含蜜罐、节点网关等等，通过流量收集转发到流量center。&lt;/p&gt;

&lt;h4 id=&quot;ngnix&quot;&gt;&lt;em&gt;Ngnix&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在ngnix代理层面，通过镜像旁路流量收集，转发到流量center。&lt;/p&gt;

&lt;p&gt;最后，我们在流量center处理流量格式后，再经过中间件（kafka），传输到数据库es存储起来，这样既可以用于被动漏扫进行流量复刻，也可以用于后面kibana（或者二改的）前端进行查询。&lt;/p&gt;

&lt;h3 id=&quot;扫描调度&quot;&gt;扫描调度&lt;/h3&gt;

&lt;p&gt;在我们覆盖到足够多的流量后，会通过任务调度对存储的流量进行fuzz，这里先假一些基础条件：&lt;/p&gt;

&lt;h4 id=&quot;强制线下环境&quot;&gt;&lt;em&gt;强制线下环境&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;如果是线上流量测试，敏感线上接口需要打标签和加白名单，也会涉及https和qps控制速率的问题，比较麻烦。&lt;/p&gt;

&lt;h4 id=&quot;token或者session池&quot;&gt;&lt;em&gt;token或者session池&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;除非线下环境本身设置的是登录态能够长期保持的，我们需要单独维护一套token或者session池，通过不同业务线和权限级别分类，方便在测试时做对比替换。&lt;/p&gt;

&lt;h4 id=&quot;线下环境保持稳定&quot;&gt;&lt;em&gt;线下环境保持稳定&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;由于公用的资源有限，在测试完毕后，质量人员一般会把资源给下掉，这样时间一长我们收集到的流量会失效。&lt;/p&gt;

&lt;p&gt;做好准备工作，我们就可以进行扫描调度了，我们这里采用的是下发任务给多个扫描agent的方式。任务会存在中间件redis管道里，等待各个agent读取之后，然发送fuzz请求包。&lt;/p&gt;

&lt;p&gt;这样虽然扫描进度不一样，但是配置和命令是统一在控制台下发的。保障在启动扫描时各方的统一可控，出现问题也能及时关停止损。&lt;/p&gt;

&lt;p&gt;在扫描完成后，我们会把结果传输到数据库中台，再通过人工review进行确认，但这属于漏洞sdl闭环的事儿，这里就不多提了。&lt;/p&gt;

&lt;h3 id=&quot;漏扫模块&quot;&gt;漏扫模块&lt;/h3&gt;

&lt;p&gt;漏扫模块的话，我们主要考虑检测下面几个方面：&lt;/p&gt;

&lt;h4 id=&quot;owasp-top-10&quot;&gt;&lt;em&gt;OWASP TOP 10&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;对于常规漏洞的检测是需要的，涉及一些延时的漏洞（比如前端渲染、ssrf），我们主要依靠第三方平台比如类ceye的内网版本，通过标签的形式进行延时漏洞确认。&lt;/p&gt;

&lt;h4 id=&quot;owasp-api-top-10&quot;&gt;&lt;em&gt;OWASP API TOP 10&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;其实自动化只能覆盖部分检测，还有部分需要人工辅助评估。&lt;/p&gt;

&lt;p&gt;只能说api检测这块儿，我们可以做的探索还比较多。&lt;/p&gt;

&lt;h4 id=&quot;poc探测&quot;&gt;&lt;em&gt;POC探测&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;主要针对可能出现的服务和cms类漏洞进行poc探测，对主动扫描能力进行补充。&lt;/p&gt;

&lt;p&gt;很明显，部分poc通过流量复刻进行fuzz，会比主动扫描准确的多。&lt;/p&gt;

&lt;h4 id=&quot;fuzz检测&quot;&gt;&lt;em&gt;FUZZ检测&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;这块儿的话，是我们根据安全团队的测试经验和业务方的特性，进行fuzz字典定制的，主要用于发现一些意外绕过和泄露的情况。&lt;/p&gt;

&lt;p&gt;我们使用流量复刻的手段进行fuzz，能在权限认证可控的情况下，针对正常的返回进行对比，去除掉一些误报，提升检测的准确性。&lt;/p&gt;</content><author><name>HellSec</name></author><category term="scanner" /><summary type="html">做被动漏扫也有很长一段时间了，期间自用的版本和企业公用的版本，各单独起分支做持续迭代。目前市面上优秀的轮子已经不少，所以这里只简单讲讲企业内部漏扫设计的思路。</summary></entry><entry><title type="html">私有云安全-纵深防御设计实践</title><link href="http://localhost:4000/cloud/2021/01/30/strategic-depth-sec-design/" rel="alternate" type="text/html" title="私有云安全-纵深防御设计实践" /><published>2021-01-30T23:40:00+08:00</published><updated>2021-01-30T23:40:00+08:00</updated><id>http://localhost:4000/cloud/2021/01/30/strategic-depth-sec-design</id><content type="html" xml:base="http://localhost:4000/cloud/2021/01/30/strategic-depth-sec-design/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;在对于私有云安全落地的实践中，如果我们已经有了一定的安全基础，对于后期架构策略的设计，需要花更多的精力在纵深防御和整体联动之上。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;下面笔者会从多个维度，分析下能在私有云进行建设的纵深防御工作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-17/1605596169406-%E7%BA%B5%E6%B7%B1%E9%98%B2%E5%BE%A1.jpeg&quot; alt=&quot;纵深防御&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;安全产品矩阵&quot;&gt;安全产品矩阵&lt;/h3&gt;

&lt;p&gt;在私有云的纵深防御中，有些同学可能觉得安全产品堆叠实际不会产生很好的效果，更大的价值上只是为了把每年采购的预算给花出去。&lt;/p&gt;

&lt;p&gt;但其实，如果我们合理安排对于各项领域安全产品的组合，在一定程度上也能为我们的系统迁移上云，提供更加可靠的保障。&lt;/p&gt;

&lt;p&gt;下面笔者会简单聊下各个领域的产品，为私有云安全架构的建设工作提供参考。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-16/1605518469119-%E4%BA%A7%E5%93%81%E7%BA%B5%E6%B7%B1.jpeg&quot; alt=&quot;产品纵深&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;通信安全&quot;&gt;&lt;em&gt;通信安全&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在通信流量的安全控制上，通常我们会对边界进行阻断，再通过节点部署安全产品，逐步进行流量监控的补充。&lt;/p&gt;

&lt;p&gt;对于堡垒机，我们可以通过4A认证，结合IAM和VPN对出入的流量进行细粒度管控。&lt;/p&gt;

&lt;p&gt;边界防火墙，则可以对私有云出入口，以及不同安全域之间的流量进行拦截。&lt;/p&gt;

&lt;p&gt;至于NIDS，则是部署在节点之后，利用高吞吐量和对内网流量协议的高切合度的分析规则，对防火墙没能拦截的内容做补充分析。&lt;/p&gt;

&lt;h4 id=&quot;主机安全&quot;&gt;&lt;em&gt;主机安全&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在主机系统安全上，其实也存在很多竞品，功能有不少重合之处。&lt;/p&gt;

&lt;p&gt;比较常用的就是HIDS，它会监控主机的事件和系统调用的监控，但对系统做适配和定制化。&lt;/p&gt;

&lt;p&gt;而EDR作为终端安全产品概念，也被一些厂商概念混用，拿去做IDC（或者容器）安全监控了，这个不做过多的讨论。&lt;/p&gt;

&lt;p&gt;此外，我们针对WebServer层面，是可以考虑嵌入RASP，做运行时安全监控的。
但是这个相对来讲，对业务性能影响也会更多，所以需要做审慎启用。&lt;/p&gt;

&lt;p&gt;在私有云的容器群中，我们也可以考虑部署蜜罐。&lt;/p&gt;

&lt;p&gt;业务方一般不会主动去访问蜜罐主机，所以当它们意外收到访问流量时，就可以在不动声色的情况下，被动监控黑客的横向拓展动作。&lt;/p&gt;

&lt;h4 id=&quot;数据安全&quot;&gt;&lt;em&gt;数据安全&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;我们的系统在上云时，除了要考虑数据库访问的稳定性和资源分配合理性，也需要对数据库访问的SQL进行行为分析。&lt;/p&gt;

&lt;p&gt;或者，我们也可以直接针对单条恶意SQL进行阻断，或通过链式SQL分析进行告警，这就涉及到数据安全审计（数据库防火墙）了。&lt;/p&gt;

&lt;p&gt;与此同时，我们在对外展示数据和存数据入库时，也可能涉及到加解密和脱敏问题，尤其是第三方人员账号介入，公共的云加解密（脱敏）平台能很好的缓释相关风险。&lt;/p&gt;

&lt;p&gt;另外，对于我们通常讲的硬编码问题，云管平台也一般会配上KMS密钥管理功能，通过线上线下分离，也能缓释内部Git库对第三方开放的风险。&lt;/p&gt;

&lt;h4 id=&quot;应用安全&quot;&gt;&lt;em&gt;应用安全&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在针对应用的保护上，从粗粒度来看，我们可以通过添WAF（主机层面和代理层面）、添加网页防窜改（植入文件监控）、添加反爬机制（做好验证和混淆策略，然后接入风控）。&lt;/p&gt;

&lt;p&gt;同时，我们在考虑外部风险点时，防DDOS和CC也会是个重点治理方案，这个在前面的《私有云安全-边界安全设计实践》已经提到过。&lt;/p&gt;

&lt;p&gt;最后，我们需要特别关注下，云管平台自带的（或者另行搭建）API安全网关，是否能完美支持我们私有云服务间接口的协议。&lt;/p&gt;

&lt;h4 id=&quot;态势感知sa&quot;&gt;&lt;em&gt;态势感知SA&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;安全态势感知产品的类别也很多，比如SIEM就是软件和服务的组合，通过对各类告警信息的整合，提升私有云整体审计溯源、应急响应的能。&lt;/p&gt;

&lt;p&gt;而SOC其实跟前者有些类似，但更趋向于以资产为核心，以安全事件管理为关键流程，采用安全域划分的思想，建立一套实时的资产风险模型进行集中式管理。&lt;/p&gt;

&lt;p&gt;至于Gartner之后力推的SOAR，个人觉得比较难去适配各家的产品。
上云的系统在针对各类标准不同的API进行自动化编排时，会对研发成本消耗加剧，应当有待观察后续的发展情况。&lt;/p&gt;

&lt;h3 id=&quot;鉴权防护&quot;&gt;鉴权防护&lt;/h3&gt;

&lt;p&gt;在私有云的接口调用和角色操作行为方面，有很多地方涉及到细粒度鉴权。&lt;/p&gt;

&lt;p&gt;我们可能不一定有机会，在代码层面去附加设计鉴权策略，但是可以借助云管平台和PaaS资源，根据现有的基础做一些配置优化。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-17/1605596905800-%E9%89%B4%E6%9D%83%E4%BD%93%E7%B3%BB.png&quot; alt=&quot;鉴权体系&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;中间件鉴权&quot;&gt;&lt;em&gt;中间件鉴权&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;在私有云架构里使用中间件时，由于采用的大规模集群管理，不可能单独做配置，所以一般会有两种基础鉴权方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;第一，我们可以使用Security插件，这类是附带的中间件管理机制，但缺点是无法统一做管控溯源。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;第二，我们可以在Nginx层面做反向代理，然后在此层面提高吞吐量，通过第三方接口回调或者代理配置进行认证。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;api鉴权&quot;&gt;&lt;em&gt;API鉴权&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;至于API鉴权，在可信域范围之内，我们可以通过注册信息，直接进行通信交互。
否则的话，我们需要借助签名信息，与KMS密钥管理平台协作，完成交互认证闭环。&lt;/p&gt;

&lt;h4 id=&quot;cdn鉴权&quot;&gt;&lt;em&gt;CDN鉴权&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;由于私有云上存储的文件，其访问和外发都是比较敏感的。&lt;/p&gt;

&lt;p&gt;因此，除了要对文件本身做好加密，我们还需要在访问控制做好鉴权。&lt;/p&gt;

&lt;p&gt;在访问URL时，我们可以通过带上私钥和时间戳的加密组合，结合身份来验证是否盗链访问，同时这样也能限制文件访问的过期时间。&lt;/p&gt;

&lt;p&gt;此外，我们可以通过IP白名单的方式做辅助验证，系统发现更改来源IP（或者直接进行内网IP绑定）后，会直接拒绝访问。&lt;/p&gt;

&lt;h4 id=&quot;微服务鉴权&quot;&gt;&lt;em&gt;微服务鉴权&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;这里讨论的其实不属于资源层面的，而是属于架构层面的认证鉴权。&lt;/p&gt;

&lt;p&gt;微服务鉴权目前的做法，使用全局Token+API网关鉴权会多一些。&lt;/p&gt;

&lt;p&gt;在这种形式下微服务是透明化的，通过网关时会把原始的用户令牌，转换为内部会话ID令牌，单方面注销相对来说也精准一些。&lt;/p&gt;

&lt;h4 id=&quot;元素基准鉴权&quot;&gt;&lt;em&gt;元素基准鉴权&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;元素基准鉴权的问题，在前面的系列文章《私有云安全-边界安全设计实践》也简单提到过。&lt;/p&gt;

&lt;p&gt;我们通过对角色、资源、服务等多个维度，构建对应的复杂关系网络，其中再以某个元素为基准点，进行XABC类型的鉴权，实际应用可以参考IAM访问控制模型。&lt;/p&gt;

&lt;h4 id=&quot;多因素鉴权&quot;&gt;多因素鉴权&lt;/h4&gt;

&lt;p&gt;我们统一认证入口处，为了鉴别登入人的身份，通常会采用多因素认证。
比如短信+密码，或者令牌+密码，有时候还可能加上硬件KEY、指纹、虹膜识别等等技术手段。&lt;/p&gt;

&lt;p&gt;在这里需要注意的是，由于私有云环境是相对固定的运维人员做操作，也希望减少对外的交互，最好采用在断网情况下也能进行认证的方式。&lt;/p&gt;

&lt;h3 id=&quot;人为监督&quot;&gt;人为监督&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-17/1605596995987-%E4%BA%BA%E4%B8%BA%E7%9B%91%E7%9D%A3.png&quot; alt=&quot;人为监督&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在私有云复杂环境下，如果出现问题，我们需要对资产进行细分，建立相应资产责任人制度。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;落地一级责任人自查，针对资源设置直接责任人，方便出问题时进行溯源和告警。&lt;/li&gt;
  &lt;li&gt;实行多人Backup机制，针对资源进行归类划分，一个资源群有2-3人递归负责。&lt;/li&gt;
  &lt;li&gt;推广支撑部门互相监督，在业务中如果出现了异常调用的流量，支撑部门需要对可能出现问题的一方进行监督提醒。&lt;/li&gt;
  &lt;li&gt;制定分级上报流程，在上报期间，如果出现延期响应和业务方推诿后，可以分期逐级再向上推送。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;分级隔离&quot;&gt;分级隔离&lt;/h3&gt;

&lt;p&gt;在针对私有云内的安全隔离上，主要分多个层面逐级进行隔离，建立纵深防御的隔离体系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-17/1605597244651-%E5%8C%BA%E5%9F%9F%E9%9A%94%E7%A6%BB.png&quot; alt=&quot;分级隔离&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;acl隔离&quot;&gt;ACL隔离&lt;/h4&gt;

&lt;p&gt;ACL一般是在防火墙或者交换机上实现的，是基于IP地址的控制策略。&lt;/p&gt;

&lt;p&gt;在此基础上，我们通过IP地址段和端口列表的形式进行控制，但其控制粒度相对较粗，这种适合对应用层访问控制进行补充。&lt;/p&gt;

&lt;h4 id=&quot;vpc区域隔离&quot;&gt;VPC区域隔离&lt;/h4&gt;

&lt;p&gt;专有网络VPC属于第二层的隔离，可以为每个用户创建多个子网。&lt;/p&gt;

&lt;p&gt;VPC代表了不同的租户，以它为限制区域隔离出了租户，每个租户的子网之间也是可以继续隔离的。&lt;/p&gt;

&lt;p&gt;部分云厂商由于拓展偏好，借鉴了AWS的Vxlan协议，对每个VPC网络进行隔离。&lt;/p&gt;

&lt;p&gt;但其实传统的Vlan隔离出来的数量，也是够用的。&lt;/p&gt;

&lt;p&gt;另外，在每个云租户内部，如果隔离要求比较严格，其实也可以借用VPC进行安全隔离。&lt;/p&gt;

&lt;h4 id=&quot;安全组隔离&quot;&gt;安全组隔离&lt;/h4&gt;

&lt;p&gt;在云租户内部的VPC内，如果还想尝试更细粒度的划分，可以尝试安全组形式的隔离。&lt;/p&gt;

&lt;p&gt;安全组用于设置多台云服务器的网络访问控制，是重要的网络安全隔离手段，用于在云端划分安全域。&lt;/p&gt;

&lt;p&gt;安全组是一个逻辑上的划分，这个分组由同一个地域内具有相同安全保护需求，并相互信任的实例组成。&lt;/p&gt;

&lt;p&gt;它在不同的云厂商实现中，叫法和实现方式可能也有差异，比如将VPC内的隔离成为Subnet（子网），意思应该与安全组一样。&lt;/p&gt;

&lt;h4 id=&quot;容器层隔离&quot;&gt;容器层隔离&lt;/h4&gt;

&lt;p&gt;这个概念是用虚拟化技术来作为容器（确切地说是 Kubernetes Pod）的沙箱，目前主流的开源安全容器项目是Kata、Gvisor，各大互联网厂商应用的也比较多。&lt;/p&gt;

&lt;p&gt;虽然作为安全容器项目，Kata和Gvisor目前在业务稳定性和兼容性上，比起Docker还有待优化，但毕竟是未来替代发展的大趋势。&lt;/p&gt;

&lt;p&gt;它们能提供虚拟机级别的隔离性，这个安全级别是在过去数年以来的云服务中，是被广泛接受的。&lt;/p&gt;

&lt;p&gt;新型的安全容器不再共享内核，而是利用Hypervisor等技术进行加固和隔离。&lt;/p&gt;

&lt;p&gt;它们对Linux依赖都是比较固定，而且能在每个阶段做到安全审计，降低被攻破的风险，是我们对于容器研究里程碑的较大的突破。&lt;/p&gt;

&lt;h3 id=&quot;参考文档&quot;&gt;参考文档&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/ExMan/p/12125640.html&quot;&gt;微服务认证鉴权的四种方案&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/ask/214321?spm=a2c6h.13524658&quot;&gt;CDN 如何实现鉴权配置&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.tencent.com/developer/article/1628386&quot;&gt;关于网络安全域隔离问题的研究与思考&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/article/738934&quot;&gt;安全容器：开启云原生沙箱技术的未来&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freebuf.com/articles/others-articles/228615.html&quot;&gt;云原生之容器安全实践&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://baijiahao.baidu.com/s?id=1630404422879648523&amp;amp;wfr=spider&amp;amp;for=pc&quot;&gt;全景解析5种云安全技术方案&lt;/a&gt;&lt;/p&gt;</content><author><name>HellSec</name></author><category term="cloud" /><summary type="html">在对于私有云安全落地的实践中，如果我们已经有了一定的安全基础，对于后期架构策略的设计，需要花更多的精力在纵深防御和整体联动之上。</summary></entry><entry><title type="html">私有云安全-边界安全设计实践</title><link href="http://localhost:4000/cloud/2021/01/15/boundary-sec-design/" rel="alternate" type="text/html" title="私有云安全-边界安全设计实践" /><published>2021-01-15T23:40:00+08:00</published><updated>2021-01-15T23:40:00+08:00</updated><id>http://localhost:4000/cloud/2021/01/15/boundary-sec-design</id><content type="html" xml:base="http://localhost:4000/cloud/2021/01/15/boundary-sec-design/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;对于私有云环境的安全建设工作中，需要做的事务繁杂而冗长。但我们在梳理要点后，会发现非传统边界的加固工作，也是我们应该重点关注的。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-26/1606397128210-%E8%BE%B9%E7%95%8C%E5%AE%89%E5%85%A8.jpg&quot; alt=&quot;边界安全概述&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先，我们需要关注私有云环境的&lt;strong&gt;访问控制&lt;/strong&gt;问题，在实现一定程度的可信准入后，对各层次的边界进行鉴权和认证；另外，我们需要针对潜在外部的大流量，以及容器间的流量风暴，通过代理集群和容灾多活的方式，进行&lt;strong&gt;分流防御&lt;/strong&gt;处理；当然，无论是IDC资源还是容器资源，我们都务必要做好&lt;strong&gt;主机加固&lt;/strong&gt;的工作，阻止黑客进行权限提升和横向扩散。&lt;/p&gt;

&lt;h3 id=&quot;主机加固&quot;&gt;主机加固&lt;/h3&gt;

&lt;h4 id=&quot;端口策略&quot;&gt;端口策略&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;出入策略&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在对网络边界的安全进行设计时，我们需要对出入端口的访问控制策略，进行精细化的控制。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;云环境边界：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;对于云环境边界的宿主机和IDC来说，端口开放的越多，意味着攻击面会扩大。而它们被黑客尝试或者存在风险的可能性，也会越大。&lt;/p&gt;

&lt;p&gt;所以在此逻辑基础上，这些主机需要依赖云容器管理平台和人工配置，借助类iptables的系统工具，对开放的端口实行最小化原则，同时也要对所有的端口进行严格的认证控制，保障基础的传统边界控制。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;安全域隔离边界：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;至于云环境网络隔离区，则相对来说没有没那么严格。在不同的服务之间，本身可能会存在流量交互，所以进出口规则会比较复杂，变动也会相对来说更多，如果同样采用最小化策略，可能会对业务带来比较多的困扰。&lt;/p&gt;

&lt;p&gt;但是，对于网络隔离区的出入策略，对于端口控制，也是需要做好认证和鉴权的，这块儿的内容也需要云管平台去做统一的配置管理。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;流量监控&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于端口出入的流量，我们除了做好异常流控，保证网络稳定以外，也需要对其中的流量进行监控检测。&lt;/p&gt;

&lt;p&gt;在黑客通过手段获取了云环境内主机的临时权限后，可能会尝试横向移动或者后门反弹，抑或直接进行数据拖取。在这种情况下，对于端口流量的监控就显得尤为重要了。&lt;/p&gt;

&lt;p&gt;在监控到端口流量异常后，安全运营中心（SOC）可以通过行为联动进行分析告警，及时阻断隔离可能沦陷的机器的访问控制。&lt;/p&gt;

&lt;h4 id=&quot;攻击阻断&quot;&gt;攻击阻断&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;边界WAF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在私有云环境提供对外的WEB服务时，对于外部用户可触达的范围，是需要提供WAF保护的。&lt;/p&gt;

&lt;p&gt;在针对这部分内容的防御上，主要是针对传统URI进行访问核查，以及对API的访问控制。&lt;/p&gt;

&lt;p&gt;其中对于接口的访问频次和流量内容，是否存在异常，都是边界WAF需要关注的。&lt;/p&gt;

&lt;p&gt;这里显然不建议在安全域隔离边界进行WAF部署，我们应该专注对云环境边界的主机（如Nginx层）进行流量收口阻断控制，这样是相对合适的做法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;边界防火墙&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于边界防火墙的话，可以对私有云网络的内外流量进行交互限制，限制异常流量的出入。&lt;/p&gt;

&lt;p&gt;就部署的优先级来看，是私有云边界优先于安全域隔离边界。&lt;/p&gt;

&lt;p&gt;我们也可以在云环境边界，尝试部署硬件防火墙，再在安全域边界通过软件防火墙的形式进行补充，当然具体实施要看情况而定。&lt;/p&gt;

&lt;p&gt;在实际部署的过程中，我们可能会遇到预算有限、业务复杂性较高、网络稳定性要求高等多重问题困扰。&lt;/p&gt;

&lt;p&gt;所以我们需要在保障最低安全标准的情况下，对相关安全能力逐步进行建设和方案优化。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;HIDS&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在边界主机上，我们部署HIDS的同时，进行日志的留存分析也是很有必要的。&lt;/p&gt;

&lt;p&gt;在入侵事件发生的事前和事中，我们可以通过内网态势感知（日志流量的监控联动），对攻击进行拦截阻断。&lt;/p&gt;

&lt;p&gt;而边界主机上的HIDS的监控告警和相关安全日志，能帮助安全运维人员更好的治病于腠理。&lt;/p&gt;

&lt;p&gt;另外需要强调的一点，日志最好统一上传到日志分析中心（比如ELK或者Splunk），以免被拿到权限的黑客清除入侵痕迹。&lt;/p&gt;

&lt;h3 id=&quot;分流防御&quot;&gt;分流防御&lt;/h3&gt;

&lt;p&gt;对于进入私有云环境的流量，我们首先需要做好流量控制，防止内部脆弱的网络出现稳定性故障。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;负载均衡SLB：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;负载均衡（Server Load Balancer）是将访问流量根据转发策略，分发到后端多台机器的流量分发控制服务，这是在流量出入口实施分流控制的常见做法。&lt;/p&gt;

&lt;p&gt;比如在云环境边界的Nginx集群接受了用户的访问请求后，经过流量控制检测，如果没有问题会继续向后分发到真实主机，经过逻辑处理后跟数据主机集群进行交互，最后把结果再反馈给Nginx集群，形成完整的访问闭环。&lt;/p&gt;

&lt;p&gt;这个问题其实更偏向于运维的内容，而非安全主要关注的点。但它又在对外防御流量攻击（Dos和CC）上，会起到不可或缺的作用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;CDN防护和流量清洗：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CDN防护和流量清洗（类比云堤的黑洞），这些概念其实应该归类于混合云的范畴了。&lt;/p&gt;

&lt;p&gt;如果想要在私有云环境里完成独立的防护和清洗，先不论实现效果的差异，我们需要注意下面几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;静态服务器集群：主要功能是静态资源托管，这是完成内网CDN替代的关键。不过混合云使用的CDN本身是可以隐藏真实IP的，这能在一定流量上限范围内，防止外部流量打穿从而泄露真实主机。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;高防硬件防火墙清洗：如果不能把流量导向到各大云厂商进行清洗后回流，那高标准的硬件边界防火墙是必要的。否则极端情况下，会直接导致业务瘫痪。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;高吞吐量的带宽：这也是高稳定性业务必须要具备的基础，它能支持我们在对入口流量进行清洗的同时，还能稳定的对外提供访问。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;访问控制&quot;&gt;访问控制&lt;/h3&gt;

&lt;p&gt;在私有云环境里的边界，通过多维度进行准入访问控制，也是我们需要重点关注的内容。&lt;/p&gt;

&lt;h4 id=&quot;iam认证&quot;&gt;IAM认证&lt;/h4&gt;

&lt;p&gt;IAM（身份和访问管理）通常负责用户需要访问的各种系统中的身份生命周期管理。简单来说，分为身份、认证、授权三个大模块。这个无论在边界访问控制，还是在纵深防御的建设上，我们都会重点提及。&lt;/p&gt;

&lt;p&gt;在私有云边界入口，有试图对内部发起访问的行为时时，我们需要对其属性和角色进行合法性校验，再对其进行细致的访问控制。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;身份（身份生命周期管理）：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这块儿的内容是企业关注的重点之一，在员工入职、角色变更、离职等单一流程线之外，其实也涉及到了复杂的关联变化。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;认证（身份认证）：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这点我们可以采用多种方式进行，比如token、签名秘钥、或者多因子认证等等。&lt;/p&gt;

&lt;p&gt;而针对用户身份进行鉴别准入时，我们可能会考虑到多维度的内容，比如IP、用户指纹、设备指纹等等。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;授权（权限分配）：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这是IAM安全管控的核心，确保正确的人在正确的时间，动态地获得正确的访问权。&lt;/p&gt;

&lt;p&gt;从概念上来讲，PBAC是当前和未来的IAM最佳授权方法，因为PBAC结合了RBAC和ABAC的最佳特性。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;落地补充：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;对于私有云落地实施来讲，部分云厂商给的采购解决方案里，管控平台统一为未来接入的系统，预留了IAM管控方案，我们可以少操一点心。&lt;/p&gt;

&lt;p&gt;但如果如果在中后期独立研发、采购的系统，或者本身就是自己组建的私有云环境，则需要通过设计接口和相关改造方案，尽快接入统一的IAM体系。&lt;/p&gt;

&lt;p&gt;我们在私有云网络组建前期，需要定制和遵循基础的安全原则和方针路线，这能在一定程度上，减少后期改造和维护的成本。&lt;/p&gt;

&lt;h4 id=&quot;api网关&quot;&gt;API网关&lt;/h4&gt;

&lt;p&gt;针对API的细粒度访问控制来讲，我们可能会借助平台，在其间扮演网关的角色进行管控，我们主要关注下面几点：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;API全生命周期管理：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;涵盖API的完整生命周期管理功能，可随时回滚指定环境到特定版本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;API流量控制：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;细致精确的流量控制，用户可根据自身业务对API服务进行流量配置，精确到秒级请求过滤和控制，避免突发高流量，导致后端服务出现过载。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;API认证安全：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在API访问时进行代理认证，保障不出现横向和纵向的越权访问。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;落地总结：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;这些工作的进行，一般是基于平台进行第三方管控的。&lt;/p&gt;

&lt;p&gt;但实际情况是，我们的系统代码里可能为了调试和兼容，不用强行要求认证也能进行访问。所以针对这些情况的治理，也是我们需要特别思考的。&lt;/p&gt;

&lt;h4 id=&quot;可信原则&quot;&gt;可信原则&lt;/h4&gt;

&lt;p&gt;在私有云建设中，由于我们需要遵循政策，做到IaaS和PaaS层面的安全合规，实现一定程度可信链路。&lt;/p&gt;

&lt;p&gt;而不是一味追求零信任，从而真实的优化我们的安全管理工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;流量可信：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先在日常的私有云管理中，每日管理员操作的时段是有限的，操作行为也是有限的，所以剩下的都是自动化运维的行为。&lt;/p&gt;

&lt;p&gt;剔除这部分流量后，剩下的异常流量行为审计，可以在自动化的分析下比较明显的突显出来，最终将减少我们溯源和应急所耗费的精力。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;账号可信：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在接入的账号里，管理级别权限的账号数量和操作行为应该做到最小化，细节操作应该更多的去依赖子账号。&lt;/p&gt;

&lt;p&gt;另外，针对租户内子账号的具体角色绑定，也需要进行细致配置核查，坚持实现权限最小化，防止错误配置导致事故的发生。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;边界隔绝：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;私有云由于其合规的特殊性，一般要求不能直接接入互联网边界。在这种情况下，我们对于私有云IDC数据流量是可控的，进出的数据可以做旁路和代理前置监控。&lt;/p&gt;

&lt;p&gt;另外，这种情况下一般也不能接入VPN和外部专线，即使需要实现异地多活进行容灾，可能也需要独立运维进行支持，不可一概而论。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;终端可信：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在我们在接入平台进行运维的时候，需要尝试遵循一致性原则，通过EUC（终端用户上下文）ticket，多维度证明初始请求者（original requester）的身份。&lt;/p&gt;

&lt;p&gt;我们在实践中颁发凭证时，可以尝试把设备和服务请求进行分别绑定，使得服务间授权后能完成互相信任。&lt;/p&gt;

&lt;p&gt;这样一来，即使出现了事故我们需要溯源，定位也会更加精确迅速。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;服务可信：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当不同的服务间进行RPC通信时，本身应该遵从Service Access Policy（服务访问策略），Service Mesh服务网格技术（比如Istio）能帮我们做好这点。&lt;/p&gt;

&lt;p&gt;在实际应用中，即使在同一租户内的不同微服务之间，默认不应该保持信任，需要其他的安全控制：例如认证和加密。&lt;/p&gt;

&lt;p&gt;同时，向微服务的转变，给我们提供了一个对传统安全模型进行重新思考的机会。&lt;/p&gt;

&lt;h3 id=&quot;参考文章&quot;&gt;参考文章&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://help.aliyun.com/document_detail/101318.html&quot;&gt;互联网边界防火墙
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/linjpg/article/details/98481182&quot;&gt;阿里云负载均衡SLB 详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/64ynVhD9jnMh0LTqAuXqQA&quot;&gt;网络安全架构 | IAM（身份访问与管理）架构的现代化
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.dbappsecurity.com.cn/show-43-220-1.html&quot;&gt;某医院智慧医疗网络安全项目案例（DMZ区域建设解决方案）
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arthurchiao.art/blog/google-beyondprod-zh/&quot;&gt;[译] BeyondProd：云原生安全的一种新方法（Google, 2019）
&lt;/a&gt;&lt;/p&gt;</content><author><name>HellSec</name></author><category term="cloud" /><summary type="html">对于私有云环境的安全建设工作中，需要做的事务繁杂而冗长。但我们在梳理要点后，会发现非传统边界的加固工作，也是我们应该重点关注的。</summary></entry><entry><title type="html">私有云安全-容器安全设计实践</title><link href="http://localhost:4000/cloud/2020/12/08/container-sec-design/" rel="alternate" type="text/html" title="私有云安全-容器安全设计实践" /><published>2020-12-08T21:40:00+08:00</published><updated>2020-12-08T21:40:00+08:00</updated><id>http://localhost:4000/cloud/2020/12/08/container-sec-design</id><content type="html" xml:base="http://localhost:4000/cloud/2020/12/08/container-sec-design/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;私有云安全由于其合规性和独立性，不同于公有云和混合云，需要单独定制落地的流程和规划，这里跟大家简单聊聊关于容器安全设计的相关问题。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-9-26/1601107557741-容器安全-0926.jpg&quot; alt=&quot;容器安全概述&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;容器安全综述&quot;&gt;容器安全综述&lt;/h3&gt;

&lt;p&gt;容器安全架构设计，可以在其三大关键生命周期阶段进行实施，其中包括镜像安全、配置安全、运行安全。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;镜像安全：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在初始阶段对镜像本身做把控，从供应链进行核查，在日常会对镜像做验证和校验，最后会在构建时进行阻断或者检测。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;配置安全：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于构建好的初始镜像，需要遵从行CIS、NIST最佳安全实践。在构建好系统后，需要检查相应的Docker配置、Kubernetes配置、以及操作系统本身。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;运行安全：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在容器运行的时候，我们会对里面的系统运行状态进行监控，还会对CMDB资产图谱状态进行对比核查。&lt;/p&gt;

&lt;h3 id=&quot;镜像安全&quot;&gt;镜像安全&lt;/h3&gt;

&lt;h4 id=&quot;嵌入流程检测&quot;&gt;嵌入流程检测&lt;/h4&gt;

&lt;p&gt;这部分工作可以结合CICD流水线进行，一般有两种建议。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;CI构建阻断：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;日常构建新镜像时，接入配置扫描和白盒扫描流程，如果核查出错会直接阻断，这种做法稍显粗暴，而且会影响业务，但这样可以减少镜像出现风险的可能性，后续重构建的工作也会相应减少。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;旁路分析：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;复制需要检测的镜像，进行模拟构建，出问题直接阻断。这样的好处是不会影响业务进程，但脆弱的镜像可能会被构建到生产环境，存在潜在风险。&lt;/p&gt;

&lt;p&gt;出现问题后，研发运维人员需要根据镜像标签去做追踪，剔除脆弱镜像或者排期重新构建。&lt;/p&gt;

&lt;p&gt;这种一般是在构建时，不会直接阻断流程，而是旁路提供鉴定报告，后续再供给安全运维人员进行分析。&lt;/p&gt;

&lt;p&gt;当然如果需要得到即时结果，则构建扫描可以提供API攻击给Docker CI平台，作为结果自动化处理的参考。&lt;/p&gt;

&lt;h4 id=&quot;镜像初始安全&quot;&gt;镜像初始安全&lt;/h4&gt;

&lt;h5 id=&quot;镜像库内控&quot;&gt;&lt;em&gt;镜像库内控&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;首先，对于镜像本身，由于企业自用的镜像大部分是规范定制化的，纯私有云落地的话不建议使用公网镜像库，可以看看harbor之类的，这也是为保障供应链风险可控。&lt;/p&gt;

&lt;p&gt;由于私有云合规性的特殊要求，一般是要求不能接入专线（与混合云不同），这样能在减少诸如水坑攻击，或者防止被黑客大规模挂马之类攻击误伤。&lt;/p&gt;

&lt;h5 id=&quot;镜像仓库核查&quot;&gt;&lt;em&gt;镜像仓库核查&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;对于初始镜像，需要定期核查软件镜像库，查看镜像是否被植入木马。如果私有镜像仓库由于配置不当而开启了2357端口，将会导致私有仓库暴露在公网中。&lt;/p&gt;

&lt;p&gt;这样的话，攻击者可以直接访问私有仓库并篡改镜像内容，存在仓库镜像被污染的隐患。&lt;/p&gt;

&lt;h5 id=&quot;镜像扫描核查&quot;&gt;&lt;em&gt;镜像扫描核查&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;镜像核查，主要会做两方面的工作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;第一，是进行常规漏洞检测，根据CVE漏洞库进行清单核查式扫描。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;第二，是对系统进行基线检查，把潜在的风险扼杀在摇篮里。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这部分核查结果，需要分布走进行治理，首先更新初始镜像，把相关镜像的缺陷统一修复。
然后，会根据线上业务和系统本身的稳定性和适配度，在测试环境调试回归并灰度完毕，再统一进行替换。&lt;/p&gt;

&lt;h5 id=&quot;镜像数字签名&quot;&gt;&lt;em&gt;镜像数字签名&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;Docker的内容信任（Content Trust）机制，可保护镜像在镜像仓库与用户之间传输过程中的完整性。目前。目前Docker的内容信任机制是默认关闭的，需要手动开启。&lt;/p&gt;

&lt;p&gt;内容信任机制启用后，镜像发布者可对镜像进行签名，而镜像使用者可以对镜像签名进行验证。&lt;/p&gt;

&lt;p&gt;镜像构建者在通过docker build命令运行Dockerfile文件前，需要通过手动或脚本方式将DOCKER_CONTENT_TRUST环境变量置为1进行启用。&lt;/p&gt;

&lt;p&gt;在内容信任机制开启后，push、build、create、pull、run等命令均与内容信任机制绑定，只有通过内容信任验证的镜像才可成功运行这些操作。例如，Dockerfile中如果包含未签名的基础镜像，将无法成功通过docker  build进行镜像构建。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;（命令示例：export DOCKER_CONTENT_TRUST = 1）
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;供应链核查&quot;&gt;&lt;em&gt;供应链核查&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;对于代码中使用的公共组件包，需要对来源进行核查。在私有云的落地实践中，最好是建立私有包的仓库，这样即使镜像被进行植入性攻击，也在可溯源和可控制的范围内。&lt;/p&gt;

&lt;p&gt;顺便提一下，企业日常使用的代码编译器，也需要在企业核准控制范围内。最好通过统一采购源，在内网提供正版的下载，以及提供内网激活API等方式。&lt;/p&gt;

&lt;p&gt;但是，在目前大环境下，由于版权和虚拟资产购置价格过高的问题，不少互联网大厂也很难统一解决这一点。&lt;/p&gt;

&lt;h3 id=&quot;配置安全&quot;&gt;配置安全&lt;/h3&gt;

&lt;h4 id=&quot;内部核查&quot;&gt;内部核查&lt;/h4&gt;

&lt;h5 id=&quot;dockerfile核查&quot;&gt;&lt;em&gt;Dockerfile核查&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;如果Dockerfile存在漏洞或被插入恶意脚本，那么生成的容器也可能产生漏洞或被恶意利用。例如，攻击者可构造特殊的Dockerfile压缩文件，在编译时触发漏洞获取执行任意代码的权限。&lt;/p&gt;

&lt;p&gt;如果在Dockerfile中没有指定USER，Docker将默认以root用户的身份运行该Dockerfile创建的容器，如果该容器遭到攻击，那么宿主机的root访问权限也可能会被获取。&lt;/p&gt;

&lt;p&gt;如果在Dockerfile文件中存储了固定密码等敏感信息，并对外进行发布，则可能导致数据泄露的风险。&lt;/p&gt;

&lt;p&gt;如果在Dockerfile的编写中添加了不必要的应用，如SSH、Telnet等，则会产生攻击面扩大的风险。&lt;/p&gt;

&lt;h5 id=&quot;metadata安全监控&quot;&gt;&lt;em&gt;Metadata安全监控&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;本身私有云环境是相对隔离的，但如果其中的数据访问鉴权方式不当，或者意外造成了安全密钥泄露的话，黑客可以比较容易的获取到敏感元数据。&lt;/p&gt;

&lt;p&gt;所以在安全配置和访问控制上，我们仍旧需要在日常做好监控工作。&lt;/p&gt;

&lt;h4 id=&quot;攻击面核查&quot;&gt;攻击面核查&lt;/h4&gt;

&lt;h5 id=&quot;强制访问控制&quot;&gt;&lt;em&gt;强制访问控制&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;强制访问控制（Mandatory Access Control, MAC）是指每一个主体（包括用户和程序）和客体都拥有固定的安全标记，主体能否对客体进行相关操作，取决于主体和客体所拥有安全标记的关系。&lt;/p&gt;

&lt;p&gt;在Docker容器应用环境下，可通过强制访问控制机制限制容器的访问资源。Linux内核的强制访问控制机制包括SELinux、AppArmor等。&lt;/p&gt;

&lt;h5 id=&quot;内容信任机制&quot;&gt;&lt;em&gt;内容信任机制&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;Linux内核能力表示进程所拥有的系统调用权限，决定了程序的系统调用能力。&lt;/p&gt;

&lt;p&gt;因此，不当的容器能力配置可能会扩大攻击面，增加容器与宿主机面临的安全风险。&lt;/p&gt;

&lt;p&gt;在执行docker run命令运行Docker容器时可根据实际需求通过–cap-add或–cap-drop配置接口对容器的能力进行增删，或者尝试通过配置文件进行统一调整。&lt;/p&gt;

&lt;h5 id=&quot;构建资管图谱&quot;&gt;&lt;em&gt;构建资管图谱&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;枚举初始镜像内的软件包、端口、服务，构建初始清单列表。&lt;/p&gt;

&lt;p&gt;如果私有云安全架构体系中，本身可以考虑构建类似于CMDB资产管理的系统，也可以考虑搭配像Dependency-track等工具，核查对于第三方的依赖。&lt;/p&gt;

&lt;p&gt;否则的话，可以通过灰盒扫描和流量agent方式，对资产数据库进行补充检查。&lt;/p&gt;

&lt;p&gt;同时，一旦出现异常，通过自动化对比初始状态数据，我们也能较快地发现风险。&lt;/p&gt;

&lt;h3 id=&quot;运行安全&quot;&gt;运行安全&lt;/h3&gt;

&lt;h5 id=&quot;敏感信息检查&quot;&gt;&lt;em&gt;敏感信息检查&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;核查运行的镜像系统的代码中，是否存在敏感信息泄露：密码、秘钥、token等。&lt;/p&gt;

&lt;p&gt;这个问题比较微妙，可能更偏向普适性的安全问题，但作为系统运行时安全，还是要提一下的。&lt;/p&gt;

&lt;h5 id=&quot;黑盒扫描&quot;&gt;&lt;em&gt;黑盒扫描&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;在容器中系统的运行时，可能会无意中暴露一些攻击面，因此我们需要选择适当的扫描器进行定期核查。&lt;/p&gt;

&lt;p&gt;这里简单例举几个黑盒扫描引擎：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;nessus（网络主机漏洞扫描器）&lt;/li&gt;
  &lt;li&gt;awvs（web爬虫扫描器）&lt;/li&gt;
  &lt;li&gt;appscan（IBM重量级web扫描器）&lt;/li&gt;
  &lt;li&gt;dirscan（web路径扫描器）&lt;/li&gt;
  &lt;li&gt;docker-bench-security（官方基线核查脚本）&lt;/li&gt;
  &lt;li&gt;anchore（针对容器Docker的CVE扫描）&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;白盒扫描&quot;&gt;&lt;em&gt;白盒扫描&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;白盒扫描的话，同样也可以嵌入CICD流水线进行检查，如果出现问题，对比实际情况（比如命中率）进行阻断和漏洞上报。&lt;/p&gt;

&lt;p&gt;当然，如果误报率较高的话，可以仅从旁路记录，等待安全人员审核漏洞是否存在后，再督促业务方修复迭代。&lt;/p&gt;

&lt;p&gt;如果扫描规则有更新，也可以日常对代码库进行扫描，匹配筛选出可能存在缺陷的分支进行告警，以便及时更新镜像系统中的代码。&lt;/p&gt;

&lt;p&gt;简单例举几类白盒扫描引擎：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SonarQube（定制安全规则）&lt;/li&gt;
  &lt;li&gt;BlackDuck（开源组件检查）&lt;/li&gt;
  &lt;li&gt;Findsecbug（开源的findbug安全插件，可作用于编译器和平台）&lt;/li&gt;
  &lt;li&gt;Coverity（可以定制安全规则）&lt;/li&gt;
  &lt;li&gt;Fortify（误报较多，但覆盖面较全，pj版本多）&lt;/li&gt;
  &lt;li&gt;Semmle QL（官方有分析平台lgtm.com）&lt;/li&gt;
  &lt;li&gt;KunLun-M（404实验室的开源产品）&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;安全监控&quot;&gt;安全监控&lt;/h4&gt;

&lt;h5 id=&quot;资产图谱核查&quot;&gt;&lt;em&gt;资产图谱核查&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;对于在配置阶段获取的清单图谱，需要在系统运行阶段，进行资产定期巡检对比核查。&lt;/p&gt;

&lt;p&gt;如果容器中的系统，开放了异常端口和服务，或者安装了异常的软件包，应该对该镜像构建的相关容器系统，通过核实后尽快进行冻结处理。&lt;/p&gt;

&lt;h5 id=&quot;hids监控&quot;&gt;&lt;em&gt;HIDS监控&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;在容器系统运行的过程中，如果出现被入侵或者污染的情况，仅凭日常的扫描和事前事中检查，不一定能及时精确的定位溯源。&lt;/p&gt;

&lt;p&gt;这时就需要对文件变更进行监控，以及对异常进程及时发现，还有记录异常行为和疑似的后门留存等等。&lt;/p&gt;

&lt;p&gt;云厂商们的技术各有千秋，无论是宿主机级别，还是能植入容器的HIDS，都能一定程度上提升容器的安全兜底能力。&lt;/p&gt;

&lt;h5 id=&quot;容器进程安全审计&quot;&gt;&lt;em&gt;容器进程安全审计&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;在安全审计方面，对于运行Docker容器的宿主机而言，除需对主机Linux文件系统等进行审计外，还需对Docker守护进程的活动进行审计。&lt;/p&gt;

&lt;p&gt;由于系统默认不会对Docker守护进程进行审计，需要通过主动添加审计规则或修改规则文件进行。&lt;/p&gt;

&lt;p&gt;除Docker守护进程之外，还需对与Docker的运行相关的文件和目录进行审计，同时需要配置审计规则文件（/etc/audit/audit.rules）。&lt;/p&gt;

&lt;p&gt;命令示例：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auditctl -w /usr/bin/docker -k docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;容器网络安全&quot;&gt;&lt;em&gt;容器网络安全&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;由于Docker容器默认的网桥模式，不会对网络流量进行控制和限制。所以为了防止网络被DoS攻击的风险，需要根据实际需求对网络流量进行相应的控制。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;针对openstack的网络&lt;/em&gt;：&lt;/p&gt;

&lt;p&gt;在&lt;strong&gt;同一主机内相同子网中的不同容器之间&lt;/strong&gt;，可以通过建立的虚拟化集群，通过vlan对不同租户进行子网隔离不同，基于overlay网络的容器集群，默认可以直接访问。&lt;/p&gt;

&lt;p&gt;如需控制&lt;strong&gt;宿主机外部到内部容器应用的访问&lt;/strong&gt;，可通过在宿主机iptables中的DOCKER-INGRESS链，手动添加ACL访问控制规则，以控制宿主机的eth0到容器的访问，或者在宿主机外部部署防火墙等方法实现。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;针对k8s的网络：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;策略可以应用于通过常用标签标识的pod组。然后，可以使用标签来模拟传统的分段网络，这些网络通常用于在多层应用程序中隔离层：例如，您可以通过特定的“段”标签来标识前端和后端pod。策略控制这些段之间的流量，甚至控制来自外部源的流量。&lt;/p&gt;

&lt;p&gt;由于存在频繁的微服务动态变化更新，通过手动的方式配置iptables或更新防火墙是不现实的。因此，可通过微分段（Micro-Segmentation）实现面向容器云环境中的容器防火墙。&lt;/p&gt;

&lt;p&gt;微分段是一种细粒度的网络分段隔离机制，与传统的以网络地址为基本单位的网络分段机制不同，微分段可以以单个容器、同网段容器、容器应用为粒度实现分段隔离，并通过容器防火墙对实现微分段间的网络访问控制。&lt;/p&gt;

&lt;h5 id=&quot;容器认证安全&quot;&gt;&lt;em&gt;容器认证安全&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;我们在进行集群的控制时，需要注意做好访问控制，而k8s恰好自带了一套完整的认证授权机制。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于属性的访问控制(ABAC)，基于属性的访问控制(ABAC)定义了一种访问控制范式，通过将属性组合在一起的策略将访问权限授予用户。策略可以使用任何类型的属性(用户属性、资源属性、对象、环境属性等)。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于角色的访问控制(RBAC)是一种基于企业中单个用户的角色来调节对计算机或网络资源的访问的方法。在此上下文中，访问是单个用户执行特定任务的能力，例如查看、创建或修改文件。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NODE 授权——一个特殊用途的授权器，根据调度到kubelet所在节点的pod向kubelet授予权限。有关使用节点授权模式的更多信息，请参见节点授权。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WEBHOOK 授权——WebHook是当某个事件发生时触发一个HTTP POST的回调;实现webhook的web应用程序将向URL发送一条消息。有关使用Webhook模式的更多信息，请参见Webhook模式。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在进行访问控制时，k8s采用的是链式认证, 每个请求被认证插件验证时，插件会试图将请求与以下属性进行关联：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;api server 认证方式(k8s 的所有访问都是通过 api server)&lt;/li&gt;
  &lt;li&gt;https 证书认证: 基于CA根证书签名的双向数字证书认证方式&lt;/li&gt;
  &lt;li&gt;http token 认证: 通过一个token来识别合法用户&lt;/li&gt;
  &lt;li&gt;http basic 认证: 通过用户名密码的方式认证&lt;/li&gt;
  &lt;li&gt;authenticating proxy: 第三方授权协议&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当启用多个验证器模块时，第一个模块将成功地验证 “请求短路评估” (request short-circuits evaluation)，如果验证失败，则进行断路操作，api服务器不保证运行验证器的执行顺序。&lt;/p&gt;

&lt;p&gt;所有通过验证的用户都会被添加进system:authenticated组。&lt;/p&gt;

&lt;p&gt;可以使用authenticating proxy 或authentication webhook与其他身份验证协议(LDAP、SAML、Kerberos、备用x509方案等)集成。&lt;/p&gt;

&lt;h5 id=&quot;容器及平台日志分析&quot;&gt;&lt;em&gt;容器及平台日志分析&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;Kubernetes本身没有提供集群级别的日志管理功能，如想实现集群级别的日志管理有三种方案：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在每个Node中运行日志采集代理，将日志收集到集中的日志管理平台。这种方案对应用没有侵入性，是优选方案。&lt;/li&gt;
  &lt;li&gt;在前一种方案的基础上，在每个应用Pod中增加Sidecar容器来实现日志的分离。&lt;/li&gt;
  &lt;li&gt;应用直接将日志输出到统一的日志管理平台，不在本地落地，这种方案对于应用的侵入性较大。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实践过的方式是第一种，一般k8s集群的容器日志都存储在/var/lib/docker/。所以简单来说，本方式就是在每个node上各运行一个日志代理容器，对各个节点/var/log和/var/lib/docker/containers/两个目录下的日志进行采集，然后汇总到elasticsearch集群，最后通过kibana展示。&lt;/p&gt;

&lt;p&gt;至于中间流程的规则筛选和检测策略，可以借助自定义或者Sisdig之类工具的规则来配置。&lt;/p&gt;

&lt;h3 id=&quot;市场方案调研&quot;&gt;市场方案调研&lt;/h3&gt;

&lt;p&gt;当然，大家如果预算足够的话，可以直接采购整套云管平台和配套资源。
我前面所描述的东西，在大点的云服务厂商那里，基本是都可以满足的。&lt;/p&gt;

&lt;h4 id=&quot;国内厂商部署方案&quot;&gt;国内厂商部署方案&lt;/h4&gt;

&lt;p&gt;近期由于接洽项目的需要，对接触过的几家国内的厂商容器安全方案进行过调研。&lt;/p&gt;

&lt;p&gt;其中包含云厂商，也有传统安全厂商，笔者对其中部分指标进行了简要统计，这里暂不讨论未接触过的其他优秀厂商。&lt;/p&gt;

&lt;p&gt;这里仅针对公开信息披露进行展示，不做具体的优劣评价，可能存在错漏和沟通失误的情况，欢迎各路大佬斧正。&lt;/p&gt;

&lt;p&gt;另外，这里给个小建议，如果公司预算比较充裕，可以入手头部云厂商的产品进行环境自建。否则可以考虑采购二线云厂商+传统安全厂商，也能达到既定的大部分效果。&lt;/p&gt;

&lt;p&gt;如果收尾的架构设计中，还剩下一些低性价比，但是不得不上的资源。届时手中的业务方研发能力足够作为补充的话，可以二改和对接开源的系统，逐步推进实施落地。&lt;/p&gt;

&lt;p&gt;另外，需要特别注意业务方的需求，云环境的要求是否必须纯粹，是否需要等保等级规划，不同的需求成本差别可能会很大。&lt;/p&gt;

&lt;p&gt;这里附上调研过的几家厂商能力矩阵，不代表本人观点，如有错漏欢迎指出：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;安全能力&lt;/th&gt;
      &lt;th&gt;阿里&lt;/th&gt;
      &lt;th&gt;腾讯&lt;/th&gt;
      &lt;th&gt;华为&lt;/th&gt;
      &lt;th&gt;深信服&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;隔离沙箱&lt;/td&gt;
      &lt;td&gt;ACK安全隔离沙箱&lt;/td&gt;
      &lt;td&gt;未知&lt;/td&gt;
      &lt;td&gt;仅数据沙箱&lt;/td&gt;
      &lt;td&gt;未知&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;基线检查&lt;/td&gt;
      &lt;td&gt;安骑士&lt;/td&gt;
      &lt;td&gt;未知&lt;/td&gt;
      &lt;td&gt;HSS&lt;/td&gt;
      &lt;td&gt;BVT&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;漏洞检查&lt;/td&gt;
      &lt;td&gt;CSS&lt;/td&gt;
      &lt;td&gt;VSS&lt;/td&gt;
      &lt;td&gt;VSS&lt;/td&gt;
      &lt;td&gt;传统漏扫&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;资产核查&lt;/td&gt;
      &lt;td&gt;资产管理（专有云）+指纹核查（企业版）&lt;/td&gt;
      &lt;td&gt;云资产管理中心&lt;/td&gt;
      &lt;td&gt;ROMA资产中心&lt;/td&gt;
      &lt;td&gt;未知&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;主机安全&lt;/td&gt;
      &lt;td&gt;安骑士&lt;/td&gt;
      &lt;td&gt;CWP&lt;/td&gt;
      &lt;td&gt;HSS&lt;/td&gt;
      &lt;td&gt;EDR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;密钥管理KMS&lt;/td&gt;
      &lt;td&gt;KMS&lt;/td&gt;
      &lt;td&gt;HSM&lt;/td&gt;
      &lt;td&gt;HSM&lt;/td&gt;
      &lt;td&gt;未知&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;是否支持纯私有云&lt;/td&gt;
      &lt;td&gt;不支持&lt;/td&gt;
      &lt;td&gt;支持&lt;/td&gt;
      &lt;td&gt;不支持&lt;/td&gt;
      &lt;td&gt;可配合采购&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;参考文章&quot;&gt;参考文章&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freebuf.com/column/173989.html&quot;&gt;Docker容器安全最佳实践白皮书V1.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/ajianboke/p/10917776.html&quot;&gt;Kubernetes集群安全配置案例&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aliyun.com/solution/security/containersecurity&quot;&gt;阿里云容器安全解决方案&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://weekly.dockerone.com/article/10098&quot;&gt;容器安全在证券行业的最佳实践&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/squirrelanimal0922/article/details/82492557&quot;&gt;10个确保微服务与容器安全的最佳实践&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freebuf.com/articles/es/239266.html&quot;&gt;使用Anchore Engine来完善DevSecOps工具链&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freebuf.com/articles/system/221319.html&quot;&gt;Docker容器安全性分析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://juejin.im/entry/577083fd5bbb5000596a1528&quot;&gt;Docker 安全&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/java_zyq/article/details/82179175&quot;&gt;从零开始搭建K8S–如何监控K8S集群日志&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43671511&quot;&gt;Docker安全入门与实战：Sysdig&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://baijiahao.baidu.com/s?id=1610191436506146444&quot;&gt;谈一下Docker与Kubernetes集群的日志和日志管理&lt;/a&gt;&lt;/p&gt;</content><author><name>HellSec</name></author><category term="cloud" /><summary type="html">私有云安全由于其合规性和独立性，不同于公有云和混合云，需要单独定制落地的流程和规划，这里跟大家简单聊聊关于容器安全设计的相关问题。</summary></entry><entry><title type="html">漏洞POC验证系统实践</title><link href="http://localhost:4000/scanner/2020/08/11/all-info-scan/" rel="alternate" type="text/html" title="漏洞POC验证系统实践" /><published>2020-08-11T20:46:00+08:00</published><updated>2020-08-11T20:46:00+08:00</updated><id>http://localhost:4000/scanner/2020/08/11/all-info-scan</id><content type="html" xml:base="http://localhost:4000/scanner/2020/08/11/all-info-scan/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;在渗透过程中，我们针对特定的系统，在通过插件识别类型后，可以利用漏洞脚本进行fuzz。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-30/1606731837227-%E6%BC%8F%E6%B4%9E%E9%AA%8C%E8%AF%81POC%E7%B3%BB%E7%BB%9F.jpg&quot; alt=&quot;漏洞POC验证系统全景图&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;系统综述&quot;&gt;系统综述&lt;/h3&gt;

&lt;p&gt;在本系统设计之初，是采用的也是分布式平台设计架构，后来因为考虑和分布式资产扫描平台兼容接口，最后为了解耦合，完全改成了单机版。&lt;/p&gt;

&lt;p&gt;首先我们可以看到，在这个地方我们没有单独设计web管理端，只能通过命令行去调度。&lt;/p&gt;

&lt;p&gt;但是，在设计时预留了守护进程rest api，可以接受第三方平台发来的调度请求。&lt;/p&gt;

&lt;p&gt;在插件模块调用方面，主模块有三个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;流行漏洞插件：主要用于复现常见手工测试用的手段，以及部分团队挖掘的内部漏洞。&lt;/li&gt;
  &lt;li&gt;口令漏洞插件：主要包含端口、中间件、应用的未授权和弱口令漏洞，包含弱口令字典。&lt;/li&gt;
  &lt;li&gt;第三方漏洞插件：主要用于接入网上的部分开源和泄露的插件，用于结果整合和性能调优。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在获取目标信息时，主要有下面几种的形式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;api调度：第三方平台通过rpc传递目标信息，对本系统的api进行调度。&lt;/li&gt;
  &lt;li&gt;命令行调度：通过命令行参数传递目标信息，直接进行调度。&lt;/li&gt;
  &lt;li&gt;资产导入调度：通过接入接口的方式，对于第三方平台api给出的数据进行调度扫描。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本身在分布式资产扫描平台，是存在cms类别和应用类型信息的落库的。但是为了考虑内网的情况，还是单独提取了两个子模块出来：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cms鉴别插件：主要针对目标进行cms类型的鉴别，如目标匹配到本地的指纹库，会给他打上标签，否则会接入互联网查询接口。&lt;/li&gt;
  &lt;li&gt;应用鉴别插件：主要针对cms类型进行补充，识别服务和应用的类型打上标签，作为第二梯队尝试。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，除了这些模块，还有部分次要的效果优化类插件，这里就不再多提了。&lt;/p&gt;

&lt;p&gt;最后，我们简单讲讲扫描结果落库的问题。&lt;/p&gt;

&lt;p&gt;这里采用的是log打印存储+数据库回传分布式资产扫描平台，而回传的选项是可以关闭的，这就保障了我们在苛刻环境中本系统的兼容性。&lt;/p&gt;

&lt;h3 id=&quot;坑点总结&quot;&gt;坑点总结&lt;/h3&gt;

&lt;p&gt;在对于内网系统，或者存在敏感防火墙的系统进行扫描时，我们可支持接入多类型的代理。&lt;/p&gt;

&lt;h4 id=&quot;速率控制&quot;&gt;速率控制&lt;/h4&gt;

&lt;p&gt;在针对敏感的server进行探测时，第三方插件可能自带的口令认证爆破机制，会比较粗暴。&lt;/p&gt;

&lt;p&gt;要么就是单线程转，要么就是起个粗放的线程池，容易把服务器给跑挂了，或者让人IDS给很快查到。&lt;/p&gt;

&lt;p&gt;所以针对这块儿的控制，我们需要做细致优化，采用动态控制速率的策略，而不光是硬编码配置下发任务。&lt;/p&gt;

&lt;h4 id=&quot;数据统一&quot;&gt;数据统一&lt;/h4&gt;

&lt;p&gt;对于第三方插件，我们花了不少功夫在统一调度机制和库文件上，还有一点比较关键的点，是针对落库的格式上。&lt;/p&gt;

&lt;p&gt;我们在各类插件的上报流程中，对于第三方的插件，会尽量进行数据上报层hook，统一格式后进行上报。&lt;/p&gt;

&lt;p&gt;但并不是每一类插件都有统一落库记录的流程的，对于这类插件可能需要做函数重写。&lt;/p&gt;

&lt;h4 id=&quot;探测尺度&quot;&gt;探测尺度&lt;/h4&gt;

&lt;p&gt;我们的插件目前都是点到为止，为了遵守法律法规层面的制度，都没有进行漏洞深度利用，需要后续人工进行利用和复核。&lt;/p&gt;

&lt;h3 id=&quot;未来企划&quot;&gt;未来企划&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;后渗透利用：目前是没有关于内网利用信息收集的插件的，后面进行后相关poc的开发。&lt;/li&gt;
  &lt;li&gt;云环境利用：对于云环境漏洞目前没有poc，后面会考虑添加。&lt;/li&gt;
  &lt;li&gt;安全设备利用：对于目前流行的对于安全设备的反攻，后面会考虑专门添加插件。&lt;/li&gt;
  &lt;li&gt;WAF、蜜罐探测：对于WAF类和蜜罐环境，以前只是封装了辅助类的函数，后面也会提供专门的插件进行探测。&lt;/li&gt;
&lt;/ul&gt;</content><author><name>HellSec</name></author><category term="scanner" /><summary type="html">在渗透过程中，我们针对特定的系统，在通过插件识别类型后，可以利用漏洞脚本进行fuzz。</summary></entry><entry><title type="html">分布式资产扫描平台实践</title><link href="http://localhost:4000/scanner/2020/07/29/poc-scan/" rel="alternate" type="text/html" title="分布式资产扫描平台实践" /><published>2020-07-29T21:28:00+08:00</published><updated>2020-07-29T21:28:00+08:00</updated><id>http://localhost:4000/scanner/2020/07/29/poc-scan</id><content type="html" xml:base="http://localhost:4000/scanner/2020/07/29/poc-scan/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;以前在乙方干活的时候，对于黑盒扫描的内容研究的比较多也早。此前做了一些内部项目，但没有走涉密。本来还想出来做做开源，不过后来看着不挣钱，加上代码水准有限，也就作罢了。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;时过境迁，相关内容也不太敏感了，项目被人接手以后，应该已经做了不少迭代。&lt;/p&gt;

&lt;p&gt;这里主要想跟大家分享下，原来在建设扫描平台中遇到的思路，文中会拿以前的项目进行脱敏式分析。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://gitee.com/hellsec/ppp/raw/master/2020-11-29/1606648128250-%E5%9F%BA%E7%A1%80%E5%85%A8%E6%99%AF%E5%9B%BE.png&quot; alt=&quot;分布式资产采集平台基础架构&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;平台综述&quot;&gt;平台综述&lt;/h3&gt;

&lt;p&gt;通过图中的内容可以看到，我们的平台是通过两种方式调度的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于命令行。&lt;/li&gt;
  &lt;li&gt;基于flower（web api）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们通过在主控服务器，调度celery借助中间件redis，将任务下发到各个agent节点。&lt;/p&gt;

&lt;p&gt;此后，各个节点会根据规则，随机抽取proxy代理池里的ip，以期达到隐匿自身的作用。&lt;/p&gt;

&lt;p&gt;然后，节点会通过对基础模块的复杂调用，向目标发送探测请求包，实现对信息探测结果的落库。&lt;/p&gt;

&lt;p&gt;最后，我们在反馈数据回主控服务器数据库时，会采用加密方式，以免被中间人侦听。&lt;/p&gt;

&lt;p&gt;我们的主要模块有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;信息探测：实现对目标开放的ip或者域名，进行端口、端口banner、服务类型和版本等基础信息探测。&lt;/li&gt;
  &lt;li&gt;cms识别：主要对探测到web类型的目标，进行cms识别。&lt;/li&gt;
  &lt;li&gt;社工引擎：对多个搜索引擎进行爬取，并辅以github之类的第三方接口进行补充。&lt;/li&gt;
  &lt;li&gt;端口服务扫描：调用nmap和masscan接口，并辅以fofa、shodan之类的接口进行补充。&lt;/li&gt;
  &lt;li&gt;路径爆破：对爬取到域名去重探活后，如不存在waf进行暴力扫描，如存在进行智能低频探测。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那么，我们在以上的模块进行交叉调用后，又能得到什么结果呢：&lt;/p&gt;

&lt;p&gt;基础信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;目标base结果：包含ip、端口、端口banner、服务类型和版本。&lt;/li&gt;
  &lt;li&gt;cms结果：包含cms类型、版本，如果没有会优先展示webserver。&lt;/li&gt;
  &lt;li&gt;端口扫描结果：分析出真实开放的端口，筛选出其服务类型和版本。&lt;/li&gt;
  &lt;li&gt;路径爆破结果：分析状态码和返回的网页内容，去重找出真实的接口和路径集合。&lt;/li&gt;
  &lt;li&gt;dns资产：ip域名的基础映射集合。&lt;/li&gt;
  &lt;li&gt;email资产：对于某个实体（或者根域名）的所有email资产组合。&lt;/li&gt;
  &lt;li&gt;敏感词资产：对于某个实体（或者根域名）的所有敏感信息组合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整合信息（包含基础信息）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;主机资产：针对单台主机的所有基础信息探测合集。&lt;/li&gt;
  &lt;li&gt;子域名资产：针对特定域名的所有子域名的基础信息合集。&lt;/li&gt;
  &lt;li&gt;子网检测结果：针对特定实体下单IP段的主机资产的基础信息合集。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;对接服务&quot;&gt;对接服务&lt;/h3&gt;

&lt;p&gt;我们讲到了我们获取的资产合集，那么我们可以对接的服务又有哪些呢？&lt;/p&gt;

&lt;p&gt;前面的&lt;a href=&quot;http://blog.hellsec.net/pentest/2017/12/16/architecture-of-apt-pentest/&quot;&gt;《规模渗透架构部署设想》&lt;/a&gt;里，我们提到了一些规划，目前已经研发落地的有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;漏洞POC验证系统：针对采集到的资产数据，针对CMS类型进行验证，如果不能识别会使用通用的脚本进行fuzz。&lt;/li&gt;
  &lt;li&gt;渗透方案查询：类似于私有化的wiki平台，这个在前司曾经维护了一段时间，但后来改成了云笔记协作。&lt;/li&gt;
  &lt;li&gt;漏洞分储系统：当时爬取了seebug等几个国内外知名漏洞库，并单独提取poc，后来由于各大接口经常改动，精力不足暂停维护。&lt;/li&gt;
  &lt;li&gt;被动漏扫系统：这个落地项目在前规划里没有提到，依赖离线web流量会多一些，主要规则涵盖owasp top10，以及主流api漏洞检测，目前暂时没有主动接入本平台api，只会尝试提取生成的数据库结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;适用匹配&quot;&gt;适用匹配&lt;/h3&gt;

&lt;p&gt;在当初设计平台时，碰到两个比较重要的问题。&lt;/p&gt;

&lt;h4 id=&quot;兼容性&quot;&gt;&lt;em&gt;兼容性&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;由于本身设计的属于外网资产扫描的平台，在内网渗透时会比较尴尬，很多特性都不太匹配。&lt;/p&gt;

&lt;p&gt;在进行内网渗透时，兼容性都会比单机版要差很多。&lt;/p&gt;

&lt;p&gt;这点在以前跟前大boss讨论时，没少被喷被教育。当然他以价值输出为导向，后来觉得讲的还是很有道理的。&lt;/p&gt;

&lt;p&gt;所以后面再考虑是单开分支，还是单写逻辑在这个项目进行内网模块处理。&lt;/p&gt;

&lt;h4 id=&quot;便携性&quot;&gt;&lt;em&gt;便携性&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;系统本身是分布式的，单机版本也可以，但是启动运行效率不高。&lt;/p&gt;

&lt;p&gt;当初考虑做时候，考虑过打包docker镜像，但是前大boss不太认，觉得在出去搞环境比较复杂，镜像不一定能找机会pull。&lt;/p&gt;

&lt;p&gt;后来，考虑的是一键安装脚本，将容易出问题的库尽量用常规库替代，保证能一键启动关闭，然后任务在丢失后有重试和完整重放机制等等，即在容错兜底上加强了不少。&lt;/p&gt;

&lt;h3 id=&quot;后话&quot;&gt;后话&lt;/h3&gt;

&lt;p&gt;本文大概介绍了对于平台架构、资产收集结果的方案设计要略，后面会单独分析几个对接服务的细节，以及一些新的想法。&lt;/p&gt;</content><author><name>HellSec</name></author><category term="scanner" /><summary type="html">以前在乙方干活的时候，对于黑盒扫描的内容研究的比较多也早。此前做了一些内部项目，但没有走涉密。本来还想出来做做开源，不过后来看着不挣钱，加上代码水准有限，也就作罢了。</summary></entry><entry><title type="html">漏洞扫描：从fuzz到payload</title><link href="http://localhost:4000/scanner/2019/11/19/from-fuzz-to-payload/" rel="alternate" type="text/html" title="漏洞扫描：从fuzz到payload" /><published>2019-11-19T15:53:00+08:00</published><updated>2019-11-19T15:53:00+08:00</updated><id>http://localhost:4000/scanner/2019/11/19/from-fuzz-to-payload</id><content type="html" xml:base="http://localhost:4000/scanner/2019/11/19/from-fuzz-to-payload/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;在实现漏洞扫描的时候，部分开发者会使用payload直接进行fuzz，而且并没有对server端的拦截和过滤，有相对智能的反馈，这对扫描效率和成功率是影响比较大的。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;那么，我们该如何去实现精细化定制呢？&lt;/p&gt;

&lt;h3 id=&quot;字符fuzz&quot;&gt;字符fuzz&lt;/h3&gt;

&lt;p&gt;另外，在存在waf的时候，我们发送payload过多，可能直接会触发server端的拒绝服务或者ban ip的操作，所以要在保持一定的发包频率的情况下，尽可能少触发waf的规则。
另外，web系统本身可能自带了过滤，如果无脑发payload，可能fuzz了半天都是白费功夫。
在字符形态的时候，如果能直接过滤掉一批有害或者无效字符，能较大程度上提升扫描器的效率。&lt;/p&gt;

&lt;h3 id=&quot;字符规则过滤&quot;&gt;字符规则过滤&lt;/h3&gt;

&lt;p&gt;在检测时，先通过&lt;strong&gt;字符键值对&lt;/strong&gt;进行探测，如果不通过，则在黑名单库里加上该&lt;strong&gt;字符键值对&lt;/strong&gt;。然后，我们再接着尝试&lt;strong&gt;单边字符&lt;/strong&gt;是否被过滤，过滤了再把&lt;strong&gt;单边字符&lt;/strong&gt;加入黑名单，这样能在一定程度上提升检测效率。&lt;/p&gt;

&lt;p&gt;校验存在键值对的流程：
&lt;img src=&quot;https://s2.ax1x.com/2019/12/26/lEax00.png&quot; alt=&quot;lEax00.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;graph LR
字符键值对--&amp;gt;payload流程
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;需要注意的是，我们在进行字符fuzz的时候，别忘了加入定位的keyword，不然在复杂的response响应中，不太容易匹配到。&lt;/p&gt;

&lt;p&gt;键值对举例如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;lt;payload&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;(payload)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;||&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/*payload*/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;校验不存在键值对的流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/12/26/lEavmq.png&quot; alt=&quot;lEavmq.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;graph LR
字符键值对--&amp;gt;单个字符
单个字符--&amp;gt;payload流程
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;单个字符举例如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;?
'
&quot;
+
#
;
,
～
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;当然，有人会问是不是每种漏洞类型的检测，都需要按照上述list。&lt;/p&gt;

&lt;p&gt;很明显不是啊，每种漏洞检测需要用到的字符集是不一样的，所以需要根据实际检测的情况，以及所使用的payload库进行分类处理。&lt;/p&gt;

&lt;p&gt;最后，我们可以根据需要，生成&lt;strong&gt;字符（键值对）&lt;/strong&gt; 到黑名单库，从而转到&lt;strong&gt;payload流程&lt;/strong&gt;进行下一步判断。&lt;/p&gt;

&lt;h3 id=&quot;从fuzz到payload&quot;&gt;从fuzz到payload&lt;/h3&gt;

&lt;p&gt;那么我们已经获取到黑名单&lt;strong&gt;字符（键值对）&lt;/strong&gt;以后，该如何去尝试fuzz payload呢？&lt;/p&gt;

&lt;p&gt;每个人习惯不一样，这里可以直接提取黑名单里的内容作为list，接着挨个对payload进行对比尝试，如果满足黑名单条件，则从fuzz列表里去除掉。&lt;/p&gt;

&lt;p&gt;这样一来，能在一定程度上减少fuzz发包数，简易代码示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# /usr/bin/python2.X
...
...

#init payload list
fuzz_payload = orig_payload

for y in orig_payload:
    for x in char_blacklist:
        #x may be: '(.*?)'
        if re.match(x, y):
            fuzz_payload.remove(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;手工简化payload&quot;&gt;手工简化payload&lt;/h3&gt;

&lt;p&gt;拿刷众测举例，我们常常需要在短时间内定位到一个参数是否含有漏洞，配合自动化提交快人一步。&lt;/p&gt;

&lt;p&gt;就sql注入而言，某某曾说过一个参数发两个包就能确认是否含有注入，发四个包就能确定是什么数据库。&lt;/p&gt;

&lt;p&gt;当然可能讲的稍微夸张，但理确实是这个理。&lt;/p&gt;

&lt;p&gt;我们先需要通过fuzz点，来确认单参数是否含有sql注入。这里可以提下，一旦结束单参数fuzz，我们可以直接结束掉该url所有参数的fuzz。&lt;/p&gt;

&lt;p&gt;另外，不少厂商是是接受多点同类漏洞的，这意思是拿到某个或者某类参数，我们可以去尝试自动化类比fuzz其他地方，是否也存在相似问题。&lt;/p&gt;

&lt;p&gt;过滤掉前面的的黑名单字符集，把poc打散然后闭合和注释，再通过判断content-length差距、页面hash变化、时间延时变化，分两步检测快速fuzz出初步结果。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;or 8 &amp;lt; 9
or 0 between 8 and 9
and 8 &amp;gt; 9
xor 8 &amp;gt; 9
xor 0 between 8 and 9
or sleep(8)
and sleep(8)
xor sleep(8)
;select sleep(8)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;接着，我们可以通过主流数据库的&lt;strong&gt;单个甄别性&lt;/strong&gt;查询语句（比如mssql/oracle/mysql/…），分独立的包发送进行探测，这样能很快的作出判断。&lt;/p&gt;

&lt;p&gt;可能有人会说，这样可能会漏掉部分特殊的点。&lt;/p&gt;

&lt;p&gt;拜托，众测的时候就是比手快的，你说一个url所有参数都跑一遍的话，就拿sqlmap举例，如果某个点稍微有点特殊，可能几分钟都过去了，才能判断那个点是不是有注入。&lt;/p&gt;

&lt;p&gt;如果你在半小时后找到个注入，人家已经在十来分钟的时候提交了，不把人气的吐血三升？&lt;/p&gt;

&lt;p&gt;而且，并不是说我们就不关注细节。只不过是做深入fuzz的优先级是相对的，我们需要优先关注cover的广度，而不是cover的深度。&lt;/p&gt;

&lt;p&gt;最后，我们可以最快速度得出结果后，再后续使用专用fuzz工具进行二段攻击。&lt;/p&gt;

&lt;h3 id=&quot;分级fuzz&quot;&gt;分级fuzz&lt;/h3&gt;

&lt;p&gt;另外，在发送payload之前，我们能够检测到系统框架的类型，是一个比较有利的起手，这代表我们可以不用盲目尝试payload。&lt;/p&gt;

&lt;p&gt;在使用网上的payload的同时，我们也需要去尽可能的去分级和简化。大家要相信，这玩意儿是给自己或者团队用的，不是用来忽悠客户的。&lt;/p&gt;

&lt;p&gt;搞一堆华丽丽的东西，最后自己吃没吃饱，只有自家的肚子知道。&lt;/p&gt;

&lt;p&gt;分级和简化payload后的结果就是，比如在使用传统扫描器（含接口）如sqlmap和awvs的时候，大家可能永远会觉得机器会帮你扫出来，永远有惰性不会去精简poc，别人一键化拿shell的时候，你可能才刚刚确认cms框架类型。&lt;/p&gt;

&lt;p&gt;最终，我们根据测试的场景，可以将手中的扫描器按测试需求，分级别去发送payload：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;level1: 最简单可以获取基础反馈的poc。&lt;/li&gt;
  &lt;li&gt;level2: 直接可以获取数据和证明危害的payload。&lt;/li&gt;
  &lt;li&gt;level3: 直接可以获取shell，或者证明rce/上传漏洞存在的txt文件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，如果这里调的level比较高的话，低级别的poc检测成功后，是可以向上跃迁检测的。&lt;/p&gt;

&lt;p&gt;另外，这里不建议优先采用公共dns平台，因为即使是使用了api交互，公共dns平台也一般会有延时的风险，更别提可能被GA监控管制，再者说证明截图啥的，也需要时间的。&lt;/p&gt;</content><author><name>HellSec</name></author><category term="scanner" /><summary type="html">在实现漏洞扫描的时候，部分开发者会使用payload直接进行fuzz，而且并没有对server端的拦截和过滤，有相对智能的反馈，这对扫描效率和成功率是影响比较大的。</summary></entry></feed>