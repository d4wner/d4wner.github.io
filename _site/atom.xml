<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-11-03T20:09:32+08:00</updated><id>http://localhost:4000/</id><title type="html">地狱爬行者’s Blog</title><subtitle>原创网络安全博客</subtitle><author><name>demon</name></author><entry><title type="html">域名采集爬虫</title><link href="http://localhost:4000/spider/2017/10/19/domain-spider/" rel="alternate" type="text/html" title="域名采集爬虫" /><published>2017-10-19T20:59:18+08:00</published><updated>2017-10-19T20:59:18+08:00</updated><id>http://localhost:4000/spider/2017/10/19/domain-spider</id><content type="html" xml:base="http://localhost:4000/spider/2017/10/19/domain-spider/">&lt;p&gt;
    &lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;
    前段时间在研究扫描器的问题，在涉及域名采集这块时， 突然有了一些特别的想法，所以单独将这个模块提取出来，做成了一套独立系统。
    &lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;对于域名采集技术，市面上大概有这样几种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DNS域传送泄露&lt;/li&gt;
  &lt;li&gt;接口查询（包括aizhan，shodan等）&lt;/li&gt;
  &lt;li&gt;字典枚举&lt;/li&gt;
  &lt;li&gt;Github泄露&lt;/li&gt;
  &lt;li&gt;搜索引擎泄露&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不过这里要谈的显然不是这种，尽管自研的扫描器里已经集成了前面说的这些，今天的主角是域名采集爬虫。&lt;/p&gt;

&lt;p&gt;那么何谓域名采集爬虫呢，其实也是老概念。我们要先从一个URL或者多个URL入口进行爬取。在开始，需要设定好爬行深度和允许爬行的根域名，通过不断地迭代存取和过滤，直到爬取完所有可触及的域名为止。&lt;/p&gt;

&lt;p&gt;这样的域名采集方法，较其他技术优势在于：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;能采到不被搜索引擎收录的可用域名。&lt;/li&gt;
  &lt;li&gt;囊括范围广，除去CDN影响，能更多地去收集目标资产。&lt;/li&gt;
  &lt;li&gt;细致入微，在域名采集普及的时代，厂商一般子域名都会多加注意起名，这样做能多收获一些隐藏域名。&lt;/li&gt;
  &lt;li&gt;可分布式，相对于普通的单机大字典爆破，可以选择性节省更多的时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然讲了这么多，其劣势也很明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将耗费更多的时间和各类资源，即使采用了分布式，由于需要考虑部分目标主机的脆弱性，建立的连接数还是有限的。&lt;/li&gt;
  &lt;li&gt;容易被封，WAF和各种IDS的规则在检测到爬虫后，可能会直接封禁IP，不过这点可以通过技术手段缓解。&lt;/li&gt;
  &lt;li&gt;不易部署，相对于常用的域名接口查询和爆破，这点的劣势还是相当明显的。&lt;/li&gt;
  &lt;li&gt;比较鸡肋，没有太多Team会专门为了爬取域名单独建立一个可用的爬虫体系，一般是扫描器只兼容了传统接口。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优劣对比还是比较明显的，这里不多做探讨，下面纯技术介绍。&lt;/p&gt;

&lt;p&gt;这里的系统采用的是scrapy+django+mongodb，每采集好一批数据后，将存入mongodb，其中的数据包括：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;页面源URL
子域名
根域名

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;后续其他项目将会解析生成的所有子域名，最后再进行资产汇总计算。这些本不属于此项目，不再细说。&lt;/p&gt;

&lt;p&gt;另外，在采集过程中，爬虫会在中间件（合规的做法如此，笔者则是直接在爬虫文件里进行的过滤），根据条件过滤重复和不合规的URL。&lt;/p&gt;

&lt;p&gt;代码其实很简单，现在已有的是前台查询+后台爬虫（命令行）的形式。后期如果项目可用，会加上前台scrapyd发布任务+MMQ分布式队列缓存。&lt;/p&gt;

&lt;p&gt;另外，以后也考虑集成传统接口、泄露查询和枚举模块。&lt;/p&gt;

&lt;p&gt;至于为啥现在没有做，因为懒。&lt;/p&gt;

&lt;p&gt;代码由于还需要优化，暂时没有放出，以后会补上。&lt;/p&gt;

&lt;p&gt;待续。&lt;/p&gt;</content><author><name>demon</name></author><category term="spider" /><summary type="html">前段时间在研究扫描器的问题，在涉及域名采集这块时， 突然有了一些特别的想法，所以单独将这个模块提取出来，做成了一套独立系统。</summary></entry><entry><title type="html">爬虫去重优化</title><link href="http://localhost:4000/spider/2017/10/14/url-duplicate-removal/" rel="alternate" type="text/html" title="爬虫去重优化" /><published>2017-10-14T08:05:18+08:00</published><updated>2017-10-14T08:05:18+08:00</updated><id>http://localhost:4000/spider/2017/10/14/url-duplicate-removal</id><content type="html" xml:base="http://localhost:4000/spider/2017/10/14/url-duplicate-removal/">&lt;p&gt;
    &lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;以前在做漏洞Fuzz爬虫时，曾做过URL去重相关的工作，当时是参考了seay法师的文章以及网上零碎的一些资料，感觉做的很简单。近来又遇到相关问题，于是乎有了再次改进算法的念头。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;首先，针对URL本身的去重，可以直接对整块URL进行处理。在参考网上的一些文章时，发现它们大多做了URL压缩存储。使用这些算法在数据量较大的时候，诚然能大幅减小存储的空间：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于磁盘的顺序存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于Hash算法的存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于MD5压缩映射的存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于嵌入式Berkeley DB的存储。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于布隆过滤器（Bloom Filter）的存储。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而对于URL逻辑上的去重，这里先给出seay文章中的相似度去重算法，大致是下面这样的：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def urlsimilar(url):
        hash_size=199999
        tmp=urlparse.urlparse(url)
        scheme=tmp[0]
        netloc=tmp[1]
        path=tmp[2][1:]
        query=tmp[4]
        #First get tail
        if len(path.split('/'))&amp;gt;1:
            tail=path.split('/')[-1].split('.')[-1]
            #print tail
        elif len(path.split('/'))==1:
            tail=path
        else:
            tail='1'
         #Second get path_length
        path_length=len(path.split('/'))-1
        #Third get directy list except last
        path_list=path.split('/')[:-1]+[tail]
        #Fourth hash
        path_value=0
        for i in range(path_length+1):
            if path_length-i==0:
                path_value+=hash(path_list[path_length-i])%98765
            else:
                path_value+=len(path_list[path_length-i])*(10**(i+1))

        #get host hash value
        netloc_value=hash(hashlib.new(&quot;md5&quot;,netloc).hexdigest())%hash_size
        url_value=hash(hashlib.new(&quot;md5&quot;,str(path_value+netloc_value)).hexdigest())%hash_size

        return url_value
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这段函数的大概作用是，最后它会根据算法返回一个hash值，这个hash值也就是该URL的hash相似度。如果两个URL计算出的hash值最后比较相等，我们则可以判断两个URL是具有较高的相似度的。&lt;/p&gt;

&lt;p&gt;但是这个函数应该是seay举例时随手提出的（这里强调下，免得被喷，后文不再细说），只是简单做了demo，并没有进行细化检验。在比较粗糙的情况下，该算法确实能剔除一些简单的参数重复的情况，但一旦参数复杂或者url不规范，是不太能很好的进行去重的。&lt;/p&gt;

&lt;p&gt;那么在针对URL获取的过程中，我们还可以做的小优化有哪些呢？&lt;/p&gt;

&lt;h3 id=&quot;日期时间命名&quot;&gt;日期时间命名&lt;/h3&gt;
&lt;p&gt;首先，我们可以根据日期来去重。我们知道，在爬取一些Blog和和门户等系统时，经常会遇到以日期命名的目录。&lt;/p&gt;

&lt;p&gt;这些目录大概归纳起来，存在类似下面的形式：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
2010-11-11
10-11-11
20101111

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;当然，还有些文件会以时间+随机值命名，也可能是用unix时间戳命名，这些可能是根据上传和编辑时间来定义的。&lt;/p&gt;

&lt;p&gt;笔者建议是，使用redis或者memcache之类等缓存型数据库，将其直接存储;或者在数据大的时候，考虑作为临时hash set存储。&lt;/p&gt;

&lt;p&gt;比如，一旦出现日期时间命名的目录或静态文件，我们可以将其URL地址作为样本存储在数据库里，存储的内容可以是：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;目录层级
命名格式
URL地址(或hash值)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;有人可能说，在前面seay提出的那个案例里，好像是可以解决类似日期相似度的问题。那我们先看看下面的例子，此处输出仍然基于上面那个函数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print urlsimilar('http://www.baidu.com/blog/2010-10-11/')
print urlsimilar('http://www.baidu.com/blog/2010-10-13/')
print urlsimilar('http://www.baidu.com/blog/2010-9-13/')
print urlsimilar('http://www.baidu.com/whisper/2010-10-11/')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;输出结果如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;110086
110086
37294
4842

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;我们可以看到，在普通情况下，确实于相同父级目录下，相似度算法是可以判断正确的。
但是一旦日期格式不规范，或者父级目录存在一定的差异，这里是不能很好的判断的。&lt;/p&gt;

&lt;p&gt;当然，我们也可以通过机器学习来完成去重的工作。不过就简化工作而言，还是可以使用一些小Tips，根据规则匹配来做到。&lt;/p&gt;

&lt;h3 id=&quot;静态文件的去重&quot;&gt;静态文件的去重&lt;/h3&gt;

&lt;p&gt;我们知道，在爬取URL的过程中，也会遇到许多静态文件，如shtml、html、css等等。这些文件在大多数的情况下，是没有太大意义的。除非测试者倾向于使用“宁可错杀一百，绝不放过一个”的全量采集手法。&lt;/p&gt;

&lt;p&gt;这时候，我们可以配置黑名单，建立文件后缀规则库进行过滤。&lt;/p&gt;

&lt;p&gt;当然，在这些静态后缀的URL链接，也可能带上参数混淆的情况。
个人建议是，用于回调的json、xml等URL，里面可能储存敏感内容，尽量别动；其他类型的静态文件，仍然采取将参数分离的方式，最后对URL进行去重存储。&lt;/p&gt;

&lt;h3 id=&quot;特定情况的过滤&quot;&gt;特定情况的过滤&lt;/h3&gt;

&lt;p&gt;在爬取特定网站时，我们可以预先做好配置，指定过滤一些目录和页面，以节省大量时间资源。&lt;/p&gt;

&lt;p&gt;反过来，我们也可以指定只爬取指定目录下的页面，定向获取我们想要的内容。&lt;/p&gt;

&lt;h3 id=&quot;敏感页面的感知&quot;&gt;敏感页面的感知&lt;/h3&gt;

&lt;p&gt;在seay提出的demo算法中，是有一定局限的。比如我们需要在敏感目录下，尽可能多的拿到文件信息。比如我们爬取到了后台管理目录，可能会遇到下面的情况：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print urlsimilar('http://www.baidu.com/blog/admin/login.php')
print urlsimilar('http://www.baidu.com/blog/admin/manage_index.php')
print urlsimilar('http://www.baidu.com/blog/admin/test.css')

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;输出结果如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;40768
40768
40768
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;很明显有问题不是么？&lt;/p&gt;

&lt;p&gt;当然，我们可以通过对敏感页面关键词进行监控；或者也可以指定后缀文件，进行白名单监控。&lt;/p&gt;

&lt;p&gt;但是一旦这样做，而且还想采用前面的hash算法的话，大家定义的过滤函数的优先级，肯定需要大于该算法。并且，我们在这样做的过程中，也应该考虑过滤成本的问题，建议采用选择性启用。&lt;/p&gt;

&lt;h3 id=&quot;高频敏感目录的优待&quot;&gt;高频敏感目录的优待&lt;/h3&gt;

&lt;p&gt;可能在爬取的过程中，部分爬虫是并用了目录爆破的手段的。如果采用了这种手法并且匹配成功后，我们可以将该目录下的内容单独使用一份过滤规则，从而避免去重算法的误判。&lt;/p&gt;

&lt;h3 id=&quot;响应页面的过滤&quot;&gt;响应页面的过滤&lt;/h3&gt;

&lt;p&gt;对于某些网站来讲，可能有不少页面因为链接是失效的，会被冠以404页面和50x错误。另外，在无权访问的时候，可能网站会做30x跳转和403目录限制。&lt;/p&gt;

&lt;p&gt;这些页面没有实质性内容，在大多数时候是没有意义的，我们可以在配置文件里对需要爬取的这类页面做白名单，比如保留403页面，或者存取30x跳转前（后）的页面。&lt;/p&gt;

&lt;h3 id=&quot;waf警告页面过滤&quot;&gt;WAF（警告）页面过滤&lt;/h3&gt;

&lt;p&gt;某些网站可能被装上了WAF，在访问频率过快时，可能会得到一个WAF的警告页面。而在CMS本身就做了限制的情况下，会以20x的响应码展示一些没有不存在的页面。&lt;/p&gt;

&lt;p&gt;当然，我们可以通过分布式换代理的方式，去解决部分这样的问题，这里先不多做讨论。&lt;/p&gt;

&lt;p&gt;这时候，我们可以配置相应的次数阈值，如果某些页面出现的次数过多，可以将其标记为警告（WAF）页面，进而做出过滤处理。这里对某页面的识别，可以通过黑名单关键字标记，或者计算页面hash值，比如下面这样：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;content = urllib2.urlopen('http://www.test.com/').read()
md5_sum = hashlib.md5()
md5_sum.update(content)
print md5_sum.hexdigest()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当然，我们在实际计算页面hash值和取关键字（做分词或黑名单）时，也可能由于反爬虫机制的存在（如添加随机值），需要适时调整相似度来计算hash值或者采用其他手段。当然这也会消耗更多的时间和机器资源。但某些特定的情况下，可能也会带来意想不到的收获。&lt;/p&gt;

&lt;h3 id=&quot;无意义参数页面去重&quot;&gt;无意义参数页面去重&lt;/h3&gt;

&lt;p&gt;我们在采集页面的过程中，同样有可能会遇到一些毫无意义的、高频出现的、多参数页面。这类页面可能是回调页面，也可能是临时渲染的随机页面。&lt;/p&gt;

&lt;p&gt;在这里，大家可以通过前面处理WAF（警告）的方法进行过滤。当然，使用前面的hash算法也是可以应对大部分情况的。毕竟网站的这类的URL有限，不必为了几种特型去消耗更多的资源，这样得不偿失。&lt;/p&gt;

&lt;h3 id=&quot;js代码中的url&quot;&gt;JS代码中的URL&lt;/h3&gt;

&lt;p&gt;在我们提取js代码，也就是遇到ajax之类的交互情况时，可能会遇到需要拼接的GET请求，或者直接可以取用的POST请求。&lt;/p&gt;

&lt;p&gt;这类的URL地址，最好是结合phantomjs等webkit，更方便地进行动态拼接。&lt;/p&gt;

&lt;p&gt;它们会显得比较特殊，可能仅仅返回状态码，也可能会返回实质性的敏感内容。这种情况，就需要根据爬取者的要求，对爬取的过滤规则进行适应性调整。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;这里旨在提出一些对相似URL去重的小优化，可能效果有限，也可能存在未尽人意之处。欢迎大家提出建议，诸君共勉。&lt;/p&gt;

&lt;h3 id=&quot;参考文章&quot;&gt;参考文章&lt;/h3&gt;

&lt;p&gt;如何避免重复抓取同一个网页
https://segmentfault.com/q/1010000002664904&lt;/p&gt;

&lt;p&gt;浅谈动态爬虫与去重
http://bobao.360.cn/learning/detail/3391.html&lt;/p&gt;

&lt;p&gt;网络爬虫：URL去重策略之布隆过滤器(BloomFilter)的使用
http://blog.csdn.net/lemon_tree12138/article/details/47973715&lt;/p&gt;

&lt;p&gt;实用科普：爬虫技术浅析 编写爬虫应注意的点
http://www.cnseay.com/?p=4102&lt;/p&gt;

&lt;p&gt;网络爬虫 (spider) URL消重设计 URL去重设计
http://woshizn.iteye.com/blog/532605&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">以前在做漏洞Fuzz爬虫时，曾做过URL去重相关的工作，当时是参考了seay法师的文章以及网上零碎的一些资料，感觉做的很简单。近来又遇到相关问题，于是乎有了再次改进算法的念头。</summary></entry><entry><title type="html">迁移博客</title><link href="http://localhost:4000/jekyll/update/start/2017/09/02/welcome-to-jekyll/" rel="alternate" type="text/html" title="迁移博客" /><published>2017-09-02T08:05:18+08:00</published><updated>2017-09-02T08:05:18+08:00</updated><id>http://localhost:4000/jekyll/update/start/2017/09/02/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/start/2017/09/02/welcome-to-jekyll/">&lt;p&gt;
	&lt;span style=&quot;color:#00B050;&quot;&gt;&lt;strong&gt;最近原来的空间服务里到期了，钱包吃紧又想diy，最后就选择了转入Github page。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;在转入时遇到了许多坑，尤其是在ruby gem安装时老遇到依赖问题，下载没代理也老卡住。&lt;/p&gt;

&lt;p&gt;好不容易弄好后，本来想自己改个简单的主题将就用下。结果又遇到分页和一系列插件配置问题，搞得头都大了。&lt;/p&gt;

&lt;p&gt;作为一名老年选手差点崩溃，最后无奈下了个Jekyll主题，稍微改下就全部搞定了。&lt;/p&gt;

&lt;p&gt;好吧，以前用CMS差点被人搞，这次全静态，could you tell me how to play with it?&lt;/p&gt;

&lt;p&gt;If you can…&lt;/p&gt;

&lt;p&gt;Dalao,  ball dai fly…&lt;/p&gt;</content><author><name>demon</name></author><category term="start" /><summary type="html">最近原来的空间服务里到期了，钱包吃紧又想diy，最后就选择了转入Github page。</summary></entry><entry><title type="html">Selenium+phantomjs刷量</title><link href="http://localhost:4000/spider/2016/11/12/seleniumphantomjs-e5-88-b7-e9-87-8f/" rel="alternate" type="text/html" title="Selenium+phantomjs刷量" /><published>2016-11-12T21:52:00+08:00</published><updated>2016-11-12T21:52:00+08:00</updated><id>http://localhost:4000/spider/2016/11/12/seleniumphantomjs%E5%88%B7%E9%87%8F</id><content type="html" xml:base="http://localhost:4000/spider/2016/11/12/seleniumphantomjs-e5-88-b7-e9-87-8f/">&lt;p&gt;
	&lt;span style=&quot;color:#DAA520;&quot;&gt;&lt;strong&gt;最近有这方面的需求，要帮人在某网站上刷点击量。由于该网站是一家比较知名的大型门户，反作弊机制肯定是有的，所以突发奇想用Selenium试试。&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
	与Selenium类似的东西有lxml，它们采用正则xpath路径匹配标签会多一些。当然，也有人会使用beautifulsoup4去解析网页结构，最后再得到需要的标签。达成目的的路不止一条，这点不再赘述。
&lt;/p&gt;

&lt;p&gt;
	本来想用Selenium随便找个浏览器模拟人的浏览网页行为，结果每次需要重新打开一次浏览器，几乎让人抓狂。最后还是采用了phantomjs，这是一种后台浏览器，同样满足模拟行为。
&lt;/p&gt;

&lt;p&gt;
	这里提一下，Selenium需要跟浏览器版本的更新度一致。本来我采用的firefox28.0+Selenium3.01,结果踩了半天坑没找到原因。最后，将火狐更新到最新版47.01才解决。
&lt;/p&gt;

&lt;p&gt;
	像这类调度，我发现的大概需要注意的有两点：
&lt;/p&gt;

&lt;p&gt;
	第一：他们都需要有个浏览器调用的中间件【windows下举例】：
&lt;/p&gt;

&lt;p&gt;
	&lt;span style=&quot;color:#FF0000;&quot;&gt;比如phantomjs.exe（同类的有chromedriver.exe【谷歌】、geckodriver.exe【火狐】），这些需要放在python环境的script目录下。&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
	第二：浏览器需要装在默认目录：
&lt;/p&gt;

&lt;p&gt;
	&lt;span style=&quot;color:#FF0000;&quot;&gt;当然在浏览器目录添加系统环境变量（PATH）也是可以的，这个不是默认添加的，否则会报错。&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
	&lt;span style=&quot;color:#000000;&quot;&gt;完成了这些工作以后，我们就可以后台对&lt;/span&gt;Selenium进行调度了，另外phantomjs具有JS特性，自然也是可以对后台加载的网页进行一些诸如定位、拖动滚动条之类的操作。
&lt;/p&gt;

&lt;p&gt;
	比如下面这段代码片段【案例来自于网络】：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
&amp;nbsp; &amp;nbsp; driver.get(pageURL)&amp;nbsp;
&amp;nbsp; &amp;nbsp; js1 = &amp;#39;return document.body.scrollHeight&amp;#39;
&amp;nbsp; &amp;nbsp; js2 = &amp;#39;window.scrollTo(0, document.body.scrollHeight)&amp;#39;
&amp;nbsp; &amp;nbsp; old_scroll_height = 0
&amp;nbsp; &amp;nbsp; while(driver.execute_script(js1) &amp;gt; old_scroll_height):
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; old_scroll_height = driver.execute_script(js1)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; driver.execute_script(js2)
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; time.sleep(1) &amp;nbsp;&lt;/pre&gt;

&lt;p&gt;
	我们刷流量嘛，自然需要代理。这个可以在网上买，大概5块几千个没问题，不保证稳定性。当然，你去其他免费代理网站爬下来也是可以的。
&lt;/p&gt;

&lt;p&gt;
	设置代理的法子大概写一下【案例来自于网络】：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
service_args = [
 &amp;#39;--proxy=127.0.0.1:9999&amp;#39;,
 &amp;#39;--proxy-type=socks5&amp;#39;,
 ]
driver = webdriver.PhantomJS(&amp;#39;../path_to/phantomjs&amp;#39;,service_args=service_args)&lt;/pre&gt;

&lt;p&gt;
	UA在放在list池子里，需要的时候自行启用：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
from random import choice&lt;/pre&gt;

&lt;p&gt;
	header里设置UA【案例来自于网络】：
&lt;/p&gt;

&lt;pre class=&quot;brush:python;&quot;&gt;
from selenium import webdriver
from selenium.webdriver import DesiredCapabilities
driver=webdriver.PhantomJS(executable_path=&amp;#39;存放路径\phantomjs.exe&amp;#39;)
desired_capabilities= DesiredCapabilities.PHANTOMJS.copy()
headers = {&amp;#39;Accept&amp;#39;: &amp;#39;*/*&amp;#39;,
&amp;#39;Accept-Language&amp;#39;: &amp;#39;en-US,en;q=0.8&amp;#39;,
&amp;#39;Cache-Control&amp;#39;: &amp;#39;max-age=0&amp;#39;,
&amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36&amp;#39;,#这种修改 UA 也有效
&amp;#39;Connection&amp;#39;: &amp;#39;keep-alive&amp;#39;
&amp;#39;Referer&amp;#39;:&amp;#39;http://www.baidu.com/&amp;#39;
}
for key, value in headers.iteritems():
    desired_capabilities['phantomjs.page.customHeaders.{}'.format(key)] = value
desired_capabilities['phantomjs.page.customHeaders.User-Agent'] = &amp;#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&amp;#39;
driver= webdriver.PhantomJS(desired_capabilities=desired_capabilities)
driver.get(&amp;quot;http://www.myip.cn/judge.php&amp;quot;)
print driver.page_source&lt;/pre&gt;

&lt;p&gt;
	其他涉及具体细节的，这里就不多谈了，网上很多。
&lt;/p&gt;

&lt;p&gt;
	我这里也是临时用下，没有太高深的技术。希望对某些朋友有帮助，Good luck!
&lt;/p&gt;

&lt;p&gt;
	&lt;strong&gt;&lt;span style=&quot;color:#FFD700;&quot;&gt;[转载请注明来源&lt;/span&gt;&lt;a href=&quot;http://www.dawner.info/&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color:#FFD700;&quot;&gt;本站&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;color:#FFD700;&quot;&gt;,谢谢。]&lt;/span&gt;&lt;/strong&gt;
&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">最近有这方面的需求，要帮人在某网站上刷点击量。由于该网站是一家比较知名的大型门户，反作弊机制肯定是有的，所以突发奇想用Selenium试试。</summary></entry><entry><title type="html">看我如何从邮箱附件的逆向分析到揪出黑客源头</title><link href="http://localhost:4000/translate/2016/07/06/e7-9c-8b-e6-88-91-e5-a6-82-e4-bd-95-e4-bb-8e-e9-82-ae-e7-ae-b1-e9-99-84-e4-bb-b6-e7-9a-84-e9-80-86-e5-90-91-e5-88-86-e6-9e-90-e5-88-b0-e6-8f-aa-e5-87-ba-e9-bb-91-e5-ae-a2-e6-ba-90-e5-a4-b4/" rel="alternate" type="text/html" title="看我如何从邮箱附件的逆向分析到揪出黑客源头" /><published>2016-07-06T08:55:00+08:00</published><updated>2016-07-06T08:55:00+08:00</updated><id>http://localhost:4000/translate/2016/07/06/%E7%9C%8B%E6%88%91%E5%A6%82%E4%BD%95%E4%BB%8E%E9%82%AE%E7%AE%B1%E9%99%84%E4%BB%B6%E7%9A%84%E9%80%86%E5%90%91%E5%88%86%E6%9E%90%E5%88%B0%E6%8F%AA%E5%87%BA%E9%BB%91%E5%AE%A2%E6%BA%90%E5%A4%B4</id><content type="html" xml:base="http://localhost:4000/translate/2016/07/06/e7-9c-8b-e6-88-91-e5-a6-82-e4-bd-95-e4-bb-8e-e9-82-ae-e7-ae-b1-e9-99-84-e4-bb-b6-e7-9a-84-e9-80-86-e5-90-91-e5-88-86-e6-9e-90-e5-88-b0-e6-8f-aa-e5-87-ba-e9-bb-91-e5-ae-a2-e6-ba-90-e5-a4-b4/">&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;这个故事要从一次垃圾邮件攻击事件说起，下图是一个笔者从某封垃圾邮件里提取的可疑附件。至于下面这蹩脚的英语，这也是值得我们注意的地方。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;上图的附件使用了“.doc”作为后缀，但它其实是RTF（富文本）格式的文件。该文件包含了一个针对性的RTF栈溢出exp，它利用了CVE-2010-3333，也就是在微软Word RTF解析器在处理pFragments时会产生的一个漏洞。然而，该漏洞在五年前就已经修补了。&lt;/p&gt;

&lt;p&gt;正如你在上图中看到的那样，该exp和shellcode做了混淆来逃避杀软的检测。经过各种提取整理解密之后，笔者发现该shellcode会从volafile.io上面下载文件来执行。&lt;/p&gt;

&lt;p&gt;下载下来的这个文件是微软.net Win32可执行文件，简单hex dump了这个文件，笔者发现了HawkEyekeylogger字符串。&lt;/p&gt;

&lt;p&gt;在谷歌后发现，它指引笔者找到了开发该键盘记录器的官网。在网站里，他们列出了该键盘记录器一些的特性。&lt;/p&gt;

&lt;p&gt;在笔者动态的分析中，该键盘记录器会把自身复制一份到Application Data（%appdata%）文件夹，并且将复制后的文件命名为WindowsUpdate.exe。同时，它在注册表里设置了开机启动，以实现其持续性攻击。&lt;/p&gt;

&lt;p&gt;并且，它还会在受感染的系统里释放以下文件：&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;%Temp%\Sysinfo.txt – 释放的恶意软件exe路径&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;%Appdata%\pid.txt –恶意软件进程ID&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;%Appdata%\pidloc.txt – 恶意软件进程exe路径&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;接着，笔者观察到该键盘记录器试图去checkip.dyndns.com，获取受感染系统的外网IP。这个合法的网站经常被恶意软件利用，拿来确定受感染系统的IP地址。&lt;/p&gt;

&lt;p&gt;过了一会儿，笔者监控到了SMTP流量，发现了受感染系统发送信息给黑客email的动作。&lt;/p&gt;

&lt;p&gt;里面的信息可能包括：&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;计算机名&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;本地日期和时间&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;系统语言&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;操作系统&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;平台&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;操作系统版本&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;内存&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;.net框架&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;系统权限&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;默认浏览器&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;防火墙&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;内网IP地址&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;外网IP地址&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;恢复邮件设置和密码&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff6600;&quot;&gt;恢复浏览器和FTP密码&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;正如前面笔者提到的，这款键盘记录器是由.net编译的。所以，笔者接下来需要反编译这个可执行文件。笔者使用了一个开源的.net反编译工具&lt;a href=&quot;https://github.com/icsharpcode/ILSpy&quot;&gt;ILSpy&lt;/a&gt;来完成这个任务。&lt;/p&gt;

&lt;p&gt;笔者反编译出了源代码，并将其与官网的特性列表进行比较，结果表明是完全符合的。笔者发现其代码有以下的特点：&lt;/p&gt;

&lt;p&gt;一个剪贴板记录器&lt;/p&gt;

&lt;p&gt;一个浏览器，FTP和邮件客户端密码记录器。它也会去尝试窃取密码管理器证书和windows密钥。&lt;/p&gt;

&lt;p&gt;蠕虫类的USB感染程序，可以让记录器感染扩散到其他windows机器。&lt;/p&gt;

&lt;p&gt;它也针对一些Steam游戏平台的用户，通过删除配置和登录数据文件，用户会强制再次登录。这就给了键盘记录器窃取用户Steam认证的可乘之机。&lt;/p&gt;

&lt;p&gt;窃取的信息里包括桌面截图，它们会被发送到黑客的邮箱，或者键盘记录器里配好的FTP服务器上。&lt;/p&gt;

&lt;p&gt;黑客貌似也会配置键盘记录器，通过HTTP将窃取的信息上传到PHP服务器上。但是奇怪的是，这里的代码留空了。&lt;/p&gt;

&lt;p&gt;笔者在反编译时，发现最有趣的是一个C#的构造函数Form1()。这是键盘记录器储存配置的地方，但是为了确保黑客电子邮件地址和FTP登录凭证的安全，它们使用了Rijndael算法和Base64加密。&lt;/p&gt;

&lt;p&gt;但是我们知道，这些加密的数据并不一定安全，特别是解密的部分写在了笔者能够反编译的代码里。&lt;/p&gt;

&lt;p&gt;下面这张图是Decrypt（解密）方法，它会接收两个字符串参数：encryptedBytes和secretKey。这个安全密钥恰好是硬编码字符串HawkSpySoftwares。&lt;/p&gt;

&lt;p&gt;正如提到的那样，该键盘记录器使用了Rijndael算法，安全密钥用了Unicode字符串“099u787978786”进行加盐，也是硬编码。&lt;/p&gt;

&lt;p&gt;处于好奇，笔者复制了这部分代码，简单修改适应后，在MS Visual Studio里面去进行编译。当然，最后笔者应该是解密成功了（待验证）。&lt;/p&gt;

&lt;p&gt;最后，笔者拿着邮箱认证信息去登陆尝试。&lt;/p&gt;

&lt;p&gt;这些似乎是感染系统上的电子邮件地址。所以笔者检查了邮件设置，结果发现了意外之喜！发送到这个邮箱的电子邮件会自动转发到黑客的Gmail账户里。你可以在下面截图里看到黑客的Gmail地址。&lt;/p&gt;

&lt;p&gt;也许黑客知道HawkEye容易被破解，所以为了保护他们自己的电子邮件认证信息，就劫持了一个无辜的电子邮件账户作为初始的接收器，最后它会把收到的内容统统转给黑客的真实电子邮件地址。&lt;/p&gt;

&lt;p&gt;最终，笔者把受害的电子邮件帐户还给了失主，并为他们修改了密码，移除了黑客的电子邮件重定向设置。&lt;/p&gt;

&lt;p&gt;如文中所写的，笔者也收到了包含CVE-2012-0158的exp附件，里面是同一款键盘记录器，但是却配置了另一个电子邮箱账户作为窃取数据的初始接收邮箱。&lt;/p&gt;

&lt;p&gt;攻击中的这两个漏洞虽然已经比较早了，但是仍广泛用于电子邮件攻击之中，这里建议读者更新好补丁，使用好相应的杀软，从而防御黑客的攻击。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;strong&gt;[参考来源&lt;a style=&quot;color: #ff0000;&quot; href=&quot;https://www.trustwave.com/Resources/SpiderLabs-Blog/How-I-Cracked-a-Keylogger-and-Ended-Up-in-Someone-s-Inbox/&quot; target=&quot;_blank&quot;&gt;trustwave&lt;/a&gt;，转载请注明本站翻译]&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">这个故事要从一次垃圾邮件攻击事件说起，下图是一个笔者从某封垃圾邮件里提取的可疑附件。至于下面这蹩脚的英语，这也是值得我们注意的地方。</summary></entry><entry><title type="html">Asacub：从间谍软件到银行木马</title><link href="http://localhost:4000/translate/2016/01/25/asacub-ef-bc-9a-e4-bb-8e-e9-97-b4-e8-b0-8d-e8-bd-af-e4-bb-b6-e5-88-b0-e9-93-b6-e8-a1-8c-e6-9c-a8-e9-a9-ac/" rel="alternate" type="text/html" title="Asacub：从间谍软件到银行木马" /><published>2016-01-25T00:17:00+08:00</published><updated>2016-01-25T00:17:00+08:00</updated><id>http://localhost:4000/translate/2016/01/25/asacub:%E4%BB%8E%E9%97%B4%E8%B0%8D%E8%BD%AF%E4%BB%B6%E5%88%B0%E9%93%B6%E8%A1%8C%E6%9C%A8%E9%A9%AC</id><content type="html" xml:base="http://localhost:4000/translate/2016/01/25/asacub-ef-bc-9a-e4-bb-8e-e9-97-b4-e8-b0-8d-e8-bd-af-e4-bb-b6-e5-88-b0-e9-93-b6-e8-a1-8c-e6-9c-a8-e9-a9-ac/">&lt;p&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;strong&gt;我们最近分析了一个系列银行木马Trojan-Banker.AndroidOS.Asacub，发现了其中在用的一台CC服务器chugumshimusona.com，也在为一款名为CoreBot的Windows木马所使用,这让我们起了对这款移动端银行木马进行分析的心思。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;!--more--&gt;
&lt;p&gt;据我们所知，最初版本的Asacub木马出现在2015年6月。与其说当时的Asacub是银行恶意软件，不如说它是一款木马。早期的Asacub会窃取用户收到的所有短信，并上传到黑客的服务器。这款木马能够从CC服务器接收和处理下面的命令：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;get_history: 将浏览器历史上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_contacts: 将联系人列表上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_listapp: 将安装的应用列表上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; block_phone: 锁屏。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; send_sms: 发送带有指定内容的短信到指定号码。&lt;/span&gt;
而新版本的Asacub出现在2015年7月，这款恶意软件在接口使用了欧洲银行的logo，取代了早期版本的美国银行的logo。
与此同时，它可以执行的命令数量也有了很大的增长：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;get_sms: 将所有短信上传到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; del_sms: 删掉指定的短信。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; set_time: 为CC服务器的联系设定新的时间间隔。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_time: 指定CC目标与CC服务器联系的时间间隔。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; mute_vol: 静默电话模式。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; start_alarm: 启用设备白屏处理器继续运行模式。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; stop_alarm: 禁用设备白屏处理器继续运行模式。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; block_phone: 锁屏。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; rev_shell: 反弹shell执行命令。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; intercept_start: 启用短信拦截。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; intercept_stop: 关闭短信拦截。&lt;/span&gt;
上述的远程命令执行（反弹shell）功能，其实对这类恶意软件的来讲是不太正常的。在接收到命令后，木马会主动将远程服务器接入肉鸡设备的控制台，以便黑客在设备上执行命令和获取输出的结果。这个功能是典型的后门功能，我们其实很少发现银行类恶意软件会使用它。因为大多数银行类恶意软件，旨在从受害者银行账户里窃取资金，而不是控制设备本身。
最新版本的Asacub出现在2015年9月之后，这里的功能比起早期版本来更加关注窃取银行的敏感信息。早期版本只是使用了银行的logo图标，新版本中我们则发现了一些带着银行logo的钓鱼页面：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;alignnone&quot; src=&quot;https://cdn.securelist.com/files/2016/01/blog_corebot_1nn-768x698.jpg&quot; alt=&quot;&quot; width=&quot;768&quot; height=&quot;698&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该木马名叫“ActivityVTB24”。这听起来似乎是一家俄罗斯的大型银行，但是其却自称为乌克兰银行。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;alignnone&quot; src=&quot;https://cdn.securelist.com/files/2015/12/blog_corebot_2-768x683.jpg&quot; alt=&quot;&quot; width=&quot;768&quot; height=&quot;683&quot; /&gt;
自去年9月Asacub改版以来，钓鱼窗口出现在所有变种之中，但是其中只有银行卡的输入框可用。这意味着黑客可能只攻击他们所使用银行的客户，当然也可能这只是其中一个版本。
在启动后，“秋日版本（autumnal version）”的木马就开始窃取短信，并且还能执行下面的命令：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;get_history: 上传浏览器历史记录到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_contacts: 上传联系人列表到服务器上。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_cc: 弹出钓鱼窗口来窃取银行卡数据。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; get_listapp: 上传已安装程序列表到服务器。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; change_redir: 转发所有来电到指定手机号码。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; block_phone: 锁屏。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; send_ussd: 运行指定的USSD请求。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; update:下载指定的文件并安装。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; send_sms: 短信发送指定内容到指定号码。&lt;/span&gt;
虽然目前我们并没有注意到有美国用户受到它的攻击，但是黑客对美国银行logo的使用应该是个危险的信号。该木马正在迅速发展，随时有新的特性可能会激活，然后添加进木马里。
&lt;strong&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;今天的Asacub&lt;/span&gt;&lt;/strong&gt;
2015年末，我们发现了一个新的Asacub，它增添了下面的命令：
&lt;span style=&quot;color: #ffcc99;&quot;&gt;GPS_track_current – 获取设备的坐标定位，发送给攻击者。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; camera_shot – 使用设备的相机拍照。&lt;/span&gt;
&lt;span style=&quot;color: #ffcc99;&quot;&gt; network_protocol – 目前我们不知道它有任何用处，但应该在未来会和CC服务器产生交互。&lt;/span&gt;
这些变种里没有钓鱼功能，但代码中涉及到了银行关键词。有意思的是，该木马一直试图关闭乌克兰银行的官方应用：
&lt;img class=&quot;alignnone&quot; src=&quot;https://cdn.securelist.com/files/2016/01/blog_corebot_3.png&quot; alt=&quot;&quot; width=&quot;838&quot; height=&quot;207&quot; /&gt;
此外，我们还分析了该木马和CC服务器的通信，它似乎对俄罗斯手机银行服务特别感兴趣，
在新年假期，新的改动在俄罗斯通过短信疯狂传播。短短一个星期内，从2015.12.28到2016.01.04，我们已发现6500名感染的用户，该木马由此跻身最活跃的恶意程序TOP5.。Asacub改版后发展的速度才有所减缓，我们将继续跟踪这类恶意软件。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[参考来源&lt;a href=&quot;https://securelist.com/blog/research/73211/the-asacub-trojan-from-spyware-to-banking-malware/&quot; target=&quot;_blank&quot;&gt;securelist&lt;/a&gt;，转载请注明本站翻译]&lt;/strong&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">我们最近分析了一个系列银行木马Trojan-Banker.AndroidOS.Asacub，发现了其中在用的一台CC服务器chugumshimusona.com，也在为一款名为CoreBot的Windows木马所使用,这让我们起了对这款移动端银行木马进行分析的心思。</summary></entry><entry><title type="html">python字符编码处理</title><link href="http://localhost:4000/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/2015/08/04/python-e5-ad-97-e7-ac-a6-e7-bc-96-e7-a0-81-e5-a4-84-e7-90-86/" rel="alternate" type="text/html" title="python字符编码处理" /><published>2015-08-04T18:58:00+08:00</published><updated>2015-08-04T18:58:00+08:00</updated><id>http://localhost:4000/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/2015/08/04/python%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E5%A4%84%E7%90%86</id><content type="html" xml:base="http://localhost:4000/%E7%BC%96%E7%A8%8B%E4%B9%8B%E8%B7%AF/2015/08/04/python-e5-ad-97-e7-ac-a6-e7-bc-96-e7-a0-81-e5-a4-84-e7-90-86/">&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;最近因业务需求在写爬虫时，遇到点编码的问题，加上以前曾被类似问题困扰过，特此记录一下。&lt;/span&gt;&lt;/strong&gt;
由于开发和使用环境常在Linux和Win下切换，常遇到字符处理错误，总结一些问题如下：&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;1.如何中文匹配网页内容，而不会被各种编码扰乱&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;s=&quot;编码&quot;
rs=unicode(s, &quot;utf8&quot;)
u'\u7f16\u7801'&lt;/pre&gt;
&lt;p&gt;很简单普遍的做法，unicode编码能很好的转码中文，平时储存中文字符串时可以：&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;x=u'编码'&lt;/pre&gt;
&lt;p&gt;但是这有可能会报错，python在字符处理时忒蛋疼，怎么办？&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;reload(sys)
sys.setdefaultencoding('utf-8')&lt;/pre&gt;
&lt;p&gt;又是非常简单而万精油的两句代码，默认把文件编码设成utf-8，在这时光是在代码头部写上#coding=utf-8之类的是不太管用的。
&lt;span style=&quot;color: #ff0000;&quot;&gt;2.网上摘抄一段内容，也是自己遇到过的，关于把文字直接解码。&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;Traceback (most recent call last):
File &quot;ChineseTest.py&quot;, line 3, in 
print open(&quot;Test.txt&quot;).read().decode(&quot;utf-8&quot;)
UnicodeEncodeError: 'gbk' codec can't encode character u'\ufeff' in position 0: illegal multibyte sequence&lt;/pre&gt;
&lt;p&gt;原来，某些软件，如notepad，在保存一个以UTF-8编码的文件时，会在文件开始的地方插入三个不可见的字符（0xEF 0xBB 0xBF，即BOM）。
因此我们在读取时需要自己去掉这些字符，python中的codecs module定义了这个常量：&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;# coding=gbk
import codecs
data = open(&quot;Test.txt&quot;).read()
if data[:3] == codecs.BOM_UTF8:
data = data[3:]
print data.decode(&quot;utf-8&quot;)
结果：abc中文&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;3.在Win下保存文件时，注意下保存文件的编码，鄙人一般选UTF8，默认的ANSI和UNICODE感觉不太好用。里面的内容换到Linux下容易报错。&lt;/span&gt;
&lt;span style=&quot;color: #ff0000;&quot;&gt;4.编码和解码，这样的姿势也是可以的：&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;x=r'\u7f16\u7801'
print x.decode(&quot;unicode_escape&quot;)
编码&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;
5.话不多说看图，中文字符串的比较:&lt;/span&gt;&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;'编码'.decode('utf-8') == u'编码'
True

'编码' == u'编码'
__main__:1: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal
False

unicode('编码') == u'编码'
Traceback (most recent call last):
  File &quot;&quot;, line 1, in 
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe7 in position 0: ordinal not in range(128)&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;6.附上一个链接，本文上述的问题里面多有讲述，但鄙人没有一一验证过，但确实很详细。&lt;/span&gt;
&lt;span style=&quot;color: #ff00ff;&quot;&gt;&lt;a title=&quot;Python、Unicode和中文&quot; href=&quot;http://blog.csdn.net/summerhust/article/details/6654150&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff00ff;&quot;&gt;Python、Unicode和中文&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;文章是记录型，很简单的东西，不喜勿喷。
&lt;span style=&quot;color: #00B050;&quot;&gt;[转载请注明来源&lt;a href=&quot;http://www.dawner.info&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;本站&lt;/span&gt;&lt;/a&gt;,谢谢。]&lt;/span&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">最近因业务需求在写爬虫时，遇到点编码的问题，加上以前曾被类似问题困扰过，特此记录一下。 由于开发和使用环境常在Linux和Win下切换，常遇到字符处理错误，总结一些问题如下：</summary></entry><entry><title type="html">Ubuntu更新显卡驱动失败解决办法</title><link href="http://localhost:4000/system/2014/10/13/ubuntu-e6-9b-b4-e6-96-b0-e6-98-be-e5-8d-a1-e9-a9-b1-e5-8a-a8-e5-a4-b1-e8-b4-a5-e8-a7-a3-e5-86-b3-e5-8a-9e-e6-b3-95/" rel="alternate" type="text/html" title="Ubuntu更新显卡驱动失败解决办法" /><published>2014-10-13T00:32:00+08:00</published><updated>2014-10-13T00:32:00+08:00</updated><id>http://localhost:4000/system/2014/10/13/ubuntu%E6%9B%B4%E6%96%B0%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/system/2014/10/13/ubuntu-e6-9b-b4-e6-96-b0-e6-98-be-e5-8d-a1-e9-a9-b1-e5-8a-a8-e5-a4-b1-e8-b4-a5-e8-a7-a3-e5-86-b3-e5-8a-9e-e6-b3-95/">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;前段时间因为做项目，一直整夜整夜地挂机爬站，笔记本用成台式机的负荷也是醉了-_-&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;由此出现了一个问题，鄙人笔记本物理机装的Ubuntu13.04，在挂机或者待机一段时间不能恢复登录状态，一直黑屏。对于这种情况本屌只有一个劲儿的敲键盘试图唤醒，结果只能弹出一个纯命令行，切回图形界面后控制面板却没了。&lt;/p&gt;

&lt;p&gt;于是乎，本屌在网上查了下，似乎是因为显卡驱动的原因，好嘛，那就更新显卡驱动，在Ubuntu软件更新器里面（图形化的）点了一个最新版的，兴冲冲地重启然后登陆，啪，尼玛，黑屏。。不死心再来一遍，还是那样，差点就泪奔了，这项目还没做完呢，一会儿搞半天没时间了咋办  T_T&lt;/p&gt;

&lt;p&gt;找大牛求助，指点去Ubuntu社区翻了翻，找出一篇&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?t=217062&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;帖子&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，说是可以恢复模式切回原来的状态，重启看了下，似乎需要安装盘，我去，屌丝表示没准备Ubuntu的安装盘，这条果断Pass。&lt;/p&gt;

&lt;p&gt;接着找到一篇&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?f=94&amp;amp;t=140531&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;帖子&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，让本屌在错误日志里面，找到安装新显卡驱动时的报错，查了下并没有出错，估计只是不兼容？然后在安装日志里试图直接进行卸载，未果。&lt;/p&gt;

&lt;p&gt;网上还有坑爹&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;a href=&quot;http://blog.csdn.net/crazyboy2009/article/details/8232158&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;教程&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;让直接卸载显卡驱动，然后后来就没后文了，尼玛，当本屌傻麽，直接卸载了不管依赖关系想重装系统麽，最好别酱紫做！&lt;/p&gt;

&lt;p&gt;后来本屌的解决办法是通过&lt;strong&gt;&lt;span style=&quot;color: #ff00ff;&quot;&gt;&lt;a href=&quot;http://jishu.zol.com.cn/140483.html&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff00ff;&quot;&gt;更新软件列表源&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/strong&gt;，搞一个叫Paa的玩意儿，将所有的包更新到了最新版本（还是13.x），然后才解决了。&lt;/p&gt;

&lt;p&gt;附上：&lt;span style=&quot;color: #ff00ff;&quot;&gt;&lt;strong&gt;sudo apt-get dist-upgrade&lt;/strong&gt;&lt;/span&gt;（里面的上两条其实不用）&lt;/p&gt;

&lt;p&gt;顺便附上一个没有成功的&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?f=42&amp;amp;p=2973310&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;案例&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，以及后来看到的一个复杂&lt;span style=&quot;color: #00B050;&quot;&gt;&lt;a href=&quot;http://forum.ubuntu.org.cn/viewtopic.php?t=384333&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;例子&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;。&lt;/p&gt;

&lt;p&gt;现在暂时用着笔记本还没出问题，如果再出现相应情况，打算试试这个&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a href=&quot;http://zhidao.baidu.com/link?url=rBeSSVPGK3lwAQVX3Rlj6oHY6_8bUiz-oY8CsIJAs4gTHg747YRiG8OVxSU21jS0GX7DqC4XVDO1EKlvBZViHK&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;案例&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，似乎是啥笔记本模式，看起来有点靠谱。其实最好的法子就是设置电源选项让它不挂起，不过在公司有闲杂人员出入，也只能将就了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;还有，大家没事不要乱更新卸载驱动之类的，坑爹的一B。。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;===================&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;Enjoy yourself！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">前段时间因为做项目，一直整夜整夜地挂机爬站，笔记本用成台式机的负荷也是醉了-_-  </summary></entry><entry><title type="html">对于python框架Scrapy+Gevent的研究</title><link href="http://localhost:4000/other/2014/09/20/e5-af-b9-e4-ba-8epython-e6-a1-86-e6-9e-b6scrapygevent-e7-9a-84-e7-a0-94-e7-a9-b6/" rel="alternate" type="text/html" title="对于python框架Scrapy+Gevent的研究" /><published>2014-09-20T21:26:00+08:00</published><updated>2014-09-20T21:26:00+08:00</updated><id>http://localhost:4000/other/2014/09/20/%E5%AF%B9%E4%BA%8Epython%E6%A1%86%E6%9E%B6scrapygevent%E7%9A%84%E7%A0%94%E7%A9%B6</id><content type="html" xml:base="http://localhost:4000/other/2014/09/20/e5-af-b9-e4-ba-8epython-e6-a1-86-e6-9e-b6scrapygevent-e7-9a-84-e7-a0-94-e7-a9-b6/">&lt;p&gt;&lt;span style=&quot;color: #00ffff;&quot;&gt;&lt;strong&gt;没错，标题党君又来了！文章只是做些第三方评论，不喜勿喷。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;前几天看到&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a title=&quot;Freebuf&quot; href=&quot;http://www.freebuf.com/tools/43194.html&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;FB上的一篇文章&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;，是将用Scrapy爬虫框架加以关键词词尝试，从而将可用的关键词与相应的URL返回存到结果里，个人感觉还是有比较大的改进空间的。覆盖攻击向量字段如下：&lt;/p&gt;

&lt;p&gt;Http头中的Referer字段
User-Agent字段
Cookie
表单（包括隐藏表单）
URL参数
RUL末尾，如 www.example.com/&amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;
跳转型XSS&lt;/p&gt;

&lt;p&gt;由于英文说明书的原作者说该Scrapy的XSS延伸版不能进行Ajax判断，还是有点小遗憾，希望日后改进。&lt;/p&gt;

&lt;p&gt;感觉其提供的测试页面爬下来效果不错，我这儿没有图床可用，就不展示了。（&lt;strong&gt;&lt;span style=&quot;color: #00B050;&quot;&gt;话说有朋友可以给鄙人一免费图床地址么，2333333&lt;/span&gt;&lt;/strong&gt;）&lt;/p&gt;

&lt;p&gt;附上该Git的地址：&lt;a title=&quot;xsscrapy&quot; href=&quot;https://github.com/DanMcInerney/xsscrapy&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;下载&lt;/span&gt;&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;还有就是，本人测试用的是Ubuntu13.04，作者建议pip安装，鄙人使用自带的安装包pip，表示有不少问题，后来在网上下了一个&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;a title=&quot;pip1.5&quot; href=&quot;https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz&quot; target=&quot;_blank&quot;&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;1.5版本&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;的才成功。此外，个人感觉apt-get安装确实挺给力，除了没有的包以外，基本很难报错（知道肯定有人吐槽这B说的不是废话么^_^，以前一个朋友就挺爱骂我SB，不过可惜再也没能见到他了）。&lt;/p&gt;

&lt;p&gt;PS：里面所需的BeautifulSoup最好用BeautifulSoup3.2.1，BeautifulSoup4.x版本已改名为bs4，坑惨小弟了，半天没反应过来。其他的pip安装（如pybloom），也可apt-get安装（如py-requests）,最后，对付某&lt;span style=&quot;color: #00B050;&quot;&gt;error: command ‘x86_64-linux-gnu-gcc’ failed with exit status 1&lt;/span&gt;错误时，&lt;span style=&quot;color: #00B050;&quot;&gt;sudo apt-get install python-twisted-web python2.7-dev&lt;/span&gt;，可能会用的上。&lt;/p&gt;

&lt;p&gt;Scrapy是个不错的爬虫框架，最近笔者自己打算好好研究一下，结合注入工具进行爬虫式扫描，感觉应该不错的样子。如果有朋友有兴趣，或者有现货，欢迎提出宝贵建议，不胜感激。&lt;/p&gt;

&lt;p&gt;另外，前面提到的Gevent是一名访客告诉小弟的，查看了下，是Python的一个高并发框架（高级术语名为协程）。没记错的话，还是以前那位爱骂我SB的朋友告诉我的（小感伤一下），因为以前有做分布式监控的想法。以后考虑将其纳入做项目的计划范围。&lt;/p&gt;

&lt;p&gt;本文很水，不过以后有心得会更新的，除非有单独料，我会单独提出。&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color: #00ffff;&quot;&gt;Enjoy yourself.^_^&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</content><author><name>DEMON</name></author><summary type="html">没错，标题党君又来了！文章只是做些第三方评论，不喜勿喷。</summary></entry><entry><title type="html">Md5批量+多接口破解脚本</title><link href="http://localhost:4000/tools/2014/09/08/md5-e6-89-b9-e9-87-8f-e5-a4-9a-e6-8e-a5-e5-8f-a3-e7-a0-b4-e8-a7-a3-e8-84-9a-e6-9c-ac/" rel="alternate" type="text/html" title="Md5批量+多接口破解脚本" /><published>2014-09-08T00:50:00+08:00</published><updated>2014-09-08T00:50:00+08:00</updated><id>http://localhost:4000/tools/2014/09/08/md5%E6%89%B9%E9%87%8F%E5%A4%9A%E6%8E%A5%E5%8F%A3%E7%A0%B4%E8%A7%A3%E8%84%9A%E6%9C%AC</id><content type="html" xml:base="http://localhost:4000/tools/2014/09/08/md5-e6-89-b9-e9-87-8f-e5-a4-9a-e6-8e-a5-e5-8f-a3-e7-a0-b4-e8-a7-a3-e8-84-9a-e6-9c-ac/">&lt;p&gt;=~~~~~~~~~~~~~~~~~~~~&lt;del&gt;=
+———–DemonSpider—————-+
+-Code reset by demon@f4ck.net—-+
+———-&lt;a title=&quot;www.dawner.info&quot; href=&quot;http://www.dawner.info&quot; target=&quot;_blank&quot;&gt;www.dawner.info&lt;/a&gt;————+
=&lt;/del&gt;~~~~~~~~~~~~~~~~~~~~=
&lt;span style=&quot;color: #ff0000;&quot;&gt;PS:个人觉得这脚本最大的特点就是多接口+批量，当然，也可单选。&lt;/span&gt;
代码预览如下：&lt;/p&gt;
&lt;pre lang=&quot;python&quot;&gt;
# -*- coding=utf-8 -*-
import sys
import socket
import urllib
import re
import string
import urllib2
import threading
import HTMLParser
import cookielib

def showInfo():
    print &quot;&quot;&quot;
     命令格式：md5_crack.py -hash md5hash \n
			or md5_crack.py -dic
           &quot;&quot;&quot;

comcn_tmp=[]


class timer(threading.Thread): 
    def __init__(self, HASH):  
        threading.Thread.__init__(self)  
        self.HASH = HASH    
   
    def run(self):  
        try:
            if crack_md5asia(self.HASH):
                return True
        except Exception,e:
            print e
            pass
        
        try:
            if crack_cc(self.HASH):
                return True
        except Exception,e:
            print e
            pass
        
        try:
            if crack_silic(self.HASH):
                return True
        except Exception,e:
            print e
            pass
        
        try:
            if crack_comcn(self.HASH):
                return True
        except Exception,e:
            print e
            pass

        try:
            if crack_somd5(self.HASH):
                return True
        except Exception,e:
            print e
            pass
        
        print &quot;[x]HASH Crack: &quot;+self.HASH+&quot; failed.&quot;
        f.writelines(HASH+'\n')
        return False 
              


class Parselinks(HTMLParser.HTMLParser):
    def handle_starttag(self,tag,attrs):
        if tag == 'input':
            for name,value in attrs:
                if name == 'name':
                    if value != 'sand':
                        continue
                    else:
                        count=0
                        for name,value in attrs:
                            count=count+1
                            if count == 3: 
                                comcn_tmp.append(value)

        if tag == 'input':
            for name,value in attrs:
                if name == 'name':
                    if value != 'token':
                        continue
                    else:
                        count=0
                        for name,value in attrs:
                            count=count+1
                            if count == 3: 
                                comcn_tmp.append(value)


#From http://www.md5.asia/
def crack_md5asia(Hash):   
    str_url=[&quot;http://md5ss.sinaapp.com/md5_decode.php?decoder=1&amp;amp;timeout=10&amp;amp;hash=&quot;,Hash]
    url=&quot;&quot;.join(str_url)
    #print url
            
    try:
        sock=urllib.urlopen(url)
        htmlSources=sock.read()
    except:
        #print &quot;Not Found&quot;
        return False
    else:
        sock.close()
    #本来这个地方，应该比较&quot;未找到,&quot;，但是由于编码的问题，会出问题。因此，改为16进制了。
    if string.find(htmlSources,&quot;\346\234\252\346&quot;)!=-1:
        return False
        #print &quot;Not Found2&quot;
    else:
        print &quot;Password Found:&quot;,htmlSources
        s.writelines(HASH+' '+resp+'\n')
        print 'asia:',resp
        return True
        #exit(1)


def crack_comcn(HASH):

    cj = cookielib.CookieJar();
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
    urllib2.install_opener(opener)
    parsed = Parselinks()
    parsed.feed(urllib.urlopen('http://www.md5.com.cn').read())
    post_data = {'md': HASH ,'sand':comcn_tmp[0],'token':comcn_tmp[1],'submit':'MD5+Crack'}
    post_data_urlencode = urllib.urlencode(post_data)
    requrl = &quot;http://www.md5.com.cn/md5reverse&quot;
    req = urllib2.Request(url = requrl,data =post_data_urlencode)
    req.add_header('Referer', &quot;http://www.md5.com.cn/&quot;)
    resps = urllib2.urlopen(req)
    match = re.findall('Result:.*green&quot;&amp;gt;.*&amp;lt;\/span',resps.read())
    re_match=re.findall('green&quot;&amp;gt;.*&amp;lt;\/span&amp;gt;&amp;lt;div',match[0])[0][7:-11]
    #print re_match[0][7:-11]
    s.writelines(HASH+' '+re_match+'\n')
    print 'comcn:',re_match
    return True



def crack_silic(HASH):
    post_data = {'isajax':'1' ,'md5':HASH}
    post_data_urlencode = urllib.urlencode(post_data)
    requrl = &quot;http://cracker.blackbap.org/?do=search&amp;amp;language=en&quot;
    req = urllib2.Request(url = requrl,data =post_data_urlencode)
    res_data = urllib2.urlopen(req)
    res = res_data.read()
    #print res
    resp = re.findall('Password &lt;strong&gt;.*&amp;lt;\/strong&amp;gt;',res)[0][17:-9]
    #print resp
    s.writelines(HASH+' '+resp+'\n')
    print 'silic:',resp
    return True

def crack_cc(HASH):
    url='http://www.md5.cc/ShowMD5Info.asp?GetType=ShowInfo&amp;amp;no-cache=0.4669540437658686&amp;amp;md5_str='+HASH+'&amp;amp;_='
    request = urllib2.Request(url)
    request.add_header('Referer', &quot;http://www.md5.cc/&quot;)
    res=urllib2.urlopen(request).read()
    resp = re.findall('25px&quot;&amp;gt;.*&amp;lt;\/span&amp;gt;',res)[0][6:-7].strip()
    #print resp[6:-7].strip()
    s.writelines(HASH+' '+resp+'\n')
    print &quot;cc:&quot;,resp
    return True

def crack_somd5(HASH):

    resp=urllib.urlopen('http://www.somd5.com/somd5-md5-js.html').read()
    ajax_data=re.findall('isajax=.*&amp;amp;',resp)[0][7:-1]
    post_data = {'isajax':ajax_data,'md5':HASH}
    post_data_urlencode = urllib.urlencode(post_data)
    requrl = &quot;http://www.somd5.com/somd5-index-md5.html&quot;
    req = urllib2.Request(url = requrl,data =post_data_urlencode)
    resps = urllib2.urlopen(req).read()
    match = re.findall('&amp;lt;h1.*line;&quot;&amp;gt;.*&amp;lt;\/h1',resps)[0]
    re_match = re.findall('&quot;&amp;gt;.*&amp;lt;/',match)[0][2:-2]
    s.writelines(HASH+' '+resp+'\n')
    print &quot;somd5&quot;,resp
    return True


if '__main__' == __name__:         

#这个简单脚本借用了一哥们儿的框架，虽然改动不少，按理确实应该留下原先那个兄弟版权的。。
#可是我在论坛搜了半天都没搜到。。完全遗忘了原版在哪儿找的有木有！！
#要是这兄弟看到了可以跟小弟联系，不嫌弃的话我加上~	

    print &quot;&quot;&quot;
     =~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=
     +-----------DemonSpider-----------+
     +--------Md5-Cracker--V1.0--------+
     +----------www.dawner.info--------+
     +--md5_crack.py for single or dic-+
     ===================================
                    = =
                   == ==
                ===&amp;lt;-|-&amp;gt;===
                 ====D====
                  ===e===
                   ==m==
                    =o=
                     n
    &quot;&quot;&quot;
    global f,s

    if len(sys.argv)&amp;lt;2 :
         print &quot;参数错误&quot;
         showInfo()
         exit(1)
     
    cmds = ['-hash','-dic']
     
    s=open('success_result.txt','w+')
    f=open('fail_result.txt','w+')
    cmd = sys.argv[1]
    if len(sys.argv) == 3 :
        Hash=sys.argv[2]
         
		 
    if 0 == cmds.count(cmd):  
        print cmd
        print &quot;参数错误&quot;
        showInfo()
        exit(1)
    else:
        print 'Start working,Please waiting...'
        if cmd == '-hash':
            if len(Hash)==16 or len(Hash)==32:
                crack_thread = timer(Hash)
                crack_thread.start()
                #crack_thread.setDamon()
                crack_thread.join()
            else:
                print &quot;Hash长度出错.&quot;
        elif cmd == 'dic':
            for line in open('hash.txt','r'):
                if len(line.strip())==16 or len(line.strip())==32:
                    crack_thread = timer(line.strip())
                    crack_thread.start()

                else:
                    continue
    s.close()
    f.close()
    print &quot;Crack ending....&quot;
&amp;lt;/pre&amp;gt;
接口共五个，经小弟自己测试还算合手。

用法：
python md5_cracker.py -hash md5hash
python md5_cracker.py -dic

字典文件：
hash.txt
成功结果文件：success_result.txt
失败结果文件：fail_result.txt

&lt;span style=&quot;color: #00B050;&quot;&gt;附件。。就懒得发了吧。。^m^&lt;/span&gt;
&lt;/strong&gt;&lt;/pre&gt;</content><author><name>DEMON</name></author><summary type="html">=~~~~~~~~~~~~~~~~~~~~= +———–DemonSpider—————-+ +-Code reset by demon@f4ck.net—-+ +———-www.dawner.info————+ =~~~~~~~~~~~~~~~~~~~~= PS:个人觉得这脚本最大的特点就是多接口+批量，当然，也可单选。 代码预览如下： # -*- coding=utf-8 -*- import sys import socket import urllib import re import string import urllib2 import threading import HTMLParser import cookielib</summary></entry></feed>